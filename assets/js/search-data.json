{
  
    
        "post0": {
            "title": "Integración de PyQGIS con Jupyter Lab usando Anaconda",
            "content": "Este breve tutorial es un complemento a la entrada anterior de mi blog, Introducción a PyQGIS, donde describí cómo empezar a usar PyQGIS desde la consola de Python integrada en la interfaz gráfica de QGIS. Si bien lo recomendable es que mientras desarrollamos un proceso de automatización con PyQGIS usemos la consola integrada para ir viendo visualmente que los resultados que obtenemos son los que deseamos, también es un poco engorroso escribir código desde el editor de esa consola, porque es muy básico y le faltan características como autocompletado, resaltado, revisión de sintaxis y otras herramientas que los IDES modernos ofrecen. . Desde hace pocos años es posible instalar QGIS como una librería más de conda, lo que permite crear ambientes aisalados que incluyan QGIS. De esta forma, es posible desarrollar usando PyQGIS sin afectar la instalación de la aplicación de escritorio de QGIS. Esto es importante, porque si queremos instalar librerías adicionales de análisis de datos, es mucho más seguro hacerlo desde un ambiente aislado, así en caso que haya incompatibilidades entre librerías o que se afecten aquellas de las que depende PyQGIS y esto genere errores, no hay más que crear un nuevo ambiente y empezar otra vez. Si eso ocurriera dentro de la aplicación de escritorio, probablemente tendríamos que desinstalar y volver a instalar QGIS. Antes de que QGIS fuera una librería de conda, no era fácil establecer un ambiente de desarrollo de PyQGIS que no estuviera ligado al intérprete de Python incluido en QGIS. Afortunadamente con conda es tremendamente fácil instalar un ambiente. . Para empezar es necesario contar con Anaconda o Miniconda. Luego desde la terminal de comandos podemos ejecutar de forma secuencial los sigientes comandos: . conda create -n qgis -c conda-forge qgis jupyterlab matplotlib conda activate qgis jupyter lab . En la primera línea estamos creando un nuevo ambiente llamado qgis y además le pedimos que instale las librerías qgis, jupyterlab y matplotlib desde el canal de conda-forge. En la segunda línea activamos el ambiente y en la tercera lanzamos Jupyter Lab que es mi IDE preferido para trabajar en análisis datos. . Con esto podemos crear un nuevo notebook y ya podremos ejecutar código de PyQGIS. Así de sencillo. Por ejemplo, usando una versión ligeramente modificada del primer script que usé en mi entrada anterior así lo puedo ejecutar: . import json import os import sys from qgis.core import QgsProject, QgsVectorLayer, QgsProcessingFeatureSourceDefinition, QgsFeatureRequest, QgsVectorFileWriter, QgsApplication, QgsProcessingProvider from qgis.analysis import QgsNativeAlgorithms import matplotlib.pyplot as plt os.environ[&quot;QT_QPA_PLATFORM&quot;] = &quot;offscreen&quot; # Esta línea es necesaria para poder iniciar una instancia QgsApplication(), en caso de que sea necesario QgsApplication.setPrefixPath(None, True) import processing from processing.core.Processing import Processing # Inicia algoritmos Processing.initialize() # establece directorio de trabajo os.chdir(&#39;/home/studio-lab-user/sagemaker-studiolab-notebooks/intro_pyqgis&#39;) proyecto = QgsProject.instance() # carpeta donde están los datos dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) if not proyecto.mapLayersByName(nombre_layer_colonias): proyecto.addMapLayer(layer_colonias) print(&quot;Número de capas en el proyecto:&quot;, proyecto.count()) print(&quot;CRS del proyecto:&quot;, proyecto.crs().description()) print(&quot;Directorio del proyecto:&quot;, proyecto.homePath()) print(&quot;Número de filas:&quot;, layer_colonias.featureCount()) print(&quot;Número de columnas:&quot;, len(layer_colonias.fields())) print(&quot;Sistema de coordenadas:&quot;, layer_colonias.crs().description()) for f in layer_colonias.fields(): print(f.name(), f.typeName()) # feature = layer_colonias.getFeature(10) print(feature) print(feature.attributes()) # geom = feature.geometry() print(geom.centroid()) coords = json.loads(geom.asJson())[&quot;coordinates&quot;][0][0] coords_centroide = json.loads(geom.centroid().asJson())[&quot;coordinates&quot;] x, y = [[xy[n] for xy in coords] for n in [0, 1]] ax = plt.subplot() ax.plot(x, y) ax.plot([coords_centroide[0]], [coords_centroide[1]], &#39;bo&#39;) ax.text(x=coords_centroide[0], y=coords_centroide[1], s=feature.attribute(&quot;NOMUT&quot;)) . Número de capas en el proyecto: 1 CRS del proyecto: Directorio del proyecto: Número de filas: 1815 Número de columnas: 8 Sistema de coordenadas: WGS 84 / UTM zone 14N ENT String CVEDT String NOMDT String DTTOLOC String CVEUT String NOMUT String POB2010 String ID Integer &lt;qgis._core.QgsFeature object at 0x7f09dd057400&gt; [&#39;9&#39;, &#39;2&#39;, &#39;AZCAPOTZALCO&#39;, &#39;05&#39;, &#39;02-013&#39;, &#39;CUITLÁHUAC 1 Y 2 (U HAB)&#39;, &#39;2449&#39;, 11] &lt;QgsGeometry: Point (482445.29628468106966466 2153086.92174857435747981)&gt; . QObject::startTimer: Timers can only be used with threads started with QThread . Text(482445.2962846811, 2153086.9217485744, &#39;CUITLÁHUAC 1 Y 2 (U HAB)&#39;) . Como podemos ver, el resultado es el mismo que el que obtuvimos usando la consola de Python dentro de QGIS. . Para poder correr este script tuve que hacer unas pequeñas modificaciones, la más importante y necesaria para que todo funcione, fue que tuve que agregar la línea: . QgsApplication.setPrefixPath(None, True) . antes de importar la librería processing e inicializar los algoritmos. Usualmente el primer argumento es la ubicación de la carpeta donde está instalado QGIS, aunque en mi caso parece que el sistema lo identifica automáticamente y por eso simplemente puedo poner un None. En cualquier caso, si al intentarlo no te funciona, intenta poniendo la dirección a la carpeta de tu instalación de QGIS, en mi caso sería: . QgsApplication.setPrefixPath(&#39;/home/studio-lab-user/.conda/envs/qgis/share/qgis&#39;, True) . Otra línea que tuve que agregar fue: . os.chdir(&#39;/home/studio-lab-user/sagemaker-studiolab-notebooks/intro_pyqgis&#39;) . para establecer el directorio de trabajo principal ya que este notebook lo estoy escribiendo desde el servicio de Amazon SageMaker Studio Lab, que ofrece de forma gratuita (y sin pedir medios de pago ni cuenta de AWS) una máquina virtual de 16GB de RAM y 15GB de almacenamiento persistente. El almacenamiento persistente me permite crear ambientes de conda y las librerías instaladas permanecen siempre, a diferencia de Google Colab (que también es un gran, gran, gran servicio) donde debes instalar las librerías cada vez que inicias una sesión. Que por cierto, en un post de su blog Lerry W nos muestra cómo instalar QGIS dentro de Colab. . Volviendo al código, a continuación ejecutamos un fragmento del script intro_pyqgis_04.py en el que calculamos los centroides de la capa de colonias_cdmx y exportamos los resultados en formato GeoJSON. La diferencia con el publicado anteriormente es que en lugar de añadir la capa resultante al proyecto, ya mejor la exportamos directamente especificando la ruta en el parámetro &#39;OUTPUT&#39;. Dado que ya no estamos usando la interfaz gráfica, en realidad no tiene mucho sentido añadirla al proyecto para poder visualizarla. . params = { &#39;INPUT&#39;: QgsProcessingFeatureSourceDefinition( nombre_layer_colonias, selectedFeaturesOnly=False, featureLimit=-1, flags=QgsProcessingFeatureSourceDefinition.FlagOverrideDefaultGeometryCheck, geometryCheck=QgsFeatureRequest.GeometrySkipInvalid ), &#39;ALL_PARTS&#39;: False, &#39;OUTPUT&#39;: f&quot;{dir_data}/centroides_colonias.geojson&quot; } try: out = processing.run(&quot;native:centroids&quot;, params) except: out = processing.run(&quot;native:centroids&quot;, params) . Warning 1: organizePolygons() received an unexpected geometry. Either a polygon with interior rings, or a polygon with less than 4 points, or a non-Polygon geometry. Return arguments as a collection. QObject::startTimer: Timers can only be used with threads started with QThread . . Warning: en la celda anterior tuve que introducir un bloque try-except porque, por alguna razón desconocida para mí, al ejecutar una vez la línea processing.run(&quot;native:centroids&quot;, params) arroja un error, pero después de dos o más ejecuciones funciona sin problema. Hay que tener en cuenta que esta versión de QGIS en conda no es oficialmente soportada por los desarrolladores QGIS y puede que tenga comportamientos un poco distintos que la versión oficial que es la que viene con QGIS. . Y precisamente, como no tenemos la interfaz gráfica de QGIS, no podemos visualizar directamente los resultados. Aún así, nos la podemos arreglar para usar matplotlib y graficar los puntos. Otra opción obvia es GeoPandas. . layer_centroides = QgsVectorLayer(out[&quot;OUTPUT&quot;]) geoms = [f.geometry().asPoint() for f in layer_centroides.getFeatures()] points_x = [p.x() for p in geoms] points_y = [p.y() for p in geoms] ax = plt.subplot() ax.plot(points_x, points_y, &#39;bo&#39;, alpha=0.2) ax.set_aspect(&#39;equal&#39;) ax.figure.set_size_inches(8, 8) ax.set_title(&quot;Colonias de la CDMX&quot;) . QObject::startTimer: Timers can only be used with threads started with QThread . Text(0.5, 1.0, &#39;Colonias de la CDMX&#39;) . A mí en particular me gusta mucho trabajar con Jupyter Lab, pero si se prefiere también es posible usar otros editores como PyCharm o VScode ya que estos también tienen soporte para trabajar con ambientes de Conda. . Y hasta aquí llega esta entrada. Dejo este código que nos permite encontrar el ID de todos los algoritmos disponibles en PyQGIS: . QgsApplication.processingRegistry().addProvider(QgsNativeAlgorithms()) for alg in QgsApplication.processingRegistry().algorithms(): print(alg.id(), &quot;&gt;&quot;, alg.displayName()) . Logged warning: Duplicate provider native registered . 3d:tessellate &gt; Tessellate gdal:aspect &gt; Aspect gdal:assignprojection &gt; Assign projection gdal:buffervectors &gt; Buffer vectors gdal:buildvirtualraster &gt; Build virtual raster gdal:buildvirtualvector &gt; Build virtual vector gdal:cliprasterbyextent &gt; Clip raster by extent gdal:cliprasterbymasklayer &gt; Clip raster by mask layer gdal:clipvectorbyextent &gt; Clip vector by extent gdal:clipvectorbypolygon &gt; Clip vector by mask layer gdal:colorrelief &gt; Color relief gdal:contour &gt; Contour gdal:contour_polygon &gt; Contour Polygons gdal:convertformat &gt; Convert format gdal:dissolve &gt; Dissolve gdal:executesql &gt; Execute SQL gdal:extractprojection &gt; Extract projection gdal:fillnodata &gt; Fill nodata gdal:gdal2tiles &gt; gdal2tiles gdal:gdal2xyz &gt; gdal2xyz gdal:gdalinfo &gt; Raster information gdal:gridaverage &gt; Grid (Moving average) gdal:griddatametrics &gt; Grid (Data metrics) gdal:gridinversedistance &gt; Grid (Inverse distance to a power) gdal:gridinversedistancenearestneighbor &gt; Grid (IDW with nearest neighbor searching) gdal:gridlinear &gt; Grid (Linear) gdal:gridnearestneighbor &gt; Grid (Nearest neighbor) gdal:hillshade &gt; Hillshade gdal:importvectorintopostgisdatabaseavailableconnections &gt; Export to PostgreSQL (available connections) gdal:importvectorintopostgisdatabasenewconnection &gt; Export to PostgreSQL (new connection) gdal:merge &gt; Merge gdal:nearblack &gt; Near black gdal:offsetcurve &gt; Offset curve gdal:ogrinfo &gt; Vector information gdal:onesidebuffer &gt; One side buffer gdal:overviews &gt; Build overviews (pyramids) gdal:pansharp &gt; Pansharpening gdal:pcttorgb &gt; PCT to RGB gdal:pointsalonglines &gt; Points along lines gdal:polygonize &gt; Polygonize (raster to vector) gdal:proximity &gt; Proximity (raster distance) gdal:rastercalculator &gt; Raster calculator gdal:rasterize &gt; Rasterize (vector to raster) gdal:rasterize_over &gt; Rasterize (overwrite with attribute) gdal:rasterize_over_fixed_value &gt; Rasterize (overwrite with fixed value) gdal:rearrange_bands &gt; Rearrange bands gdal:retile &gt; Retile gdal:rgbtopct &gt; RGB to PCT gdal:roughness &gt; Roughness gdal:sieve &gt; Sieve gdal:slope &gt; Slope gdal:tileindex &gt; Tile index gdal:tpitopographicpositionindex &gt; Topographic Position Index (TPI) gdal:translate &gt; Translate (convert format) gdal:triterrainruggednessindex &gt; Terrain Ruggedness Index (TRI) gdal:viewshed &gt; Viewshed gdal:warpreproject &gt; Warp (reproject) native:addautoincrementalfield &gt; Add autoincremental field native:addfieldtoattributestable &gt; Add field to attributes table native:adduniquevalueindexfield &gt; Add unique value index field native:addxyfields &gt; Add X/Y fields to layer native:affinetransform &gt; Affine transform native:aggregate &gt; Aggregate native:angletonearest &gt; Align points to features native:antimeridiansplit &gt; Geodesic line split at antimeridian native:arrayoffsetlines &gt; Array of offset (parallel) lines native:arraytranslatedfeatures &gt; Array of translated features native:aspect &gt; Aspect native:assignprojection &gt; Assign projection native:atlaslayouttoimage &gt; Export atlas layout as image native:atlaslayouttopdf &gt; Export atlas layout as PDF native:bookmarkstolayer &gt; Convert spatial bookmarks to layer native:boundary &gt; Boundary native:boundingboxes &gt; Bounding boxes native:buffer &gt; Buffer native:bufferbym &gt; Variable width buffer (by M value) native:calculatevectoroverlaps &gt; Overlap analysis native:categorizeusingstyle &gt; Create categorized renderer from styles native:cellstackpercentile &gt; Cell stack percentile native:cellstackpercentrankfromrasterlayer &gt; Cell stack percentrank from raster layer native:cellstackpercentrankfromvalue &gt; Cell stack percent rank from value native:cellstatistics &gt; Cell statistics native:centroids &gt; Centroids native:clip &gt; Clip native:collect &gt; Collect geometries native:combinestyles &gt; Combine style databases native:condition &gt; Conditional branch native:converttocurves &gt; Convert to curved geometries native:convexhull &gt; Convex hull native:countpointsinpolygon &gt; Count points in polygon native:createattributeindex &gt; Create attribute index native:createconstantrasterlayer &gt; Create constant raster layer native:createdirectory &gt; Create directory native:creategrid &gt; Create grid native:createpointslayerfromtable &gt; Create points layer from table native:createrandombinomialrasterlayer &gt; Create random raster layer (binomial distribution) native:createrandomexponentialrasterlayer &gt; Create random raster layer (exponential distribution) native:createrandomgammarasterlayer &gt; Create random raster layer (gamma distribution) native:createrandomgeometricrasterlayer &gt; Create random raster layer (geometric distribution) native:createrandomnegativebinomialrasterlayer &gt; Create random raster layer (negative binomial distribution) native:createrandomnormalrasterlayer &gt; Create random raster layer (normal distribution) native:createrandompoissonrasterlayer &gt; Create random raster layer (poisson distribution) native:createrandomuniformrasterlayer &gt; Create random raster layer (uniform distribution) native:createspatialindex &gt; Create spatial index native:dbscanclustering &gt; DBSCAN clustering native:deletecolumn &gt; Drop field(s) native:deleteduplicategeometries &gt; Delete duplicate geometries native:deleteholes &gt; Delete holes native:densifygeometries &gt; Densify by count native:densifygeometriesgivenaninterval &gt; Densify by interval native:detectvectorchanges &gt; Detect dataset changes native:difference &gt; Difference native:dissolve &gt; Dissolve native:dropgeometries &gt; Drop geometries native:dropmzvalues &gt; Drop M/Z values native:dxfexport &gt; Export layers to DXF native:equaltofrequency &gt; Equal to frequency native:explodehstorefield &gt; Explode HStore Field native:explodelines &gt; Explode lines native:exportlayersinformation &gt; Export layer(s) information native:exportmeshedges &gt; Export mesh edges native:exportmeshfaces &gt; Export mesh faces native:exportmeshongrid &gt; Export mesh on grid native:exportmeshvertices &gt; Export mesh vertices native:exporttospreadsheet &gt; Export to spreadsheet native:extendlines &gt; Extend lines native:extenttolayer &gt; Create layer from extent native:extractbinary &gt; Extract binary field native:extractbyattribute &gt; Extract by attribute native:extractbyexpression &gt; Extract by expression native:extractbyextent &gt; Extract/clip by extent native:extractbylocation &gt; Extract by location native:extractmvalues &gt; Extract M values native:extractspecificvertices &gt; Extract specific vertices native:extractvertices &gt; Extract vertices native:extractzvalues &gt; Extract Z values native:fieldcalculator &gt; Field calculator native:filedownloader &gt; Download file native:fillnodata &gt; Fill NoData cells native:filter &gt; Feature filter native:filterbygeometry &gt; Filter by geometry type native:filterlayersbytype &gt; Filter layers by type native:filterverticesbym &gt; Filter vertices by M value native:filterverticesbyz &gt; Filter vertices by Z value native:fixgeometries &gt; Fix geometries native:flattenrelationships &gt; Flatten relationship native:forcerhr &gt; Force right-hand-rule native:fuzzifyrastergaussianmembership &gt; Fuzzify raster (gaussian membership) native:fuzzifyrasterlargemembership &gt; Fuzzify raster (large membership) native:fuzzifyrasterlinearmembership &gt; Fuzzify raster (linear membership) native:fuzzifyrasternearmembership &gt; Fuzzify raster (near membership) native:fuzzifyrasterpowermembership &gt; Fuzzify raster (power membership) native:fuzzifyrastersmallmembership &gt; Fuzzify raster (small membership) native:generatepointspixelcentroidsinsidepolygons &gt; Generate points (pixel centroids) inside polygons native:geometrybyexpression &gt; Geometry by expression native:greaterthanfrequency &gt; Greater than frequency native:highestpositioninrasterstack &gt; Highest position in raster stack native:hillshade &gt; Hillshade native:hublines &gt; Join by lines (hub lines) native:importphotos &gt; Import geotagged photos native:interpolatepoint &gt; Interpolate point on line native:intersection &gt; Intersection native:joinattributesbylocation &gt; Join attributes by location native:joinattributestable &gt; Join attributes by field value native:joinbynearest &gt; Join attributes by nearest native:kmeansclustering &gt; K-means clustering native:layertobookmarks &gt; Convert layer to spatial bookmarks native:lessthanfrequency &gt; Less than frequency native:linedensity &gt; Line density native:lineintersections &gt; Line intersections native:linesubstring &gt; Line substring native:loadlayer &gt; Load layer into project native:lowestpositioninrasterstack &gt; Lowest position in raster stack native:meancoordinates &gt; Mean coordinate(s) native:mergelines &gt; Merge lines native:mergevectorlayers &gt; Merge vector layers native:meshcontours &gt; Export contours native:meshexportcrosssection &gt; Export cross section dataset values on lines from mesh native:meshexporttimeseries &gt; Export time series values from points of a mesh dataset native:meshrasterize &gt; Rasterize mesh dataset native:minimumenclosingcircle &gt; Minimum enclosing circles native:multiparttosingleparts &gt; Multipart to singleparts native:multiringconstantbuffer &gt; Multi-ring buffer (constant distance) native:nearestneighbouranalysis &gt; Nearest neighbour analysis native:offsetline &gt; Offset lines native:orderbyexpression &gt; Order by expression native:orientedminimumboundingbox &gt; Oriented minimum bounding box native:orthogonalize &gt; Orthogonalize native:package &gt; Package layers native:pixelstopoints &gt; Raster pixels to points native:pixelstopolygons &gt; Raster pixels to polygons native:pointonsurface &gt; Point on surface native:pointsalonglines &gt; Points along geometry native:pointstopath &gt; Points to path native:pointtolayer &gt; Create layer from point native:poleofinaccessibility &gt; Pole of inaccessibility native:polygonfromlayerextent &gt; Extract layer extent native:polygonize &gt; Polygonize native:polygonstolines &gt; Polygons to lines native:postgisexecutesql &gt; PostgreSQL execute SQL native:printlayoutmapextenttolayer &gt; Print layout map extent to layer native:printlayouttoimage &gt; Export print layout as image native:printlayouttopdf &gt; Export print layout as PDF native:projectpointcartesian &gt; Project points (Cartesian) native:promotetomulti &gt; Promote to multipart native:raiseexception &gt; Raise exception native:raisewarning &gt; Raise warning native:randomextract &gt; Random extract native:randompointsinextent &gt; Random points in extent native:randompointsinpolygons &gt; Random points in polygons native:randompointsonlines &gt; Random points on lines native:rasterbooleanand &gt; Raster boolean AND native:rasterize &gt; Convert map to raster native:rasterlayerstatistics &gt; Raster layer statistics native:rasterlayeruniquevaluesreport &gt; Raster layer unique values report native:rasterlayerzonalstats &gt; Raster layer zonal statistics native:rasterlogicalor &gt; Raster boolean OR native:rastersampling &gt; Sample raster values native:rastersurfacevolume &gt; Raster surface volume native:reclassifybylayer &gt; Reclassify by layer native:reclassifybytable &gt; Reclassify by table native:rectanglesovalsdiamonds &gt; Rectangles, ovals, diamonds native:refactorfields &gt; Refactor fields native:removeduplicatesbyattribute &gt; Delete duplicates by attribute native:removeduplicatevertices &gt; Remove duplicate vertices native:removenullgeometries &gt; Remove null geometries native:renamelayer &gt; Rename layer native:renametablefield &gt; Rename field native:repairshapefile &gt; Repair Shapefile native:reprojectlayer &gt; Reproject layer native:rescaleraster &gt; Rescale raster native:retainfields &gt; Retain fields native:reverselinedirection &gt; Reverse line direction native:rotatefeatures &gt; Rotate native:roundrastervalues &gt; Round raster native:ruggednessindex &gt; Ruggedness index native:savefeatures &gt; Save vector features to file native:savelog &gt; Save log to file native:saveselectedfeatures &gt; Extract selected features native:segmentizebymaxangle &gt; Segmentize by maximum angle native:segmentizebymaxdistance &gt; Segmentize by maximum distance native:selectbylocation &gt; Select by location native:serviceareafromlayer &gt; Service area (from layer) native:serviceareafrompoint &gt; Service area (from point) native:setlayerencoding &gt; Set layer encoding native:setlayerstyle &gt; Set layer style native:setmfromraster &gt; Set M value from raster native:setmvalue &gt; Set M value native:setprojectvariable &gt; Set project variable native:setzfromraster &gt; Drape (set Z value from raster) native:setzvalue &gt; Set Z value native:shortestpathlayertopoint &gt; Shortest path (layer to point) native:shortestpathpointtolayer &gt; Shortest path (point to layer) native:shortestpathpointtopoint &gt; Shortest path (point to point) native:shpencodinginfo &gt; Extract Shapefile encoding native:simplifygeometries &gt; Simplify native:singlesidedbuffer &gt; Single sided buffer native:slope &gt; Slope native:smoothgeometry &gt; Smooth native:snapgeometries &gt; Snap geometries to layer native:snappointstogrid &gt; Snap points to grid native:spatialiteexecutesql &gt; SpatiaLite execute SQL native:spatialiteexecutesqlregistered &gt; SpatiaLite execute SQL (registered DB) native:splitfeaturesbycharacter &gt; Split features by character native:splitlinesbylength &gt; Split lines by maximum length native:splitvectorlayer &gt; Split vector layer native:splitwithlines &gt; Split with lines native:stringconcatenation &gt; String concatenation native:stylefromproject &gt; Create style database from project native:subdivide &gt; Subdivide native:sumlinelengths &gt; Sum line lengths native:swapxy &gt; Swap X and Y coordinates native:symmetricaldifference &gt; Symmetrical difference native:taperedbuffer &gt; Tapered buffers native:tinmeshcreation &gt; TIN Mesh Creation native:transect &gt; Transect native:translategeometry &gt; Translate native:truncatetable &gt; Truncate table native:union &gt; Union native:wedgebuffers &gt; Create wedge buffers native:writevectortiles_mbtiles &gt; Write Vector Tiles (MBTiles) native:writevectortiles_xyz &gt; Write Vector Tiles (XYZ) native:zonalhistogram &gt; Zonal histogram native:zonalstatistics &gt; Zonal statistics (in place) native:zonalstatisticsfb &gt; Zonal statistics qgis:advancedpythonfieldcalculator &gt; Advanced Python field calculator qgis:barplot &gt; Bar plot qgis:basicstatisticsforfields &gt; Basic statistics for fields qgis:boxplot &gt; Box plot qgis:checkvalidity &gt; Check validity qgis:climbalongline &gt; Climb along line qgis:concavehull &gt; Concave hull (alpha shapes) qgis:convertgeometrytype &gt; Convert geometry type qgis:definecurrentprojection &gt; Define Shapefile projection qgis:delaunaytriangulation &gt; Delaunay triangulation qgis:distancematrix &gt; Distance matrix qgis:distancetonearesthublinetohub &gt; Distance to nearest hub (line to hub) qgis:distancetonearesthubpoints &gt; Distance to nearest hub (points) qgis:eliminateselectedpolygons &gt; Eliminate selected polygons qgis:executesql &gt; Execute SQL qgis:exportaddgeometrycolumns &gt; Add geometry attributes qgis:findprojection &gt; Find projection qgis:generatepointspixelcentroidsalongline &gt; Generate points (pixel centroids) along line qgis:heatmapkerneldensityestimation &gt; Heatmap (Kernel Density Estimation) qgis:hypsometriccurves &gt; Hypsometric curves qgis:idwinterpolation &gt; IDW interpolation qgis:importintopostgis &gt; Export to PostgreSQL qgis:importintospatialite &gt; Export to SpatiaLite qgis:joinbylocationsummary &gt; Join attributes by location (summary) qgis:keepnbiggestparts &gt; Keep N biggest parts qgis:knearestconcavehull &gt; Concave hull (k-nearest neighbor) qgis:linestopolygons &gt; Lines to polygons qgis:listuniquevalues &gt; List unique values qgis:meanandstandarddeviationplot &gt; Mean and standard deviation plot qgis:minimumboundinggeometry &gt; Minimum bounding geometry qgis:pointsdisplacement &gt; Points displacement qgis:polarplot &gt; Polar plot qgis:postgisexecuteandloadsql &gt; PostgreSQL execute and load SQL qgis:randomextractwithinsubsets &gt; Random extract within subsets qgis:randompointsalongline &gt; Random points along line qgis:randompointsinlayerbounds &gt; Random points in layer bounds qgis:randompointsinsidepolygons &gt; Random points inside polygons qgis:randomselection &gt; Random selection qgis:randomselectionwithinsubsets &gt; Random selection within subsets qgis:rastercalculator &gt; Raster calculator qgis:rasterlayerhistogram &gt; Raster layer histogram qgis:rectanglesovalsdiamondsvariable &gt; Rectangles, ovals, diamonds (variable) qgis:regularpoints &gt; Regular points qgis:relief &gt; Relief qgis:scatter3dplot &gt; Vector layer scatterplot 3D qgis:selectbyattribute &gt; Select by attribute qgis:selectbyexpression &gt; Select by expression qgis:setstyleforrasterlayer &gt; Set style for raster layer qgis:setstyleforvectorlayer &gt; Set style for vector layer qgis:statisticsbycategories &gt; Statistics by categories qgis:texttofloat &gt; Text to float qgis:tilesxyzdirectory &gt; Generate XYZ tiles (Directory) qgis:tilesxyzmbtiles &gt; Generate XYZ tiles (MBTiles) qgis:tininterpolation &gt; TIN interpolation qgis:topologicalcoloring &gt; Topological coloring qgis:variabledistancebuffer &gt; Variable distance buffer qgis:vectorlayerhistogram &gt; Vector layer histogram qgis:vectorlayerscatterplot &gt; Vector layer scatterplot qgis:voronoipolygons &gt; Voronoi polygons .",
            "url": "http://blog.jjsantoso.com/pyqgis-anaconda/",
            "relUrl": "/pyqgis-anaconda/",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducción a PyQGIS (Python + QGIS)",
            "content": "Introducci&#243;n . Últimamente he estado trabajando mucho con QGIS, el software SIG de código libre indispensable para cualquier analista de datos geográficos. El trabajo que realizaba era muy manual (crear y editar polígonos), pero había ciertos pasos que sentía que podía automatizar, así que decidí aprender al menos un poco de PyQGIS para hacer las tareas repetitivas. . PyQGIS es la librería de Python para ejecutar tareas dentro de QGIS. Si has usado QGIS seguro que en más de una vez has visto el logo de Python en la barra de herramientas. Python y QGIS tienen una gran integración y aprender a usar PyQGIS es una inversión de tiempo con rendimientos garantizados. Pese a ello, me parece que no es muy fácil iniciar con PyQGIS, incluso si uno ya sabe Python. PyQGIS y su documentación parece que están diseñados más hacia desarrolladores de plugins que hacia usuarios finales. Si uno viene de utilizar herramientas de análisis geográfico más sencillas como GeoPandas o SF seguramente PyQGIS parece muy burocrático o complicado en un principio, pero como en todo, se trata de entender su lógica y poco a poco uno va descubriendo que también es una herramienta muy completa y muy práctica. . En esta entrada me gustaría mostrar la lógica básica para que alguien que ya tiene conocimentos de la sintaxis de Python y ha trabajado con QGIS pueda empezar a automatizar procesos sencillos con PyQGIS. En particular voy a mostrar un par de ejemplos prácticos aplicando algoritnmos de validación y procesamiento. Aclaro que no soy un experto en QGIS, pero quiero compartir mi aprendizaje al iniciar porque sé que puede haber otras personas en la misma situación. . La consola de Python en QGIS . Voy a trabajar desde la interfaz gráfica de QGIS, por lo que obviamente hay que tener instalado QGIS en la computadora. En otra entrada mostraré cómo instalar QGIS desde Anaconda para trabajar usando JupyterLab. . Una vez instalado y descargado QGIS, abrimos el programa e iniciamos un nuevo proyecto. En la barra de herramientas encontraremos el símbolo de Python. Al hacer clic se abrirá una consola de Python dentro de la ventana de QGIS. Si no ves el ícono, puedes acceder usando el menú en la pestaña complementos &gt;&gt; Consola de Python . Como se ve en la siguiente imagen, la consola se divide en dos secciones, una es la ventana de resultados y la otro es la terminal de Python. . . En la terminal puedes escribir código de Python directamente y este se mostrará en la ventana de resultados, por ejemplo, si escribimos en la terminal: . print(&quot;¡Hola mundo!&quot;) . en la ventana de resultados se imprimirá el comando que escribimos y su resultado . . Escrbir código en la terminal no es muy práctico, por eso es mejor usar el editor de scripts, al que se puede acceder al hacer clic en el ícono superior de una hoja con un lápiz. También (me parece) es mejor trabajar en una ventana separada, para ello hacemos clic en el ícono de la parte superior derecha con dos cuadritos (al lado de la X). . En la nueva configuración, el editor de scripts aparece a la derecha y la ventana de resultados, junto con la terminal, del lado izquierdo. Podemos escribir código en el editor y ejecutarlo haciendo clic en el ícono superior en forma de triángulo verde (o con el atajo de teclado ctrl + shift + e), el resultado se imprime en la ventana de resultados. . Se puede guardar el script con el ícono de disco, o se puede crear uno nuevo al hacer clic en el símbolo $+$ . . B&#225;sicos de PyQGIS . Dentro de la consola de Python tenemos acceso a las librerías con las funciones importantes para poder manipular elementos de QGIS. Vamos a crear un nuevo script que se llame intro_pyqgis_01.py. En el importamos las funciones (constructores, para ser más precisos) QgsProject, QgsVectorLayer del módulo qgis.core. . from qgis.core import QgsProject, QgsVectorLayer . Como sabemos, en QGIS se usan proyectos que contienen capas (layers). Las capas normalmente hacen referencia a archivos con datos geoespaciales, típicamente Shapefiles o GeoJSON, aunque también puede haber capas de otros tipos. Bueno, esa misma estructura la debemos tener en cuenta en PyQGIS. Debemos trabajar sobre un objeto tipo proyecto que contiene objetos de tipo capas. . Vamos a iniciar manualmente un nuevo proyecto de QGIS (proyecto &gt;&gt; Nuevo) y lo guardamos en nuestro equipo con el nombre de intro_pyqgis. . Ahora, vamos al editor de script y creamos una instancia de proyecto a partir del proyecto que actualmente está activo en QGIS: . from qgis.core import QgsProject, QgsVectorLayer proyecto = QgsProject.instance() . Al elecutar lo anterior no veremos que nada cambie, solo definimos el objeto proyecto. Podemos ecribir la siguiente línea en la terminal para verificar el archivo de proyecto que estamos usando: . print(proyecto.fileName()) . Que en mi caso arroja: . &gt; &gt;&gt; &#39;C:/Users/juan.santos/OneDrive/intro_pyqgis/intro_pyqgis.qgz&#39; . Ahora hay que agregarle datos (capas). Para eso usamos la función QgsVectorLayer, que recibe dos argumentos: ruta del archivo geográfico y nombre de la capa. . Los datos que usaremos son publicados por el portal de datos abiertos de la Ciudad de México (CDMX): . colonias de la CDMX | Mercados públicos | . Como los datos pueden cambiar en el futuro, dejo aquí para descarga los que yo usé en particular para este post. . Ahora sí vamos a añadir una capa con los datos de las colonias de la CDMX que se va a llamar &quot;colonias_cdmx&quot;. . ## intro_pyqgis_01.py from qgis.core import QgsProject, QgsVectorLayer # Ruta a la carpeta datos dir_data = &quot;datos&quot; proyecto = QgsProject.instance() # Nombre de la capa nombre_layer_colonias = &quot;colonias_cdmx&quot; # crea la capa layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) # Añade la capa proyecto.addMapLayer(layer_colonias) . Ahora sí veremos que en la ventana principal de QGIS se carga el mapa de colonias. . . ¡Y así de fácil es cargar datos! Sin embargo, debemos de tener cuidado porque si corremos el mismo script varias veces se cargará la misma capa muchas veces, ya que QGIS permite cargar múltiples veces el mismo archivo. . . Para evitar eso, vamos a modificar el script e incluir una pequeña condición que primero verifica si ya existe en el proyecto una capa con el mismo nombre. En caso de que no esxista, entonces carga la capa, si ya existe, entonces no hace nada más. De esta forma podemos ejecutar el archivo intro_pyqgis_01.py tantas veces queramos sin cargar la misma capa. . ## intro_pyqgis_01.py from qgis.core import QgsProject, QgsVectorLayer # Ruta a la carpeta datos dir_data = &quot;datos&quot; proyecto = QgsProject.instance() # Nombre de la capa nombre_layer_colonias = &quot;colonias_cdmx&quot; # crea la capa layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) # verifica si la capa ya existe if not proyecto.mapLayersByName(nombre_layer_colonias): # Si no existe, Añade la capa proyecto.addMapLayer(layer_colonias) . Pues estos (QgsProject y QgsVectorLayer) me parece que son los tipos de objeto más básicos que hay que entender para empezar en PyQGIS (y saber que hay muchos, muchos más tipos de objetos). . Como todo objeto en Python, los proyectos y capas tienen métodos y atributos. De hecho, tienen muchos métodos y atributos, por lo que es dificil saber qué es lo que uno necesita. Ya vimos por ejemplo, un método para obtener el nombre del archivo del proyecto que estamos usando, y también cómo añadir una capa a un proyecto. Si queremos saber todos los métodos disponibles para un objeto projecto podemos consultar la documentación. . De ahí podemos obtener algunos datos útiles para trabajar: . print(&quot;Número de capas en el proyecto&quot;, proyecto.count()) print(&quot;CRS del proyecto&quot;, proyecto.crs().description()) print(&quot;Directorio del proyecto&quot;, proyecto.homePath()) . resultado: . Número de capas en el proyecto 1 CRS del proyecto WGS 84 / UTM zone 14N Directorio del proyecto C:/Users/juan.santos/OneDrive/intro_pyqgis . Igualmente, de las capas podemos encontrar cosas muy interesantes, como el número de filas y columnas que tienen los datos asociados, o el sistema de coordenadas. Consulta aquí la documentación de los objetos QgsVectorLayer . print(&quot;Número de filas:&quot;, layer_colonias.featureCount()) print(&quot;Número de columnas:&quot;, len(layer_colonias.fields())) print(&quot;Sistema de coordenadas:&quot;, layer_colonias.crs().description()) . resultado: . Número de filas: 1815 Número de columnas: 8 Sistema de coordenadas: WGS 84 / UTM zone 14N . Así que ya sabemos que hay 1815 colonias incluídas en este archivo y que tiene 8 columnas. ¿Cuál es el nombre de las columnas y de qué tipo son?: . for f in layer_colonias.fields(): print(f.name(), f.typeName()) . resultado: . ENT String CVEDT String NOMDT String DTTOLOC String CVEUT String NOMUT String POB2010 String ID Integer . También podemos obtener los datos de una fila en particular de la capa con el método .getFeature(). El resultado es un objeto tipo QgsFeature. En este caso por ejemplo obtenemos la fila en la posición 11 del archivo y vemos los datos que contiene. . feature = layer_colonias.getFeature(10) print(feature) print(feature.attributes()) . resultado: . &lt;qgis._core.QgsFeature object at 0x000001A94F2EBB80&gt; [&#39;9&#39;, &#39;2&#39;, &#39;AZCAPOTZALCO&#39;, &#39;05&#39;, &#39;02-013&#39;, &#39;CUITLÃ x81HUAC 1 Y 2 (U HAB)&#39;, &#39;2449&#39;, 11] . Y así, podemos seguir obteniendo más objetos. Por ejemplo del feature anterior podemos obtener la geometría, que es un objeto tipo QgsGeometry: Multipolygon. De la geometría, podemos obtener el centroide, que es un QgsGeometry: Point . geom = feature.geometry() print(geom.centroid()) . resultado: . &lt;QgsGeometry: Point (482445.29628468106966466 2153086.92174857435747981)&gt; . y así le vamos entendiendo de a poco, hasta parece sencillo. . Es muy interesante porque conociendo bien esta lógica podemos integrar muy bien QGIS con las demás herramientas de Python. Por ejemplo, solo por diversión, vamos a hacer una gráfica de la colonia que seleccionamos usando matplotlib, que ya viene instalado dentro del entorno de QGIS. . import matplotlib.pyplot as plt import json coords = json.loads(geom.asJson())[&quot;coordinates&quot;][0][0] coords_centroide = json.loads(geom.centroid().asJson())[&quot;coordinates&quot;] x, y = [[xy[n] for xy in coords] for n in [0, 1]] ax = plt.subplot() ax.plot(x, y) ax.plot([coords_centroide[0]], [coords_centroide[1]], &#39;bo&#39;) ax.text(x=coords_centroide[0], y=coords_centroide[1], s=feature.attribute(&quot;NOMUT&quot;)) ax.figure.show() . . . Así queda el script ìntro_pyqgis_01.py completo: . # ìntro_pyqgis_01.py from qgis.core import QgsProject, QgsVectorLayer proyecto = QgsProject.instance() dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) if not proyecto.mapLayersByName(nombre_layer_colonias): proyecto.addMapLayer(layer_colonias) print(&quot;Número de capas en el proyecto:&quot;, proyecto.count()) print(&quot;CRS del proyecto:&quot;, proyecto.crs().description()) print(&quot;Directorio del proyecto:&quot;, proyecto.homePath()) print(&quot;Número de filas:&quot;, layer_colonias.featureCount()) print(&quot;Número de columnas:&quot;, len(layer_colonias.fields())) print(&quot;Sistema de coordenadas:&quot;, layer_colonias.crs().description()) for f in layer_colonias.fields(): print(f.name(), f.typeName()) # feature = layer_colonias.getFeature(10) print(feature) print(feature.attributes()) # geom = feature.geometry() print(geom.centroid()) import matplotlib.pyplot as plt import json coords = json.loads(geom.asJson())[&quot;coordinates&quot;][0][0] coords_centroide = json.loads(geom.centroid().asJson())[&quot;coordinates&quot;] x, y = [[xy[n] for xy in coords] for n in [0, 1]] ax = plt.subplot() ax.plot(x, y) ax.plot([coords_centroide[0]], [coords_centroide[1]], &#39;bo&#39;) ax.text(x=coords_centroide[0], y=coords_centroide[1], s=feature.attribute(&quot;NOMUT&quot;)) ax.figure.show() . Operaciones con/entre capas . Uno de los usos que le podemos dar a PyQGIS, y en particular fue la razón por la que lo empecé a usar, es para automatizar operaciones con o entre capas. . Por ejemplo, yo estaba creando polígonos manualmente y cada tanto tenía que revisar que no tuvieran errores. Inicialmente hacía esto siguiendo la ruta del menú Vectorial &gt;&gt; Herramientas de Geometría &gt;&gt; Comprobar Validez y seleccionaba las opciones para que el algoritmo se ejecutara. Todo esto era repetitivo y además era medio molesto, porque como resultado de la ejecución siempre añadía 3 capas al proyecto, que me tocaba retirar manualmente. Entonces fue cuando descubrí que podía programar la operación de validación y así solo tenía que ejecutar el script cuando lo necesitara. Luego seguí automatizando otras operaciones que eran repetitivas. . Casi toda operación con capas que se puede hacer usando las opciones del menú de QGIS se puede hacer también usando PyQGIS de una manera relativamente sencilla. Podemos ver todas las operaciones/algoritmos disponibles en el menú Procesos &gt; Caja de herramientas. En la caja de búsqueda podemos buscar alguna operación, por ejemplo, si lo que me interesa es calcular los centroides de una capa escribo centroide y obtengo los algoritmos relacionados: . . Al pasar el cursos por encima de cualquier algoritmo aparece una pequeña nota con el ID del algoritmo en cuestión (native:centroids), lo que nos permitirá identificarlo para trabajar con PyQGIS. . Centroides . Para usar estos algoritmos necesitamos importar la librería processing. Con el ID del algoritmo podemos consultar en la terminal la información sobre los parámetros que espera recibir. Creamos un nuevo script con el nombre intro_pyqgis_02.py y ponemos las siguientes líneas: . # intro_pyqgis_02.py import processing processing.algorithmHelp(&quot;native:centroids&quot;) . que imprimen lo siguiente: . Centroides (native:centroids) Este algoritmo crea una capa de puntos nueva, con puntos que representan el centroide de las geometrías de la capa de entrada. Los atributos asociados a cada punto de la capa de salida son los mismos asociados a los objetos originales. - Input parameters - INPUT: Capa de entrada Parameter type: QgsProcessingParameterFeatureSource Accepted data types: - str: ID de la capa - str: Nombre de capa - str: Fuente de la capa - QgsProcessingFeatureSourceDefinition - QgsProperty - QgsVectorLayer ALL_PARTS: Crear centroide para cada parte Parameter type: QgsProcessingParameterBoolean Accepted data types: - bool - int - str - QgsProperty OUTPUT: Centroides Parameter type: QgsProcessingParameterFeatureSink Accepted data types: - str: archivo vectorial de destino, ej. &#39;d:/test.shp&#39; - str: &#39;memory:&#39; para guardar el resultado en una capa temporal en memoria - str: utilizando prefijo ID del proveedor vectorial y URI de destino, por ejemplo &#39;postgres:...&#39; para guardar el resultado en una tabla PostGIS - QgsProcessingOutputLayerDefinition - QgsProperty - Outputs - OUTPUT: &lt;QgsProcessingOutputVectorLayer&gt; Centroides . Con esta información sabemos que hay tres parámetros que puede recibir como input este algoritmo: . INPUT: Es la capa de la que se van a calcular los centroides. | ALL_PARTS: Indica si se va a crear centroide para cada parte, en caso de que se trate de multipolígonos | OUTPUT: dónde va a guardar el resultado. Puede ser un archivo o una capa temporal. | . Algunos parámetros pueden ser opcionales. Estos se van a pasar a PyQGIS en un diccionario. . Así es como podemos ejecutar este algoritmo sobre la capa de colonias_cdmx: . # intro_pyqgis_02.py from qgis.core import QgsProject, QgsVectorLayer import processing proyecto = QgsProject.instance() dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; if not proyecto.mapLayersByName(nombre_layer_colonias): layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) proyecto.addMapLayer(layer_colonias) params = { &#39;INPUT&#39; : nombre_layer_colonias, &#39;ALL_PARTS&#39; : False, &#39;OUTPUT&#39; :&#39;TEMPORARY_OUTPUT&#39; } out = processing.run(&quot;native:centroids&quot;, params) . Que desafortunadamente nos arroja el siguiente error: . _core.QgsProcessingException: Feature (11) from “colonias_cdmx” has invalid geometry. Please fix the geometry or change the Processing setting to the “Ignore invalid input features” option. . Esto debido a que la capa de colonias tiene errores en su creación :(. En un caso donde la capa no tuviera errores el algoritmo corriera sin problema y el resultado se guadaría en out. . Ahora usemos el algoritmo qgis:checkvalidity para verificar qué está mal con esta capa. . Validaci&#243;n . No voy a ahondar mucho en los detalles de los INPUTS de este algoritmo, pero pueden revisarlo con . processing.algorithmHelp(&quot;qgis:checkvalidity&quot;) . Solo notemos que estoy guardando el resultado en la variable out_validacion y que verifico si el algoritmo detecta características inválidas en la geometría. Si es así, entonces verifico si la capa resultante ya está en el proyecto. Si es así, entonces remuevo la capa existente. Luego, añado la capa que contiene los errores al proyecto. Esta lógica de remover y volver a añadir la capa se justifica porque en teoría yo esperaría que si detecto errores tendría que editar la capa original y luego ir verificando si el error se solucionó. Creamos un nuevo script con el nombre intro_pyqgis_03.py y ponemos las siguientes líneas: . # intro_pyqgis_03.py from qgis.core import QgsProject, QgsVectorLayer import processing proyecto = QgsProject.instance() dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; if not proyecto.mapLayersByName(nombre_layer_colonias): layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) proyecto.addMapLayer(layer_colonias) params_validacion = { &#39;ERROR_OUTPUT&#39; : &#39;TEMPORARY_OUTPUT&#39;, &#39;IGNORE_RING_SELF_INTERSECTION&#39; : False, &#39;INPUT_LAYER&#39; : layer_colonias, &#39;INVALID_OUTPUT&#39; : &#39;TEMPORARY_OUTPUT&#39;, &#39;METHD&#39; : 2, &#39;VALID_OUTPUT&#39; :&#39;TEMPORARY_OUTPUT&#39; } out_validacion = processing.run(&quot;qgis:checkvalidity&quot;, params_validacion) if out_validacion[&#39;INVALID_COUNT&#39;]: layer_error_existente = proyecto.mapLayersByName(out_validacion[&#39;ERROR_OUTPUT&#39;].name()) if layer_error_existente: proyecto.removeMapLayer(layer_error_existente[0]) print(&#39;Puntos erróneos&#39;, out_validacion[&#39;INVALID_COUNT&#39;]) proyecto.addMapLayer(out_validacion[&#39;ERROR_OUTPUT&#39;]) . El resultado se ve así: . . Se añadió al proyecto la capa (temporal) &quot;Salida Errónea&quot;, que tiene 39 puntos señalando dónde hay errores en la capa de &quot;colonias_cdmx&quot;. Como son muchos, no los vamos a corregir ahora. . Pero aún queremos ver el resultado de la operación de centroides a pesar de los errores, así que vamos a usar una opción que nos permite saltarse la validación y ejecutar la operación. Para eso vamos aimportar dos funciones más: QgsProcessingFeatureSourceDefinition y QgsFeatureRequest. . Creamos un nuevo script con el nombre intro_pyqgis_04.py y ponemos las siguientes líneas: . # intro_pyqgis_04.py from qgis.core import QgsProject, QgsVectorLayer, QgsProcessingFeatureSourceDefinition, QgsFeatureRequest, QgsVectorFileWriter import processing proyecto = QgsProject.instance() dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; if not proyecto.mapLayersByName(nombre_layer_colonias): layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) proyecto.addMapLayer(layer_colonias) params = { &#39;INPUT&#39; : QgsProcessingFeatureSourceDefinition(nombre_layer_colonias, selectedFeaturesOnly=False, featureLimit=-1, flags=QgsProcessingFeatureSourceDefinition.FlagOverrideDefaultGeometryCheck, geometryCheck=QgsFeatureRequest.GeometrySkipInvalid), &#39;ALL_PARTS&#39; : False, &#39;OUTPUT&#39; :&#39;TEMPORARY_OUTPUT&#39; } out = processing.run(&quot;native:centroids&quot;, params) layer_centroides = out[&#39;OUTPUT&#39;] if not proyecto.mapLayersByName(layer_centroides.name()): proyecto.addMapLayer(layer_centroides) . ¡Ahora sí se completó el proceso! y así se ve el resultado: . . Por último, si quiero guardar el resultado de la capa de centroides puedo exportarla a un archivo con la siguiente línea: . QgsVectorFileWriter.writeAsVectorFormat(layer_centroides, f&quot;{dir_data}/centroides_colonias.geojson&quot;, &#39;utf-8&#39;, layer_centroides.crs(), &#39;GeoJson&#39;) . Se puede exportar a varios formatos populares como shapefile, GeoJSON o GeoPackage. . Uni&#243;n espacial . Por último voy a dejar otro ejemplo de un procesamiento espacial, en este caso, una unión espacial (spatial join) entre la capa de colonias y una con la ubicación de los mercados públicos de la CDMX. El resultado de esta unión es una capa temporal (Capa Unida) que contiene solo las colonias donde hay al menos un mercado público. . Creamos un nuevo script con el nombre intro_pyqgis_05.py y ponemos las siguientes líneas: . # intro_pyqgis_05.py from qgis.core import QgsProject, QgsVectorLayer, QgsProcessingFeatureSourceDefinition, QgsFeatureRequest, QgsVectorFileWriter import processing proyecto = QgsProject.instance() dir_data = &quot;datos&quot; nombre_layer_colonias = &quot;colonias_cdmx&quot; if not proyecto.mapLayersByName(nombre_layer_colonias): layer_colonias = QgsVectorLayer(f&quot;{dir_data}/colonias_iecm_2019/mgpc_2019.shp&quot;, nombre_layer_colonias) proyecto.addMapLayer(layer_colonias) nombre_layer_mercados = &quot;mercados&quot; if not proyecto.mapLayersByName(nombre_layer_mercados): layer_mercados = QgsVectorLayer(f&quot;{dir_data}/mercados/mercados_publicos.geojson&quot;, nombre_layer_mercados) proyecto.addMapLayer(layer_mercados) params_sjoin = { &#39;INPUT&#39;: QgsProcessingFeatureSourceDefinition(nombre_layer_colonias, selectedFeaturesOnly=False, featureLimit=-1, flags=QgsProcessingFeatureSourceDefinition.FlagOverrideDefaultGeometryCheck, geometryCheck=QgsFeatureRequest.GeometrySkipInvalid), &#39;JOIN&#39;: layer_mercados, &#39;PREDICATE&#39;: 1, &#39;OUTPUT&#39;: &#39;TEMPORARY_OUTPUT&#39;, &#39;DISCARD_NONMATCHING&#39;: True, } out_sjoin = processing.run(&quot;qgis:joinattributesbylocation&quot;, params_sjoin) layer_union = out_sjoin[&#39;OUTPUT&#39;] layer_sjoin_existente = proyecto.mapLayersByName(layer_union.name()) if layer_sjoin_existente: proyecto.removeMapLayer(layer_sjoin_existente[0]) print(&#39;Features resultantes&#39;, out_sjoin[&#39;JOINED_COUNT&#39;]) proyecto.addMapLayer(layer_union) . El resultado es el siguiente: . . Conclusi&#243;n . Como hemos visto, PyQGIS es una herramienta muy poderosa que nos puede ayudar a automatizar procesos al trabajar con QGIS. Puede que la curva de aprendizaje no sea la más fácil, pero con la documentación y preguntas en internet se puede llegar a realizar la mayoría de tareas cotidianas de un analista de datos geográficos. . Por último les dejo algunos recursos que he encontrado y me han parecido muy valiosos al seguir este camino: . El curso de Anita Graser es fantástico: https://anitagraser.com/pyqgis-101-introduction-to-qgis-python-programming-for-non-programmers/ | La documentación oficial: https://docs.qgis.org/3.16/en/docs/pyqgis_developer_cookbook/index.html | Estos videos de José Vivente Pérez Peña: Introducción a PyQGIS y Acceso a features en PyQGIS | No lo comentamos aquí, pero PyQGIS también tiene una interfaz llamada iface para manejar la interfaz visual de QGIS. En este video hay una muy buena introducción: https://gidahatari.com/ih-es/introduccion-a-pyqgis-en-entorno-de-python-dentro-de-qgis | .",
            "url": "http://blog.jjsantoso.com/intro-pyqgis/",
            "relUrl": "/intro-pyqgis/",
            "date": " • Mar 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Mapas de puntos con Python",
            "content": "En esta entrada les cuento sobre este mapa de puntos que hice y tuvo bastante eco en redes sociales. Y a petición de @tomasdamerau, incluyamos a los que no votaron (boletas sobrantes) pic.twitter.com/iB339vc7OE . &mdash; Juan Javier Santos Ochoa (@jjsantoso) June 8, 2021 . Se trata de un mapa de puntos, también conocidos como mapa de distribución de puntos o mapa de densidad de puntos. En este tipo de mapa los polígonos de cada región se rellenan de puntos que representan una cantidad fija. Cada punto puede representar una sola observación o varias, lo importante es que ese valor se mantenga fijo y no varíe, como pudiera ser en un mapa de símbolos proporcionales. Cuando los puntos tienen el mismo valor es mucho más fácil hacer comparaciones. Normalmente la ubicación de cada punto dentro de la región que le corresponde no indica la ubicación exacta y muchas veces simplemente se distribuyen de forma aleatoria dentro del polígono. . A continuación describiré por qué me parecio que este mapa era adecuado en el contexto de la discusión de la polarización política de la Ciudad de México. Luego mostraré cómo fue que hice este mapa (para esta parte se necesita tener conocimientos de Python y las librerías Numpy, Pandas y Geopandas). Por último finalizaré con algunas reflexiones sobre este ejercicio. . Motivaci&#243;n . Tras los primeros resultados de la jornada electoral del 6 de junio empezaron a circular varios mapas, mostrando los ganadores de las alcaldías en la Ciudad de México. . . Estos mostraban que la ciudad quedó dividida en poniente/oriente por las dos principales fuerzas políticas. El poniente &quot;de derecha&quot; versus el oriente de &quot;izquierda&quot;. Para quienes no conocen la CDMX, además hay un fuerte componente socioeconómico en esta división que se creó, como bien ha documentado Máximo Jaramillo. Se empezó a hablar del &quot;muro de Berlín&quot;, pero entre meme y meme se empezó a asomar el clasismo/racismo de varios. Se empezaron a sacar conclusiones simplistas de que los del poniente son X, mientras los de oriente son Y. . Varios advirtieron que la división realmente no era tan dramática como parecía sugerir el mapa, porque en muchas alcaldías la victoria estuvo reñida. Pero eso no se podía ver en el mapa de la división, ahí solo cabe el ganador. Ese es un primer problema de los mapas coropléticos, no permiten mostrar más de una variable a la vez, para cada polígono solo podemos ver un solo valor (o color). Hay mapas bivariados, pero su alcance es limitado (y a mí personalmente no me gustan mucho). Mi primer objetivo entonces era poder mostrar que el voto no estaba totalmente inclinado hacia un lado u otro, sino que había diversidad en todas las alcaldías. Eso se puede mostrar usando otros tipos de gráficos, por ejemplo gráficos de barras, pero sentía que el impacto de los primeros mapas provenía de la división geográfica y que otro tipo de gráfica no reflejaba eso, entonces necesitaba un mapa para vencer a otro mapa. La división poniente/oriente existe, pero hay que dimensionarla en sus justas proporciones. . Un segundo problema de los mapas coropléticos es que el área causa una distorsión en la percepción porque las áreas más grandes sobresalen. Y en mapas de resultados electorales eso afecta mucho el mensaje, porque el dominio territorial de áreas grandes parece indicar mayor dominio político. El típico problema de &quot;La tierra no vota, las personas sí&quot;. Enrique Tejada lo explica mejor en su blog. . . Con eso en mente, recordé que sensei Segasi había publicado alguna vez un mapa de puntos y me pareció que esa era una buena forma de representarlo: permitía mostrar la diversidad del voto y no exageraba la participación política de ningún partido. Sin embargo, los mapas de puntos normalmente distribuyen los puntos de manera aleatoria dentro del polígono, y eso me parece algo confuso, porque no permite dimensionar claramente qué partido tiene más o menos votos, además de que uno puede pensar que la ubicación del punto corresponde a una posición real, cuando no es así. Pensé entonces que necesitaba un mapa de puntos en el que yo pudiera definir la ubicación de los puntos de forma contigua para que fuera más fácil hacer comparaciones. . Esta forma de visualizar me parece tiene varias ventajas: . Permite dimensionar la participación electoral: quiénes votaron y los que no. | Permite ver la distribución del voto entre los distintos partidos dentro de cada alcaldía. | Reduce sustancialmente el sesgo por tamaño de la alcaldía. | . También es necesario ser conscientes de las desventajas o sesgos que puede haber en este tipo de mapa: . No es adecuado si hay polígonos que son relativamente muy pequeños, porque no sería posible ver adecuadamente los puntos al interior. Por eso sería difícil hacer un mapa similar a nivel colonia o sección electoral. | Las alcaldías más pequeñas pueden parecer que tienen más participación porque se ven más &quot;rellenas&quot; de puntos. | La posición de los puntos no indica nada, pero es posible que algunos lo tomen como la ubicación de las casillas. | El espacio en blanco sobrante (donde no hay puntos) puede confundirse con abstención. | . Haciendo el balance, me parece que las ventajas son mayores a las desventajas para este problema particular. . Así fue como llegué a la idea del mapa, ahora el reto era hacerlo, porque hasta donde yo sabía no hay un programa o librería que los hiciera directamente. Me puse a experimentar con Python y llegué a lo que sigue a continuación. . Elaboraci&#243;n del mapa . Lo que sigue es bastante técnico y se requiere conocer bien las librerías más comunes de análisis de datos dentro del ecosistema de Python. . Lo primero que necesitamos para llegar al mapa de puntos (además de importar las librerías) es poder rellenar cada polígono con puntos. La idea a la que llegue para hacerlo es la siguiente: . Obtener los bordes (inferior izquierdo, superior derecho) de cada polígono. | Hacer una cuadrícula de puntos equidistantes usando los bordes. | Seleccionamos solo los puntos de la cuadrícula que están contenidos por el polígono original. | Veamos paso a paso cómo se hace esto. . from math import ceil import sys import geopandas as gpd import matplotlib.pyplot as plt import pandas as pd import numpy as np print(sys.version) print(pd.__name__, pd.__version__) print(gpd.__name__, gpd.__version__) print(np.__name__, np.__version__) print(plt.matplotlib.__name__, plt.matplotlib.__version__) . 3.6.10 |Anaconda, Inc.| (default, Jan 7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] pandas 0.24.2 geopandas 0.6.3 numpy 1.18.1 matplotlib 3.2.1 . Leemos el mapa de alcaldías de la CDMX. Se puede descargar aquí. . cdmx = gpd.read_file(&#39;datos/mapa_mexico/&#39;) .query(&#39;CVE_EDO==&quot;09&quot;&#39;) .set_index(&#39;CLAVE&#39;) cdmx.boundary.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x22520cc9b70&gt; . Vamos a seleccionar solo una alcaldía para ilustrar el proceso . poly_index = &#39;09005&#39; poly = cdmx.loc[poly_index, &#39;geometry&#39;] poly . Obtengamos los bordes de este polígono: . x0, y0, x1, y1 = poly.bounds print(x0, y0, x1, y1) . -99.18007062292732 19.44414241268417 -99.05068100445773 19.59186051804424 . Vamos a crear una cuadrícula de puntos que estén separados a una distancia de 0.005 unidades decimales. Creamos la cuadrícula usando Numpy, luego la transformamos en un GeoDataframe para que sea posible hacer la unión espacial. . Warning: la distancia depende del sistema de coordenadas geográficas que estén usando. En este caso yo tengo WGS84 que usa coordenadas planas. . distancia = 0.005 # Cuadrícula de puntos X, Y = np.meshgrid(np.arange(x0, x1, distancia), np.arange(y0, y1, distancia)) # lo convertimos en geodataframe df_puntos_alc = pd.DataFrame(np.array([X.flatten(), Y.flatten()]).T, columns=[&#39;X&#39;, &#39;Y&#39;]) .assign(CLAVE=poly_index) .pipe(lambda df: gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[&#39;X&#39;], df[&#39;Y&#39;]))) # así se ve df_puntos_alc.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x22520ccaac8&gt; . ¿Cómo sabemos cuáles de estos puntos caen dentro del área del polígono poly? Fácil, usamos el método .within de los GeodataFrames para saber cuáles están dentro. . df_puntos_alc = df_puntos_alc.loc[df_puntos_alc.within(poly)] df_puntos_alc.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x22520cca198&gt; . Ahora lo hacemos para todas las alcaldías usando un loop. El resultado se guarda en el GeoDataframe df_xy . lista_df_puntos = list() for poly_index in cdmx.index: poly = cdmx.loc[poly_index, &#39;geometry&#39;] x0, y0, x1, y1 = poly.bounds x = np.arange(x0, x1, 0.005) y = np.arange(y0, y1, 0.005) X,Y = np.meshgrid(x, y) df_puntos_alc = pd.DataFrame(np.array([X.flatten(), Y.flatten()]).T, columns=[&#39;X&#39;, &#39;Y&#39;]) .assign(CLAVE=poly_index) .pipe(lambda df: gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[&#39;X&#39;], df[&#39;Y&#39;]))) .loc[lambda df: df.within(poly)] lista_df_puntos.append(df_puntos_alc.iloc[::-1]) df_xy = pd.concat(lista_df_puntos, axis=0) df_xy.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} df_xy.tail() . X Y CLAVE geometry . 104 -99.072886 | 19.058221 | 09009 | POINT (-99.07289 19.05822) | . 103 -99.077886 | 19.058221 | 09009 | POINT (-99.07789 19.05822) | . 64 -99.057886 | 19.053221 | 09009 | POINT (-99.05789 19.05322) | . 63 -99.062886 | 19.053221 | 09009 | POINT (-99.06289 19.05322) | . 62 -99.067886 | 19.053221 | 09009 | POINT (-99.06789 19.05322) | . Con esto cumplimos la primera parte. La siguiente es determinar cuántos votos debería representar cada punto. Para eso, necesito tener los resultados de la votación por alcaldía. Los datos los descargue del PREP. Así lucen: . prep_cdmx = pd.read_csv(&#39;datos/prep_cdmx/CDMX_ALC_2021.csv&#39;, header=4) prep_cdmx.head(3) . CLAVE_CASILLA CLAVE_ACTA ID_ESTADO ESTADO ID_DISTRITO ALCALDIA SECCION ID_CASILLA TIPO_CASILLA EXT_CONTIGUA ... OBSERVACIONES CONTABILIZADAS MECANISMOS_TRASLADO SHA FECHA_HORA_ACOPIO FECHA_HORA_CAPTURA FECHA_HORA_VERIFICACION ORIGEN DIGITALIZACION TIPO_DOCUMENTO . 0 090056B0100 | 090056B0100ALC | 9 | CIUDAD DE MEXICO | 3 | AZCAPOTZALCO | 56 | 1 | B | 0 | ... | NaN | 1 | F | 043b1e8ffdf059edb68828172f271400111a56b3e769c3... | 2021-06-07 04:11 | 07/06/2021 06:40:29 a.m. | 07/06/2021 10:26:14 a.m. | CATD | ESCANER | ACTA_PREP | . 1 090056C0100 | 090056C0100ALC | 9 | CIUDAD DE MEXICO | 3 | AZCAPOTZALCO | 56 | 1 | C | 0 | ... | NaN | 1 | F | ba2338cdbabd8ec0b1f53620f704cb94f1d408d668eae4... | 2021-06-07 01:36 | 07/06/2021 10:17:50 a.m. | 07/06/2021 10:18:19 a.m. | CATD | ESCANER | ACTA_PREP | . 2 090056C0200 | 090056C0200ALC | 9 | CIUDAD DE MEXICO | 3 | AZCAPOTZALCO | 56 | 2 | C | 0 | ... | NaN | 1 | F | 2ec62e039cddb9850477535904f4523e2be055147516f0... | 06/06/2021 10:23:32 p.m. | 06/06/2021 10:23:32 p.m. | 06/06/2021 10:26:07 p.m. | CASILLA | MOVIL | ACTA_PREP | . 3 rows × 58 columns . Los resultados vienen a nivel casilla, así que hay que agregarlos por alcaldía. También hay que sumar los votos de las coaliciones y crear una variable que contenga la clave INEGI de cada municipio. Todo eso se hace a continuación: . dicc_alc_cve = { &#39;ALVARO OBREGON&#39;: &#39;09010&#39;, &#39;AZCAPOTZALCO&#39;: &#39;09002&#39;, &#39;BENITO JUAREZ&#39;: &#39;09014&#39;, &#39;COYOACAN&#39;: &#39;09003&#39;, &#39;CUAJIMALPA DE MORELOS&#39;: &#39;09004&#39;, &#39;CUAUHTEMOC&#39;: &#39;09015&#39;, &#39;GUSTAVO A. MADERO&#39;: &#39;09005&#39;, &#39;IZTACALCO&#39;: &#39;09006&#39;, &#39;IZTAPALAPA&#39;: &#39;09007&#39;, &#39;LA MAGDALENA CONTRERAS&#39;: &#39;09008&#39;, &#39;MIGUEL HIDALGO&#39;: &#39;09016&#39;, &#39;MILPA ALTA&#39;: &#39;09009&#39;, &#39;TLAHUAC&#39;: &#39;09011&#39;, &#39;TLALPAN&#39;: &#39;09012&#39;, &#39;VENUSTIANO CARRANZA&#39;: &#39;09017&#39;, &#39;XOCHIMILCO&#39;: &#39;09013&#39; } alianza_pan = [&#39;PAN&#39;, &#39;PRI&#39;, &#39;PRD&#39;, &#39;PAN_PRI_PRD&#39;, &#39;PAN_PRI&#39;, &#39;PAN_PRD&#39;, &#39;PRI_PRD&#39;] alianza_morena = [&#39;PVEM&#39;, &#39;PT&#39;,&#39;MORENA&#39;, &#39;PVEM_PT_MORENA&#39;, &#39;PVEM_PT&#39;, &#39;PVEM_MORENA&#39;, &#39;PT_MORENA&#39;] prep_alc = prep_cdmx[alianza_morena + alianza_pan + [&#39;TOTAL_VOTOS_ASENTADOS&#39;, &#39;LISTA_NOMINAL&#39;]].apply(pd.to_numeric, errors=&#39;coerce&#39;) .assign(alianza_pan_pri_prd=lambda x: x[alianza_pan].sum(axis=1), alianza_morena_pt_pvem=lambda x: x[alianza_morena].sum(axis=1), no_voto=lambda x: x[&#39;LISTA_NOMINAL&#39;]-x[&#39;TOTAL_VOTOS_ASENTADOS&#39;] ) .join(prep_cdmx[&#39;ALCALDIA&#39;]) .groupby(&#39;ALCALDIA&#39;, as_index=False).sum() .assign(CLAVE=lambda x: x[&#39;ALCALDIA&#39;].map(dicc_alc_cve)) prep_alc.head() . ALCALDIA PVEM PT MORENA PVEM_PT_MORENA PVEM_PT PVEM_MORENA PT_MORENA PAN PRI ... PAN_PRI_PRD PAN_PRI PAN_PRD PRI_PRD TOTAL_VOTOS_ASENTADOS LISTA_NOMINAL alianza_pan_pri_prd alianza_morena_pt_pvem no_voto CLAVE . 0 ALVARO OBREGON | 5617.0 | 4235.0 | 99544.0 | 716.0 | 54.0 | 242.0 | 867.0 | 106448.0 | 43975.0 | ... | 5213.0 | 739.0 | 365.0 | 207.0 | 327271.0 | 607719 | 182181.0 | 111275.0 | 278460.0 | 09010 | . 1 AZCAPOTZALCO | 5346.0 | 2785.0 | 71225.0 | 0.0 | 0.0 | 0.0 | 898.0 | 57933.0 | 21612.0 | ... | 2048.0 | 357.0 | 108.0 | 38.0 | 200058.0 | 381398 | 86337.0 | 80254.0 | 171959.0 | 09002 | . 2 BENITO JUAREZ | 1620.0 | 1524.0 | 44052.0 | 0.0 | 0.0 | 0.0 | 700.0 | 157178.0 | 13086.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 230967.0 | 373323 | 172336.0 | 47896.0 | 131402.0 | 09014 | . 3 COYOACAN | 5371.0 | 4299.0 | 110098.0 | 0.0 | 0.0 | 0.0 | 1353.0 | 105089.0 | 37422.0 | ... | 4908.0 | 625.0 | 255.0 | 143.0 | 316558.0 | 559122 | 168872.0 | 121121.0 | 226813.0 | 09003 | . 4 CUAJIMALPA DE MORELOS | 2501.0 | 1670.0 | 26501.0 | 0.0 | 0.0 | 0.0 | 407.0 | 23736.0 | 44779.0 | ... | 1471.0 | 281.0 | 29.0 | 100.0 | 112968.0 | 178825 | 72304.0 | 31079.0 | 64446.0 | 09004 | . 5 rows × 21 columns . Vamor ahora a unir los resultados de la votación con el GeoDataframe de las alcaldías de la ciudad. . cdmx = gpd.read_file(&#39;datos/mapa_mexico/&#39;) .query(&#39;CVE_EDO==&quot;09&quot;&#39;) .merge(prep_alc, on=&#39;CLAVE&#39;) .set_index(&#39;CLAVE&#39;) cdmx.head(2) . NOM_MUN NOMEDO CVE_EDO CVE_MUNI Area geometry ALCALDIA PVEM PT MORENA ... PRD PAN_PRI_PRD PAN_PRI PAN_PRD PRI_PRD TOTAL_VOTOS_ASENTADOS LISTA_NOMINAL alianza_pan_pri_prd alianza_morena_pt_pvem no_voto . CLAVE . 09005 Gustavo A. Madero | D.F. | 09 | 005 | 87.445402 | POLYGON ((-99.11705 19.58909, -99.11748 19.588... | GUSTAVO A. MADERO | 14623.0 | 10753.0 | 219742.0 | ... | 47959.0 | 4850.0 | 864.0 | 421.0 | 402.0 | 527327.0 | 1051850 | 212862.0 | 248042.0 | 519352.0 | . 09002 Azcapotzalco | D.F. | 09 | 002 | 33.420781 | POLYGON ((-99.15688 19.50231, -99.15684 19.502... | AZCAPOTZALCO | 5346.0 | 2785.0 | 71225.0 | ... | 4241.0 | 2048.0 | 357.0 | 108.0 | 38.0 | 200058.0 | 381398 | 86337.0 | 80254.0 | 171959.0 | . 2 rows × 26 columns . El número de votos que vale cada punto es un parámetro clave que se establece un poco por ensayo y error, no hay un número predeterminado. En este post dan algunos consejos a tener en cuenta. Lo principal es que hay que cuidar el balance entre número y tamaño de los puntos para que ningún área se vea demasiado densa o que los puntos sean tan pequeños que no sea posible identificar claramente su color. Para encontrar el número aproximado de puntos lo que hice fue dividir el padrón electoral de cada alcaldía entre el número de puntos de la cuadrícula con forma de polígono. De entre todas las alcaldías me fijé en el valor mínimo. Eso garantiza que al menos para la delegación más pequeña habrá suficientes puntos para que toda su población esté representada. En este caso me dio que cada punto debería valer al menos 4217.3 votos. Cuaqluier valor por encima de eso nos serviría. Escogí finalmente 5,000 porque me parece que logra un buen balance entre visibilidad sin hacer que se vean áreas demasiado densas. . aprox_voto_punto = df_xy.groupby(&#39;CLAVE&#39;)[[&#39;Y&#39;]].count().join(cdmx[[&#39;TOTAL_VOTOS_ASENTADOS&#39;, &#39;no_voto&#39;]]) .assign(voto_por_punto=lambda x: x[&#39;TOTAL_VOTOS_ASENTADOS&#39;].add(x[&#39;no_voto&#39;]).div(x[&#39;Y&#39;])) .get(&#39;voto_por_punto&#39;).max() print(&#39;Número aproximado de votos que debería valer cada punto&#39;, aprox_voto_punto) voto_x_punto_participacion = 5000 print(&#39;Número de votos que vale cada punto seleccionado&#39;, voto_x_punto_participacion) . Número aproximado de votos que debería valer cada punto 4217.3 Número de votos que vale cada punto seleccionado 5000 . Lo que sigue ahora es determinar cuántos puntos en total deben quedarse en cada alcaldía, cuántos van a estar coloreados para cada coalición y las personas que no votaron. Hagámoslo para una delegación: . poly_index = &#39;09005&#39; puntos_novoto = int(ceil(cdmx.loc[poly_index, &#39;no_voto&#39;] / voto_x_punto_participacion)) puntos_morena = int(ceil(cdmx.loc[poly_index, &#39;alianza_morena_pt_pvem&#39;] / voto_x_punto_participacion)) puntos_pan = int(round(cdmx.loc[poly_index, &#39;alianza_pan_pri_prd&#39;] / voto_x_punto_participacion)) puntos_total_partidos = int(ceil(cdmx.loc[poly_index, &#39;TOTAL_VOTOS_ASENTADOS&#39;] / voto_x_punto_participacion)) puntos_otros = puntos_total_partidos - puntos_morena - puntos_pan print(puntos_novoto, puntos_morena, puntos_pan, puntos_otros) . 104 50 43 13 . Así tengo, por ejemplo, que en la alcaldía Gustavo A. Madero voy a usar en total 210 puntos distribuídos de la siguiente manera: . 104 no votaron | 50 a la coalición de MORENA, PT, PVEM | 43 a la coalición de PAN, PRI, PRD | 13 a otros partidos/candidatos independientes | . Ya solo queda graficar. Así se va haciendo partido a partido: . fig, ax = plt.subplots() cdmx.loc[[poly_index]].boundary.plot(color=&#39;k&#39;, ax=ax) df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[:puntos_morena].plot(ax=ax, markersize=10, color=&#39;C3&#39;, label=&#39;Alianza MORENA-PT-PVEM&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x22523861470&gt; . df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_morena: puntos_morena + puntos_pan].plot(ax=ax, markersize=10, color=&#39;C0&#39;, label=&#39;Alianza PAN-PRI-PRD&#39;) fig . &lt;Figure size 432x288 with 0 Axes&gt; . df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_morena + puntos_pan: puntos_total_partidos].plot(ax=ax, markersize=10, color=&#39;gray&#39;, label=&#39;Otros&#39;) fig . &lt;Figure size 432x288 with 0 Axes&gt; . df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_total_partidos: puntos_total_partidos + puntos_novoto].plot(ax=ax, markersize=10, facecolor=&#39;yellow&#39;, edgecolor=&#39;gray&#39;, label=&#39;No votaron&#39;) fig . &lt;Figure size 432x288 with 0 Axes&gt; . Pues ya, un loop que lo haga para todas las alcaldías: . fig, ax = plt.subplots(figsize=(12, 12)) for poly_index in cdmx.index: puntos_total_partidos = int(ceil(cdmx.loc[poly_index, &#39;TOTAL_VOTOS_ASENTADOS&#39;] / voto_x_punto_participacion)) puntos_novoto = int(ceil(cdmx.loc[poly_index, &#39;no_voto&#39;] / voto_x_punto_participacion)) puntos_morena = int(ceil(cdmx.loc[poly_index, &#39;alianza_morena_pt_pvem&#39;] / voto_x_punto_participacion)) puntos_pan = int(round(cdmx.loc[poly_index, &#39;alianza_pan_pri_prd&#39;] / voto_x_punto_participacion)) puntos_otros = puntos_total_partidos - puntos_morena - puntos_pan cdmx.loc[[poly_index]].boundary.plot(color=&#39;k&#39;, ax=ax) df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[:puntos_morena].plot(ax=ax, markersize=10, color=&#39;C3&#39;, label=&#39;Alianza MORENA-PT-PVEM&#39;) df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_morena: puntos_morena + puntos_pan].plot(ax=ax, markersize=10, color=&#39;C0&#39;, label=&#39;Alianza PAN-PRI-PRD&#39;) df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_morena + puntos_pan: puntos_total_partidos].plot(ax=ax, markersize=10, color=&#39;gray&#39;, label=&#39;Otros&#39;) df_xy.query(&#39;CLAVE==@poly_index&#39;).iloc[puntos_total_partidos: puntos_total_partidos + puntos_novoto].plot(ax=ax, markersize=10, facecolor=&#39;yellow&#39;, edgecolor=&#39;gray&#39;, label=&#39;No votaron&#39;) handles, labels = ax.get_legend_handles_labels() ax.legend(handles[0:4], labels[0:4]) ax.set_axis_off() ax.set_title(&#39;Voto a las alcaldías de la Ciudad de México, 2021&#39;, pad=5, fontsize=15) ax.annotate(s=f&#39;Nota: cada punto equivale a {voto_x_punto_participacion:,.0f} votos nElaborado por @jjsantoso con datos del PREP, IECM&#39;, xy=(0, 50), xycoords=&#39;axes points&#39;, va=&#39;top&#39;, fontsize=10) fig.savefig(&#39;graficas/voto_alcaldia_cdmx__participacion2021.png&#39;, dpi=600, bbox_inches=&#39;tight&#39;) . ¡Listo! . Reflexiones finales . El mapa tuvo muy buen recibimiento y creo que la mayoría de personas entendió el mensaje que quería transmitir: la ciudad no está tan divida como nos decían que estaba. Algunos comentaron que era difícil conocer el total de votos de cada alcaldía, lo cual es cierto. Otros sugirieron otros tipos de análisis que se alejaban un poco del propósito inicial. Como en cualquier problema complejo, una sola visualización no puede ofrecer el panorama completo y es necesario acompañar cualquier análisis de más datos y distintos tipos de visualizaciones. Cada una puede mostrar una faceta del problema de estudio y juntas llegan a una conclusión mucho mejor. . Aunque por todo lo que dije antes parece que no me gustan los mapas coropléticos, eso no es para nada cierto, yo los uso mucho también. Cada mapa tiene su uso, depende del mensaje que se quiera transmitir. Debemos ser conscientes de las fortalezas y debilidades de cada forma de visualizar y ser cuidadosos para que, aunque tengamos las mejores intenciones, no terminemos mandando el mensaje equivocado. . En las muy reñidas elecciones presidenciales de Perú anda circulando este mapa. ¿Qué les decimos? . . Si les interesa hacer mapas en Python también escribí esta otra entrada en mi blog: . Recuadros para mapas en Geopandas | . Aquí pueden ver las demás entradas que he escrito: https://blog.jjsantoso.com/ .",
            "url": "http://blog.jjsantoso.com/mapas-distribucion-puntos/",
            "relUrl": "/mapas-distribucion-puntos/",
            "date": " • Jun 11, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Introducción a bases de datos relacionales y SQL para científicos sociales",
            "content": "Introducci&#243;n . En ciencias sociales trabajamos mucho con datos para hacer investigación cuantitativa. Casi siempre, nuestros inicio en el manejo de datos se da en cursos de estadística, por lo que las funciones que necesitamos aprender son leer, procesar y reestructurar datos para luego visualizar variables y estimar modelos estadísticos. Como parte de nuestra práctica, normalmente guardamos la información en formatos de archivos como XLSX (Excel), CSV (comma separated values), TXT (texto plano), DTA (Stata), SAV (SPSS), Rdata (R) y trabajamos con Software de análisis de datos como Stata, R, SPSS, Python, SAS, entre otros. Además, solemos estructurar nuestra información de forma tal que toda quede en una única tabla. Llamemos a estas practicas como la &quot;forma usual de trabajar&quot;. . Esta forma usual de trabajar tiene ventajas para nosotros porque nos facilita concentrarnos en procesar y analizar datos, en lugar de preocuparnos por la administración de la información. La mayoría de las veces se trata de tareas que solo ejecutaremos unas pocas veces en nuestra computadora y en las que no es necesario que otras personas puedan acceder a nuestros datos. Además, suele ser información que no se actualiza de forma continua y de una escala relativamente pequeña. Sin embargo, cuando nos enfrentamos a alguna de las situaciones que acabdo de describir anteriormente, ya no es tan conveniente seguir este flujo de trabajo. Pese a que existen desde hace mucho otras formas de trabajar cuando el proceso de manejo de datos se vuelve más complejo, normalmente desconocemos por completo otras alternativas. Una de esas alternativas son las bases de datos relacionales. . A lo largo de esta sesión veremos cuáles son las situaciones en las que podemos considerar el uso de base de datos relacionales, cuáles son sus características y cómo podemos consultarlas. Empecemos primero viendo con un poco más de detalles las limitantes en la forma usual de trabajar . . Limitantes de la forma usual de trabajar . Dificultad para leer y actualizar los datos: Los archivos planos (csv, txt) o binarios (Excel, Rdata, dta, etc) son difíciles de leer y actualizar. Siempre que se leen se debe abrir el archivo completo, aunque solo se necesite trabajar con unas pocas observaciones. Además, para agregar nuevos registros se requiere primero cargar todos los datos en memoria y luego volverlos a escribir nuevamente al disco, lo que es un proceso computacionalmente muy costoso. . | Imposibilidad para modificar por varios usuarios: Estos archivos normalmente solo están disponibles de forma local (en nuestra computadora), por lo que solo pueden ser usados por un usuario a la vez. Una vez un usuario hace modificaciones, es difícil mantener a los demás usuarios con la información actualizada. Aún peor, si varios usuarios modifican los datos de forma individual, luego es muy difícil conciliar las diferentes versiones. Si bien existen alternativas en la nube (hojas de cálculo de Google, Excel online), estas están diseñadas para trabajar de forma interactiva, no programática. . | No hay validación de datos: no hay una forma fácil de validar o aplicar restricciones sobre los datos nuevos que se ingresan en la base de datos. Una validación puede ser, por ejemplo, si tengo una variable numérica, no puedo guardar ahí un dato que es de texto, u otro ejemplo es verificar que un nuevo valor que voy a ingresar esté dentro de un catálogo de opciones predeterminadas. . | Falta de autenticación y permisos: no hay mecanismos para controlar qué usuarios pueden acceder a la información y qué facultades tienen para modificarla. . | No asegura atomicidad de las transacciones : La atomicidad se refiere a que al hacer un conjunto de operaciones en una base de datos, ante un fallo el resultado no puede quedar a medias, el proceso se hace completo o no se hace nada. . | Los datos deben caber en memoria RAM: la mayoría de software estadísticos deben leer los datos y guardarlo en la memoria RAM para poder trabajar con ellos, por lo que hay un límite sobre la cantidad de información que se puede analizar. . | Redundancia: cuando se intenta que toda la información de un proyecto esté en una sola tabla de datos, muchas veces es necesario repetir la información. . | . Todas estas limitaciones pueden ser abordadas usando sistemas de administración de bases de datos. . Sistemas de administraci&#243;n de bases de datos. . Antes que nada, hay que aclarar algo. En ciencias sociales le llamamos base de datos a cualquier archivo que contenga datos, pero tecnicamente esto no es correcto. Los ingenieros de sistemas entienden algo más complejo cuando se habla de bases de datos. Hay cierta ambiguedad en el término, pero cuando hablamos de bases de datos normalmente hacemos referencia a los Sistemas de Administración de Bases de Datos (DBMS, por sus siglas en inglés) que son sitemas que simplifican la administración de los datos de una forma organizada y consistente. Otras veces el término base de datos puede hacer referencia a un conjunto de datos guardado en un DBMS. Aunque se suelen usar indistintamente, tratemos de dar una definción a cada concepto: . Base de datos: es una colección estructurada de datos sobre entidades y sus relaciones. Modela objetos de la vida real -entidades y relaciones- y captura su estructura en formas que permiten que estas entidades y relaciones puedan ser consultadas para hacer análisis. . | Sistema de administración de bases de datos (DBMS): es un software diseñado para guardar bases de datos de forma segura y manejarlas eficientemente, además de cumplir con otras tareas de mantenimiento y consulta de las entidades y relaciones que la base de datos representa. Hay muchos tipos de DBMS que tienen funciones especiales para diferentes casos de uso. . | . Bases de datos relacionales . En una base de datos relacional tenemos tablas que representan distintas entidades y relaciones que indican relaciones entre los objetos guardados en las tablas. . Una tabla es un arreglo en filas y columnas, donde cada fila representa una observación y cada columna es un atributo de las observaciones. Es como una hoja de excel. Las columnas, también conocidas como campos, tienen un nombre y un tipo, que indica el tipo de información que se puede guardar (número, texto, fecha, etc). Todos los datos de una columna deben ser del mismo tipo. | Toda tabla debería tener una columna que es conocida como la llave primaria (primary key o PK) que debe tener un valor único dentro de la tabla y permite identificar a cada observación. Usualmente es un número entero. | . | Las relaciones son vínculos entre un objeto de una tabla y uno o varios objetos en otras tablas. La relación entre tablas permite guardar estructuras de datos muy complejas de forma mucho más sencillas. | Hay varios tipos de relaciones: uno a uno, uno a muchos y muchos a muchos. | Cuando se crea una relación se genera un llave externa (Foreign Key o FK) en la tabla. | . | . A la información que comprende las tablas, los campos de las tablas junto con el tipo de dato y las relaciones entre las tablas se le conoce como el esquema de la base de datos. Toda esta información se puede de comprender de forma mucho más fácil mediante un diagrama conocido como diagrama Entidad-Relación. . Es muy importante cuando se trabaja con una base de datos pensar en cada tabla como una entidad, esto es como si fuera el objeto independiente más sencillo con el que podemos trabajar, y luego crear las relaciones entre las entidades. . Como ejemplo de estos conceptos pensemos en una escuela donde se ofrece cursos a los alumnos, y estos cursos son dictados por profesores. Tenemos entonces tres entidades: cursos, profesores y estudiantes. Cada una de estas entidades es una tabla que tiene distintos atributos y además hay una relación clara entre ellas: todo curso tiene un profesor y uno o varios alumnos. El diagrama entidad-relación es el siguiente: . . Las tablas con los datos se verían de la siguiente manera: . . Structured Query Language (SQL) . SQL (pronunciado como sicuel o ese-cu-ele) es un lenguaje para manejar bases de datos. Sirve para crear, modificar y eliminar bases de datos, tablas, campos, datos, así como también para consultar los elementos dentro de una base de datos. SQL es realmente un estándar, ya que cada DBMS tiene su propio lenguaje y funciones, sin embargo, casi todas las bases de datos relacionales soportan las características básicas de SQL, aunque con ligeras variaciones. . Este tutorial trataremos más adelante cómo hacer consultas de información con SQL. En otro se tratará la definición de bases de datos. . SQL vs NoSQL . Los DBMS se suelen clasificar en dos grandes grupos: SQL y NoSQL. Cuando los datos con los que trabajamos son estructurados es decir, tienen un equema que puede ser facilmente representados en tablas con filas y columnas, entonces se usan DBMS relacionales conocidos como SQL. Entre los más populares están: . SQLite | MySQL | PostgreSQL | MariaDB | Oracle Database. | Google BigQuery | . Por otro lado, existen otros DBMS conocidos como NoSQL (Not only SQL) que son adecuados cuando los datos son no estructurados, es decir no tienen una estructura definida y cada observación puede constar de distintos campos y de tipos de datos mucho más complejos. Normalmente se usan cuando se trata de bases de datos muy grandes o que representan datos especiales, como por ejemplo las redes. Estas bases de datos suelen tener sus propios lenguajes de consulta, diferentes a SQL. Entre las más populares están: . MongoDB | Neo4J | Cassandra | Redis | . La decisión sobre qué tipo de base de datos es la mejor dependerá siempre del proyecto específico en el que se trabaje y las tareas que se requieran. En este tutorial usaremos SQLite. . Cuando usar una DBMS . Si hemos estudiado alguna carrera de ciencias sociales, probablemente nos costará trabajo empezar a crear y administrar nuestras propias bases de datos porque hay reglas y protocolos para los que se requiere mucha práctica para aprender a ejecutar correctamente. El diseño y puesta en producción de una base de datos debe hacerlo alguien con experiencia. . Aún cuando puede ser cierto que nosotros nunca vayamos a crear una base de datos desde cero, es necesario que aprendamos los fundamentos de las bases de datos por varias razones. . El modelo relacional es una forma poderosa de organizar la información. | Cada vez es más común el uso de SQL dentro de proyectos de ciencia de datos. | Si alguna vez tienes que proveer datos para un sistema de base de datos, podrás entender mejor cómo pasar la información al encargado de su administración. | Si entiendes sus ventajas podrás recomendarlo en los casos en que es conveniente su uso. | Sobre este último punto, deberíamos optar por usar un DBMS en cualquiera de las siguientes situaciones: . Requerimos consultar o actualizar los datos de forma constante. | Los datos deben ser consultados o editados por varios usuarios de forma remota. | Tenemos información que está en muchos archivos pero que está relacionada, y que ademas debe cumplir con ciertas restricciones, como asegurar que los valores pertenezcan a un catálogo. | Necesitamos analizar datos muy grandes que no caben en memoria RAM. | Cuando creamos un producto basado en datos para producción, por ejemplo páginas web o aplicaciones de escritorio. | Necesitamos autenticación de usuarios y/o diferentes permisos para administrar los datos. | Cuando necesitamos garantizar atomicidad. | . SQLite . La mayor parte del contenido de aquí en adelante fue tomado de:Data Management with SQL for Social Scientists . En este tutorial veremos una corta introducción a los comandos más comunes para consultar la información de una base de datos. Hacer consultas es una tarea relativamente sencilla que todos deberíamos aprender. . Para esto usaremos SQlite, que es un DBMS sencillo, adecuado para aplicaciones locales que requieren tipos de datos básicos y pocas funciones avanzadas. No provee un sistema de autenticación y permisos y tampoco soporta escritura de varias conexiones al mismo tiempo, pero es fácil de iniciar y ayuda a mostrar las principales características del lenguaje de consulta. Aunque no es adecuado para aplicaciones empresariales, puede ser buena opción para hacer análisis de datos. . Cliente . Los clientes son programas que permiten conectarse a bases de datos y efectuar operaciones sobre ellas a través de una interfaz de usuario. . Usaremos el cliente DB Browser for SQLite una aplicación ligera que puede ser descargada desde su página web: https://sqlitebrowser.org/dl/ . Una vez instalado se puede buscar en los programas como DB Browser (SQLite). . . Abrir una base de datos . Usaremos una base de datos sobre cultivos del proyecto SAFI (Studying African Farmer-led Irrigation). Los datos vienen en el archivo &quot;SQL_SAFI.sqlite&quot;, que también puede ser descargado desde https://datacarpentry.org/sql-socialsci/data/SQL_SAFI.sqlite. Esta base de datos consta de 4 tablas que describen un conjunto de granjas (Farms), parcelas (Plots) y cultivos (Crops &amp; crops_rice_old). | Vamos al cliente y hacemos clic en el botón de &quot;Open Database&quot;. Buscamos en nuestros archivos y seleccionamos &quot;&quot;SQL_SAFI.sqlite&quot;&quot; | Cuando se abre, en la pestaña &quot;Database Structure&quot; veremos lo siguiente: | . . Tenemos 4 tablas: Crops, Farms, Plots y crops_rice_old. | Si damos clic sobre las $&gt;$ de cada tabla podremos ver los campos que tiene cada una y su tipo: | . . Al seleccionar la pestaña &quot;Browse Data&quot; se puede ver la tabla con los datos. | . . Consultas SQL . Para consultar la información usamos el lenguaje SQL. | Toda consulta SQL debe llevar al menos dos comandos: SELECT y FROM con SELECT especificamos las columnas que queremos recuperar | con FROM especificamos la tabla que vamos a consultar | . | . Un ejemplo de consulta es: . SELECT * FROM Crops . Los comandos de SQL normalmente se escriben en mayúsculas, aunque no es necesario, pero así se distingue mejor cuáles con los comandos de los nombres las variable o tablas. | . . Tampoco es necesario escribirlos en líneas separadas, pero se hace más legible así. | Vamos a la pestaña &quot;Execute SQL&quot; de DB Browser y copiamos el siguiente código:SELECT * FROM Crops . | La instrucción se ejecuta con el botón ▶ del menú. | . Obtenemos como resultado una tabla con todas las filas y columnas de la tabla Crops: . . El resultado de una consulta SQL siempre es una tabla. | A continuación veremos los comandos más populares para consultar la información. | Puedes seguir este tutorial usando DB Browser o también si tienes instalada la distribución de Python de Anaconda, puedes instalar la extensión ipython-sql que permite ejecutar SQL dentro de un Jupyter Notebook. | Si sigues en DB browser, debes copiar el código de cada celda sin la primera fila (%%sql) | . # Esta instrucción solo funciona dentro de un Jupyter Notebook # pip install ipython-sql %load_ext sql %sql sqlite:///SQL_SAFI.sqlite %config SqlMagic.displaylimit=10 . SELECT . Con SELECT especificamos los nombres de las columnas que queremos que nos retorne la consulta. | Podemos especificar uno o varios nombres de columnas, separados por comas. | El * quiere decir que retorne todas las columnas. | . %%sql SELECT id, Country, A06_province FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . Id Country A06_province . 1 | Moz | Manica | . 2 | Moz | Manica | . 3 | Moz | Manica | . 4 | Moz | Manica | . 5 | Moz | Manica | . 6 | Moz | Manica | . 7 | Moz | Manica | . 8 | Moz | Manica | . 9 | Moz | Manica | . 10 | Moz | Manica | . 350 rows, truncated to displaylimit of 10 Las variables pueden ser renombrar en el resultado usando la palabra AS | . %%sql SELECT Country AS pais, A06_province AS provincia FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . pais provincia . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . Moz | Manica | . 350 rows, truncated to displaylimit of 10 LIMIT . Especifica el número máximo de filas a retornar | . %%sql SELECT * FROM Farms LIMIT 5 . * sqlite:///SQL_SAFI.sqlite Done. . Id Country A01_interview_date A03_quest_no A04_start A05_end A06_province A07_district A08_ward A09_village A11_years_farm A12_agr_assoc B_no_membrs _members_count B11_remittance_money B16_years_liv B17_parents_liv B18_sp_parents_liv B19_grand_liv B20_sp_grand_liv C01_respondent_roof_type C02_respondent_wall_type C03_respondent_floor_type C04_window_type C05_buildings_in_compound C06_rooms C07_other_buildings D_no_plots D_plots_count E01_water_use E_no_group_count E_yes_group_count E17_no_enough_water E18_months_no_water E19_period_use E20_exper_other E21_other_meth E22_res_change E23_memb_assoc E24_resp_assoc E25_fees_water E26_affect_conflicts F04_need_money F05_money_source F05_money_source_other F08_emply_lab F09_du_labour F10_liv_owned F10_liv_owned_other F_liv_count F12_poultry F13_du_look_aftr_cows F14_items_owned G01_no_meals G02_months_lack_food G03_no_food_mitigation gps:Latitude gps:Longitude gps:Altitude gps:Accuracy instanceID . 1 | Moz | 17/11/2016 | 1 | 2017-03-23T09:49:57.000Z | 2017-04-02T17:29:08.000Z | Manica | Manica | Bandula | God | 11 | no | 3 | 3 | no | 4 | no | yes | no | yes | grass | muddaub | earth | no | 1 | 1 | no | 2 | 2 | no | 2.0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | no | no | [&amp;#x27;poultry&amp;#x27;] | None | 1 | yes | no | [&amp;#x27;bicycle&amp;#x27;, &amp;#x27;television&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;table&amp;#x27;] | 2 | [&amp;#x27;Jan&amp;#x27;] | [&amp;#x27;na&amp;#x27;, &amp;#x27;rely_less_food&amp;#x27;, &amp;#x27;reduce_meals&amp;#x27;, &amp;#x27;day_night_hungry&amp;#x27;] | -19.11225943 | 33.48345609 | 698.0 | 14.0 | uuid:ec241f2c-0609-46ed-b5e8-fe575f6cefef | . 2 | Moz | 17/11/2016 | 1 | 2017-04-02T09:48:16.000Z | 2017-04-02T17:26:19.000Z | Manica | Manica | Bandula | God | 2 | yes | 7 | 7 | no | 9 | yes | yes | yes | yes | grass | muddaub | earth | no | 1 | 1 | no | 3 | 3 | yes | None | 3.0 | yes | [&amp;#x27;Aug&amp;#x27;, &amp;#x27;Sept&amp;#x27;] | 2.0 | yes | no | None | yes | no | no | once | no | None | None | yes | no | [&amp;#x27;oxen&amp;#x27;, &amp;#x27;cows&amp;#x27;, &amp;#x27;goats&amp;#x27;] | None | 3 | yes | no | [&amp;#x27;cow_cart&amp;#x27;, &amp;#x27;bicycle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;solar_torch&amp;#x27;, &amp;#x27;table&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | 2 | [&amp;#x27;Jan&amp;#x27;, &amp;#x27;Sept&amp;#x27;, &amp;#x27;Oct&amp;#x27;, &amp;#x27;Nov&amp;#x27;, &amp;#x27;Dec&amp;#x27;] | [&amp;#x27;na&amp;#x27;, &amp;#x27;reduce_meals&amp;#x27;, &amp;#x27;restrict_adults&amp;#x27;, &amp;#x27;borrow_food&amp;#x27;, &amp;#x27;seek_government&amp;#x27;] | -19.11247712 | 33.48341568 | 690.0 | 19.0 | uuid:099de9c9-3e5e-427b-8452-26250e840d6e | . 3 | Moz | 17/11/2016 | 3 | 2017-04-02T14:35:26.000Z | 2017-04-02T17:26:53.000Z | Manica | Manica | Bandula | God | 40 | no | 10 | 10 | no | 15 | no | no | no | no | mabatisloping | burntbricks | cement | yes | 1 | 1 | no | 1 | 1 | no | 1.0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | no | yes | [&amp;#x27;none&amp;#x27;] | None | 1 | yes | no | [&amp;#x27;solar_torch&amp;#x27;] | 2 | [&amp;#x27;Jan&amp;#x27;, &amp;#x27;Feb&amp;#x27;, &amp;#x27;Mar&amp;#x27;, &amp;#x27;Oct&amp;#x27;, &amp;#x27;Nov&amp;#x27;, &amp;#x27;Dec&amp;#x27;] | [&amp;#x27;na&amp;#x27;, &amp;#x27;restrict_adults&amp;#x27;, &amp;#x27;lab_ex_food&amp;#x27;] | -19.1121076 | 33.48344998 | 674.0 | 13.0 | uuid:193d7daf-9582-409b-bf09-027dd36f9007 | . 4 | Moz | 17/11/2016 | 4 | 2017-04-02T14:55:18.000Z | 2017-04-02T17:27:16.000Z | Manica | Manica | Bandula | God | 6 | no | 7 | 7 | no | 6 | no | no | no | no | mabatisloping | burntbricks | earth | no | 1 | 1 | no | 3 | 3 | no | 3.0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | no | yes | [&amp;#x27;oxen&amp;#x27;, &amp;#x27;cows&amp;#x27;] | None | 2 | yes | no | [&amp;#x27;bicycle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | 2 | [&amp;#x27;Sept&amp;#x27;, &amp;#x27;Oct&amp;#x27;, &amp;#x27;Nov&amp;#x27;, &amp;#x27;Dec&amp;#x27;] | [&amp;#x27;na&amp;#x27;, &amp;#x27;reduce_meals&amp;#x27;, &amp;#x27;restrict_adults&amp;#x27;, &amp;#x27;lab_ex_food&amp;#x27;] | -19.11222901 | 33.48342395 | 679.0 | 5.0 | uuid:148d1105-778a-4755-aa71-281eadd4a973 | . 5 | Moz | 17/11/2016 | 5 | 2017-04-02T15:10:35.000Z | 2017-04-02T17:27:35.000Z | Manica | Manica | Bandula | God | 18 | no | 7 | 7 | no | 40 | yes | no | yes | no | grass | burntbricks | earth | no | 1 | 1 | no | 2 | 2 | no | 2.0 | None | None | None | None | None | None | None | None | None | None | None | None | None | None | no | no | [&amp;#x27;oxen&amp;#x27;, &amp;#x27;cows&amp;#x27;, &amp;#x27;goats&amp;#x27;, &amp;#x27;poultry&amp;#x27;] | None | 4 | yes | no | [&amp;#x27;motorcyle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | 2 | [&amp;#x27;Aug&amp;#x27;, &amp;#x27;Sept&amp;#x27;, &amp;#x27;Oct&amp;#x27;, &amp;#x27;Nov&amp;#x27;] | [&amp;#x27;na&amp;#x27;, &amp;#x27;go_forest&amp;#x27;, &amp;#x27;migrate&amp;#x27;] | -19.11221722 | 33.48342524 | 689.0 | 10.0 | uuid:2c867811-9696-4966-9866-f35c3e97d02d | . Pr&#225;ctica . Escribe una consulta que retorne las primeras 5 filas de la tabla Farms solo con las columnas Id, y B16 a B20 | . WHERE . Este comando sirve para imponer una o varias condiciones. El resultado retornará solo las observaciones que cumplan con los criterios especificados. | Se pueden usar operadores como $=$, $&gt;$, $&lt;$, $&lt;=$, $&gt;=$, $&lt;&gt;$ | Para especificar más de una condición se pueden usar los operadores lógicos AND y OR | . %%sql SELECT Id, B16_years_liv FROM Farms WHERE B16_years_liv &gt; 25 . * sqlite:///SQL_SAFI.sqlite Done. . Id B16_years_liv . 5 | 40 | . 7 | 38 | . 8 | 70 | . 15 | 30 | . 16 | 47 | . 27 | 36 | . 32 | 69 | . 33 | 34 | . 35 | 45 | . 43 | 29 | . 118 rows, truncated to displaylimit of 10 Varias condiciones | . %%sql SELECT Id FROM Farms WHERE B17_parents_liv = &#39;yes&#39; AND B18_sp_parents_liv = &#39;yes&#39; AND B19_grand_liv = &#39;yes&#39; AND B20_sp_grand_liv = &#39;yes&#39; . * sqlite:///SQL_SAFI.sqlite Done. . Id . 2 | . 8 | . 15 | . 16 | . 19 | . 20 | . 22 | . 26 | . 31 | . 33 | . 81 rows, truncated to displaylimit of 10 %%sql SELECT Id, B16_years_liv FROM Farms WHERE B16_years_liv &gt; 50 AND B16_years_liv &lt; 60 . * sqlite:///SQL_SAFI.sqlite Done. . Id B16_years_liv . 48 | 58 | . 68 | 52 | . 101 | 52 | . 276 | 51 | . 334 | 51 | . 346 | 55 | . Se puede usar el operador BETWEEN para especificar un rango de valores. | . %%sql SELECT Id, B16_years_liv FROM Farms WHERE B16_years_liv BETWEEN 51 AND 59 . * sqlite:///SQL_SAFI.sqlite Done. . Id B16_years_liv . 48 | 58 | . 68 | 52 | . 101 | 52 | . 276 | 51 | . 334 | 51 | . 346 | 55 | . Se puede usar el operador IN para especificar una lista de valores. | . %%sql SELECT Id, B16_years_liv FROM Farms WHERE B16_years_liv IN (51, 52, 53, 54, 55, 56, 57, 58, 59) . * sqlite:///SQL_SAFI.sqlite Done. . Id B16_years_liv . 48 | 58 | . 68 | 52 | . 101 | 52 | . 276 | 51 | . 334 | 51 | . 346 | 55 | . Al poner varias condiciones, es recomendable agruparlas con paréntesis para evitar confusiones. | . %%sql SELECT Id FROM Farms WHERE (B17_parents_liv = &#39;yes&#39; OR B18_sp_parents_liv = &#39;yes&#39;) AND B16_years_liv &gt; 60 . * sqlite:///SQL_SAFI.sqlite Done. . Id . 8 | . 32 | . 77 | . 97 | . 224 | . 349 | . Pr&#225;ctica . Escribe una consulta con la tabla Farms que retorne las columnas Id, A09_village, A11_years_farm, B16_years_liv. Filtra las filas para quedarnos solo con las filas donde el valor de A09_village es God o Ruaca. Adicionalmente, solo queremos valores de A11_years_farm entre 20 y 30 y valores de B16_years_liv mayores que 40. | . seleccionar valores nulos . Para seleccionar observaciones que tienen valores nulos se usa la condición IS NULL. | Para seleccionar los que son no nulos se usa IS NOT NULL | . %%sql SELECT B16_years_liv, F14_items_owned FROM Farms WHERE F14_items_owned IS NULL . * sqlite:///SQL_SAFI.sqlite Done. . B16_years_liv F14_items_owned . 3 | None | . 20 | None | . 2 | None | . 2 | None | . 22 | None | . 15 | None | . 60 | None | . 10 | None | . 41 | None | . 49 | None | . 34 rows, truncated to displaylimit of 10 %%sql SELECT B16_years_liv, F14_items_owned FROM Farms WHERE F14_items_owned NOT NULL . * sqlite:///SQL_SAFI.sqlite Done. . B16_years_liv F14_items_owned . 4 | [&amp;#x27;bicycle&amp;#x27;, &amp;#x27;television&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;table&amp;#x27;] | . 9 | [&amp;#x27;cow_cart&amp;#x27;, &amp;#x27;bicycle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;solar_torch&amp;#x27;, &amp;#x27;table&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | . 15 | [&amp;#x27;solar_torch&amp;#x27;] | . 6 | [&amp;#x27;bicycle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | . 40 | [&amp;#x27;motorcyle&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;mobile_phone&amp;#x27;] | . 38 | [&amp;#x27;motorcyle&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;] | . 70 | [&amp;#x27;motorcyle&amp;#x27;, &amp;#x27;bicycle&amp;#x27;, &amp;#x27;television&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;solar_torch&amp;#x27;, &amp;#x27;table&amp;#x27;, &amp;#x27;fridge&amp;#x27;] | . 6 | [&amp;#x27;television&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;solar_torch&amp;#x27;] | . 23 | [&amp;#x27;cow_cart&amp;#x27;, &amp;#x27;motorcyle&amp;#x27;, &amp;#x27;bicycle&amp;#x27;, &amp;#x27;television&amp;#x27;, &amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;, &amp;#x27;solar_panel&amp;#x27;, &amp;#x27;solar_torch&amp;#x27;, &amp;#x27;table&amp;#x27;] | . 20 | [&amp;#x27;radio&amp;#x27;, &amp;#x27;cow_plough&amp;#x27;] | . 316 rows, truncated to displaylimit of 10 ORDER BY . Esta instrucción sirve para ordenar las filas de la tabla | . %%sql SELECT Id, A09_village, A11_years_farm, B16_years_liv FROM Farms WHERE A09_village = &#39;God&#39; ORDER BY A11_years_farm . * sqlite:///SQL_SAFI.sqlite Done. . Id A09_village A11_years_farm B16_years_liv . 83 | God | 1 | 43 | . 2 | God | 2 | 9 | . 6 | God | 3 | 3 | . 118 | God | 3 | 13 | . 76 | God | 5 | 4 | . 4 | God | 6 | 6 | . 11 | God | 6 | 20 | . 18 | God | 6 | 20 | . 88 | God | 6 | 6 | . 13 | God | 7 | 8 | . 43 rows, truncated to displaylimit of 10 Se puede especificar el orden ascendente o descendente con ASC o DESC, respectivamente. | . %%sql SELECT Id, A09_village, A11_years_farm, B16_years_liv FROM Farms WHERE A09_village = &#39;God&#39; ORDER BY A11_years_farm DESC . * sqlite:///SQL_SAFI.sqlite Done. . Id A09_village A11_years_farm B16_years_liv . 3 | God | 40 | 15 | . 15 | God | 30 | 30 | . 16 | God | 24 | 47 | . 20 | God | 24 | 1 | . 82 | God | 24 | 24 | . 40 | God | 23 | 23 | . 39 | God | 22 | 22 | . 41 | God | 22 | 22 | . 103 | God | 22 | 22 | . 111 | God | 22 | 22 | . 43 rows, truncated to displaylimit of 10 Se puede ordenar por varias variables | . %%sql SELECT Id, A09_village, A11_years_farm, B16_years_liv FROM Farms WHERE A09_village = &#39;God&#39; ORDER BY A11_years_farm ASC , B16_years_liv DESC . * sqlite:///SQL_SAFI.sqlite Done. . Id A09_village A11_years_farm B16_years_liv . 83 | God | 1 | 43 | . 2 | God | 2 | 9 | . 118 | God | 3 | 13 | . 6 | God | 3 | 3 | . 76 | God | 5 | 4 | . 11 | God | 6 | 20 | . 18 | God | 6 | 20 | . 4 | God | 6 | 6 | . 88 | God | 6 | 6 | . 84 | God | 7 | 48 | . 43 rows, truncated to displaylimit of 10 En conjunto con LIMIT nos puede dar el top N | . %%sql SELECT Id, A09_village, A11_years_farm, B16_years_liv FROM Farms WHERE A09_village = &#39;God&#39; ORDER BY A11_years_farm DESC LIMIT 3 . * sqlite:///SQL_SAFI.sqlite Done. . Id A09_village A11_years_farm B16_years_liv . 3 | God | 40 | 15 | . 15 | God | 30 | 30 | . 16 | God | 24 | 47 | . Funciones y operaciones con columnas . Es posible hacer algunas operaciones aritméticas entre columnas | . %%sql SELECT B16_years_liv * 100 FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . B16_years_liv * 100 . 400 | . 900 | . 1500 | . 600 | . 4000 | . 300 | . 3800 | . 7000 | . 600 | . 2300 | . 350 rows, truncated to displaylimit of 10 %%sql SELECT B_no_membrs * B16_years_liv AS total_years FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . total_years . 12 | . 63 | . 150 | . 42 | . 280 | . 9 | . 228 | . 840 | . 48 | . 276 | . 350 rows, truncated to displaylimit of 10 También se pueden usar funciones predeterminadas que transforman las variables. Hay funciones para números, para texto, para fechas, etc. | La lista de funciones que se puede usar en SQLite están se pueden consultar en https://sqlite.org/lang_corefunc.html | Por ejemplo, la función ROUND() redondea los valores de una columna. | . %%sql SELECT ROUND(D02_total_plot * 3.14159, 1) AS D02_rounded FROM Plots . * sqlite:///SQL_SAFI.sqlite Done. . D02_rounded . 1.6 | . 1.6 | . 3.1 | . 4.7 | . 3.1 | . 3.1 | . 3.1 | . 3.1 | . 3.1 | . 4.7 | . 844 rows, truncated to displaylimit of 10 La función SUBSTR() sirve para obtener un subconjunto de caracteres de un texto, dada una posición inicial y un número de caracteres. | La expresión CAST(variable AS INTEGER) convierte una variable de texto en número entero. | . %%sql SELECT CAST(SUBSTR(A01_interview_date,7,4) AS INTEGER) AS year FROM Farms ORDER BY A01_interview_date . * sqlite:///SQL_SAFI.sqlite Done. . year . 2017 | . 2017 | . 2017 | . 2017 | . 2016 | . 2016 | . 2016 | . 2016 | . 2016 | . 2016 | . 350 rows, truncated to displaylimit of 10 La expresión $||$ concatena dos variables de texto. | . %%sql SELECT A12_agr_assoc || &#39; &amp; &#39; || B11_remittance_money AS assoc_remittance FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . assoc_remittance . no &amp; no | . yes &amp; no | . no &amp; no | . no &amp; no | . no &amp; no | . no &amp; no | . no &amp; no | . yes &amp; no | . no &amp; no | . no &amp; no | . 350 rows, truncated to displaylimit of 10 Cuando se requiere ordenar por una fecha, es necesario convertir el texto en fecha, de lo contrario tendremos un orden que no es el adecuado. | Se puede usar la función DATE() para pasar de texto a fecha. Para ello es necesario especificar el texto en formato YYYY-MM-DD. | En el siguiente ejemplo, la variable A01_interview_date está guardada como texto en formato dd/mm/yyy, por tanto, cuando intentamos ordenar por esta variable obtenemos un orden distinto al que esperaríamos. | . %%sql SELECT A01_interview_date FROM Farms ORDER BY A01_interview_date . * sqlite:///SQL_SAFI.sqlite Done. . A01_interview_date . 01/07/2017 | . 01/07/2017 | . 01/07/2017 | . 01/07/2017 | . 01/12/2016 | . 01/12/2016 | . 01/12/2016 | . 01/12/2016 | . 01/12/2016 | . 01/12/2016 | . 350 rows, truncated to displaylimit of 10 En este caso lo que deberíamos hacer es reformar el texto para que quede en formato yyyy-mm-dd y usar la función DATE() para convertir en fecha. | . %%sql SELECT A01_interview_date, date( substr(A01_interview_date,7,4) || &#39;-&#39; || substr(A01_interview_date,4,2) || &#39;-&#39; || substr(A01_interview_date,1,2) ) AS converted_date FROM Farms ORDER BY converted_date . * sqlite:///SQL_SAFI.sqlite Done. . A01_interview_date converted_date . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 16/11/2016 | 2016-11-16 | . 350 rows, truncated to displaylimit of 10 Pr&#225;ctica . En la tabla Farms, usando la columna A01_interview_date, crea tres nuevas columnas que correspondan al año, mes y día. Luega ordena de forma descendente usando estas tres variables. | . CASE WHEN . Con la expresión CASE ... WHEN ... podemos cambiar los valores de una variable por otros al momento de presentar la información | . %%sql SELECT Id, B16_years_liv, CASE country WHEN &#39;Moz&#39; THEN &#39;Mozambique&#39; WHEN &#39;Taz&#39; THEN &#39;Tanzania&#39; ELSE &#39;Unknown Country&#39; END AS country_fullname FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . Id B16_years_liv country_fullname . 1 | 4 | Mozambique | . 2 | 9 | Mozambique | . 3 | 15 | Mozambique | . 4 | 6 | Mozambique | . 5 | 40 | Mozambique | . 6 | 3 | Mozambique | . 7 | 38 | Mozambique | . 8 | 70 | Mozambique | . 9 | 6 | Mozambique | . 10 | 23 | Mozambique | . 350 rows, truncated to displaylimit of 10 %%sql SELECT Id, A11_years_farm, CASE WHEN A11_years_farm BETWEEN 1 AND 30 THEN &#39;1-30&#39; WHEN A11_years_farm BETWEEN 31 AND 60 THEN &#39;31-60&#39; ELSE &#39;&gt; 60&#39; END AS A11_years_farm_range FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . Id A11_years_farm A11_years_farm_range . 1 | 11 | 1-30 | . 2 | 2 | 1-30 | . 3 | 40 | 31-60 | . 4 | 6 | 1-30 | . 5 | 18 | 1-30 | . 6 | 3 | 1-30 | . 7 | 20 | 1-30 | . 8 | 16 | 1-30 | . 9 | 16 | 1-30 | . 10 | 22 | 1-30 | . 350 rows, truncated to displaylimit of 10 Pr&#225;ctica . En la tabla Farms, usando la columna B_no_membrs, crea una nueva variable llamada tamaño_hogar que tenga los siguientes casos: Entre 1 y 4 -&gt; pequeño | Entre 5 y 10 -&gt; mediano | Mayor a 10 -&gt; grande | . | . DISTINCT . Podemos seleccionar los valores únicos de una o varias columnas con la expresión DISTINCT. | . %%sql SELECT DISTINCT A06_province FROM Farms; . * sqlite:///SQL_SAFI.sqlite Done. . A06_province . Manica | . Nampula | . Sofala | . %%sql SELECT DISTINCT A06_province, A07_district FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . A06_province A07_district . Manica | Manica | . Manica | Bandula | . Nampula | Nampula | . Sofala | Nhamatanda | . Sofala | 51 | . Sofala | Nhamantanda | . Sofala | Madangua | . Sofala | Nhamatandda | . Manica | Vanduzi | . Manica | Va | . Agregaciones y agrupaciones . Una de las características más útiles de SQL es la capacidad para calcular algunas estadísticas agregadas. | Para agregar se usan algunas funciones como COUNT(), MAX(), MIN(), AVG() | . %%sql SELECT COUNT(A11_years_farm) AS n, MAX(A11_years_farm) AS maximo, MIN(A11_years_farm) AS minimo, ROUND(AVG(A11_years_farm), 2) as promedio FROM Farms . * sqlite:///SQL_SAFI.sqlite Done. . n maximo minimo promedio . 350 | 60 | 1 | 17.51 | . Podemos calcular estadísticas agrupadas usando la expresión GROUP BY | . %%sql SELECT A06_province, A07_district, A08_ward, A09_village, COUNT(A11_years_farm) AS n, MAX(A11_years_farm) AS maximo, MIN(A11_years_farm) AS minimo, ROUND(AVG(A11_years_farm), 2) as promedio FROM Farms GROUP BY A06_province, A07_district, A08_ward, A09_village . * sqlite:///SQL_SAFI.sqlite Done. . A06_province A07_district A08_ward A09_village n maximo minimo promedio . Manica | Bandula | Bandula | Chirodzo | 1 | 12 | 12 | 12.0 | . Manica | Manica | Bandula | 49 | 1 | 12 | 12 | 12.0 | . Manica | Manica | Bandula | Chirodzo | 36 | 60 | 1 | 17.61 | . Manica | Manica | Bandula | God | 42 | 40 | 1 | 14.67 | . Manica | Manica | Bandula | Ruaca | 45 | 53 | 2 | 15.73 | . Manica | Manica | Bandula | Ruaca - Nhamuenda | 1 | 16 | 16 | 16.0 | . Manica | Manica | Bandula | Ruaca-Nhamuenda | 3 | 21 | 1 | 12.0 | . Manica | Manica | Manica | Chirodzo | 1 | 16 | 16 | 16.0 | . Manica | Manica | Manica | God | 1 | 24 | 24 | 24.0 | . Manica | Va | Vanduzi | Belas | 1 | 16 | 16 | 16.0 | . 30 rows, truncated to displaylimit of 10 Para filtrar los resultados de una consulta agregada usamos la declaración HAVING | . %%sql SELECT A06_province, A07_district, A08_ward, A09_village, COUNT(A11_years_farm) AS n, MAX(A11_years_farm) AS maximo, MIN(A11_years_farm) AS minimo, ROUND(AVG(A11_years_farm), 2) as promedio, SUM(A11_years_farm) as total FROM Farms GROUP BY A06_province, A07_district, A08_ward, A09_village HAVING promedio &gt; 20 . * sqlite:///SQL_SAFI.sqlite Done. . A06_province A07_district A08_ward A09_village n maximo minimo promedio total . Manica | Manica | Manica | God | 1 | 24 | 24 | 24.0 | 24 | . Sofala | Nhamatanda | 91 | Massequece | 1 | 45 | 45 | 45.0 | 45 | . Sofala | Nhamatanda | Lamego | Massequese | 1 | 42 | 42 | 42.0 | 42 | . Sofala | Nhamatanda | Lamego | Ndedja | 6 | 35 | 10 | 27.5 | 165 | . Sofala | Nhamatanda | Lamego | Nhansato | 48 | 50 | 3 | 25.94 | 1245 | . Sofala | Nhamatanda | Lamego | Nhansato-Castanheira | 3 | 37 | 21 | 27.33 | 82 | . Sofala | Nhamatanda | Lomego | Massequece | 1 | 40 | 40 | 40.0 | 40 | . Pr&#225;ctica . Usando la tabla Plots, calcula la extensión total y número de parcelas de cada granja. Id identifica cada granja y D02_total_plot es la extensión de cada parcela individual. | Filtra para quedarte solo con aquellas granjas que tienen más de 10 hectáreas en total y además entre 2 y 3 parcelas. | . Uniones . Hasta el momento hemos estado usando datos de una sola tabla, pera para muchos casos quisieramos responder preguntas que involucren los datos de más de una tabla. | Cuando queremos usar los datos de más de una tabla debemos hacer uniones con el comando JOIN. Esta unión se hace en la declaración FROM. | En la siguiente consulta queremos saber en cuáles granjas con más de 12 miembros se cultiva maíz. La información del número de miembro de la granja está en la tabla Farms, pero la información del tipo de cultivo está en la tabla Crops. Por tanto, debemos hacer un JOIN entre las dos tablas. | Cuando tenemos más de una tabla es usual renombrar las tablas con la declaración AS, para distinguir las variables que provienen de cada tabla y evitar problemas cuando haya variables con el mismo nombre. | Con la expresión ON especificamos la variable de cada tabla con la que se hará el emparejamiento. | . %%sql SELECT F.Id AS farm_id, F.B_no_membrs as farm_members, C.Id, C.D_curr_crop FROM Farms AS F JOIN Crops AS C ON F.Id = C.Id WHERE F.B_no_membrs &gt; 12 AND C.D_curr_crop = &#39;maize&#39; . * sqlite:///SQL_SAFI.sqlite Done. . farm_id farm_members Id D_curr_crop . 32 | 19 | 32 | maize | . 92 | 17 | 92 | maize | . 101 | 14 | 101 | maize | . 103 | 15 | 103 | maize | . 111 | 15 | 111 | maize | . 111 | 15 | 111 | maize | . 124 | 15 | 124 | maize | . 253 | 15 | 253 | maize | . Se pueden hacer joins con más de 2 tablas. En esta por ejemplo, estamos consultando cuáles son las parcelas que pertenecen a las granjas con más de 12 miembros, cultivan maíz y tienen una extensión mayor a 5 hectáreas. | . %%sql SELECT F.Id AS farm_id, F.B_no_membrs AS farm_members, C.Id AS Crops_Id, C.plot_Id AS crops_plot_id, C.D_curr_crop, P.Id , P.plot_id AS plot_id, P.D02_total_plot FROM Farms AS F JOIN Crops AS C JOIN Plots AS P ON F.Id = C.Id AND F.Id = P.Id AND C.Id = P.Id WHERE F.B_no_membrs &gt; 12 AND C.D_curr_crop = &#39;maize&#39; AND P.D02_total_plot &gt; 5 . * sqlite:///SQL_SAFI.sqlite Done. . farm_id farm_members Crops_Id crops_plot_id D_curr_crop Id plot_id D02_total_plot . 32 | 19 | 32 | 1 | maize | 32 | 1 | 10.0 | . 92 | 17 | 92 | 1 | maize | 92 | 1 | 6.0 | . 92 | 17 | 92 | 1 | maize | 92 | 2 | 6.0 | . 92 | 17 | 92 | 1 | maize | 92 | 3 | 6.0 | . 103 | 15 | 103 | 3 | maize | 103 | 1 | 22.0 | . 103 | 15 | 103 | 3 | maize | 103 | 2 | 16.0 | . 124 | 15 | 124 | 1 | maize | 124 | 1 | 7.0 | . Pr&#225;ctica . Calcula el área total de maíz sembrado actualmente en cada villa. | Tenga en cuenta que la extensión de cada cultivo (D02_total_plot) está en la tabla Plots, el cultivo actual (D_curr_crop) está en la tabla Crops y la información de la villa (A09_village) en la tabla Farms | . Referencias . Data Management with SQL for Social Scientists | Big Data &amp; Social Science. Chpater 4: Databases | Curso gratuito en Udacity: SQL for Data Analysis | .",
            "url": "http://blog.jjsantoso.com/intro-sql-sociales/",
            "relUrl": "/intro-sql-sociales/",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Recuadros para mapas en Geopandas",
            "content": "Introducci&#243;n . Con la reciente publicación de los resultados del censo 2020 de INEGI varios usuarios han estado presentando algunas gráficas en redes sociales con los principales resultados. Una de las visualizaciones más populares son los mapas coropléticos, especialmente a nivel municipal. Una ventaja de estos mapas es que nos permiten tener el panorama general de todo el país e incluso detectar a grandes rasgos algunos patrones regionales. Por ejemplo, acá @claudiodanielpc nos muestra el hacinamiento en los hogares: Siguiendo con los microdatos del #Censo2020Mx, hice este mapa en #rstats sobre el hacinamiento en cada municipio.Gracias al Dr. @SantaellaJulio y al gran equipo del @INEGI_INFORMA por brindarnos información para el análisis de los problemas de nuestro país. pic.twitter.com/K9zhJkSmEt . &mdash; Claudio Daniel Pacheco-Castro (@claudiodanielpc) March 22, 2021 . Sin embargo, un inconveniente es que hay casos de regiones donde hay un gran número de municipios en una superficie pequeña lo que dificulta la lectura de la información. Un caso común en México es el área compuesta por los estados de Oaxaca, Puebla, Chiapas, Veracruz y Estado de México que contienen 1248 municipios, poco más de la mitad del total de 2469 municipios. Muchos mapas se vuelven difícil de ver en esta zona. Por ejemplo: . . Para tratar de hacer más claras este tipo de visualizaciones podemos agregar uno o varios recuadros con acercamiento (zoom) a los resultados de estas zonas, tal como lo hace el mapa del ejemplo. Si bien eso no garantiza que toda la información se entenderá mejor, al menos puede ayudar en ciertas áreas. En esta entrada veremos cómo hacer varios tipos de recuadros para mapas en Python usando Geopandas y Matplotlib. . Empecemos importando las librerías necesarias. . import os import sys import geopandas as gpd import matplotlib.pyplot as plt import pandas as pd import requests import matplotlib_scalebar from matplotlib_scalebar.scalebar import ScaleBar print(&#39;Python&#39;, sys.version) print(pd.__name__, pd.__version__) print(gpd.__name__, gpd.__version__) print(requests.__name__, requests.__version__) print(plt.matplotlib.__name__, plt.matplotlib.__version__) print(matplotlib_scalebar.__name__, matplotlib_scalebar.__version__) . Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] pandas 1.1.3 geopandas 0.8.1 requests 2.24.0 matplotlib 3.3.2 matplotlib_scalebar 0.7.2 . Datos . Voy a usar una versión del mapa de México algo simplificada que pesa relativamente poco. Este shapefile puede descargarse aquí. . Tip: la versión más reciente de la cartografía de los municipios de México puede descargarse desde el Marco Geoestadístico Nacional de INEGI. . Leemos el archivo, asignamos la clave municipal de INEGI como el índice del GeoDataFrame y además reproyectamos las coordenadas usando el sistema de coordenadas Mexico ITRF92 / UTM zone 12N . mx = gpd.read_file(&#39;mapa_mexico/&#39;) .set_index(&#39;CLAVE&#39;) .to_crs(epsg=4485) mx.head() . NOM_MUN NOMEDO CVE_EDO CVE_MUNI Area geometry . CLAVE . 02004 Tijuana | Baja California | 02 | 004 | 1122.661145 | POLYGON ((-73565.018 3602427.487, -73564.403 3... | . 02003 Tecate | Baja California | 02 | 003 | 3670.991923 | POLYGON ((-38995.078 3617846.589, -31557.921 3... | . 02002 Mexicali | Baja California | 02 | 002 | 13119.275713 | POLYGON ((48160.716 3621731.593, 58570.990 362... | . 02005 Playas de Rosarito | Baja California | 02 | 005 | 517.120801 | POLYGON ((-70946.724 3594803.753, -70966.034 3... | . 26055 San Luis Rio Colorado | Sonora | 26 | 055 | 9033.770278 | POLYGON ((127160.493 3587762.823, 127099.688 3... | . Al GeoDataFrame anterior vamos a añadirle datos para que podamos graficar un mapa coroplético. Usaremos la API de DataMéxico para obtener los datos del Índice de Complejidad Económica (ECI). La propia definición del ECI de DataMexico es: . El Índice de Complejidad Económica (ECI) mide las capacidades productivas de una localidad (e.g. estado o municipalidad) a partir de la presencia de actividades (e.g. empleo, industrias o exportaciones) en esa y otras localidades. La complejidad económica de una localidad predice su nivel de ingreso, crecimiento económico, desigualdad, y emisiones de gases de efecto invernadero. . En general, un ECI más alto indica un mayor nivel de sofisticación en los productos que se producen en un municipio. Así se ven los datos: . url = &#39;https://api.datamexico.org/tesseract/cubes/complexity_eci/aggregate.jsonrecords?cuts%5B%5D=Latest.Latest.Latest.1&amp;drilldowns%5B%5D=Geography+Municipality.Geography.Municipality&amp;drilldowns%5B%5D=Date+Day.Date.Year&amp;measures%5B%5D=ECI&amp;parents=false&amp;sparse=false&#39; data_eci = requests.get(url).json() eci = pd.DataFrame(data_eci[&#39;data&#39;]) .assign(CLAVE=lambda x: x[&#39;Municipality ID&#39;].astype(str).str.zfill(5)) .set_index(&#39;CLAVE&#39;) eci.head() . Municipality ID Municipality Year ECI . CLAVE . 01001 1001 | Aguascalientes | 2020 | 2.976280 | . 01002 1002 | Asientos | 2020 | -0.430667 | . 01003 1003 | Calvillo | 2020 | 0.420862 | . 01004 1004 | Cosío | 2020 | -0.464407 | . 01005 1005 | Jesús María | 2020 | 3.049664 | . El índice está estandarizado con una media de 0 y varianza de 1 y tiene un rango entre -1.4 y 4.9. . eci[&#39;ECI&#39;].describe() . count 2.142000e+03 mean 7.507756e-10 std 1.000000e+00 min -1.399253e+00 25% -6.253233e-01 50% -2.969113e-01 75% 2.492735e-01 max 4.901706e+00 Name: ECI, dtype: float64 . Revisamos su distribución: . eci[&#39;ECI&#39;].hist(bins=100) . &lt;AxesSubplot:&gt; . Ahora anexamos los datos del ECI al GeoDataFrame: . mx = mx.join(eci, how=&#39;left&#39;) mx.head() . NOM_MUN NOMEDO CVE_EDO CVE_MUNI Area geometry Municipality ID Municipality Year ECI . CLAVE . 01001 Aguascalientes | Aguascalientes | 01 | 001 | 1168.762384 | POLYGON ((1416489.577 2467700.472, 1417908.226... | 1001.0 | Aguascalientes | 2020.0 | 2.976280 | . 01002 Asientos | Aguascalientes | 01 | 002 | 547.762077 | POLYGON ((1417043.958 2491681.240, 1417408.488... | 1002.0 | Asientos | 2020.0 | -0.430667 | . 01003 Calvillo | Aguascalientes | 01 | 003 | 931.300088 | POLYGON ((1347882.273 2454901.097, 1348002.307... | 1003.0 | Calvillo | 2020.0 | 0.420862 | . 01004 Cosio | Aguascalientes | 01 | 004 | 128.907513 | POLYGON ((1397788.297 2509816.078, 1398009.089... | 1004.0 | Cosío | 2020.0 | -0.464407 | . 01005 Jesus Maria | Aguascalientes | 01 | 005 | 499.207990 | POLYGON ((1388272.165 2462097.533, 1389832.232... | 1005.0 | Jesús María | 2020.0 | 3.049664 | . Creamos un par de GeoDataFrames que nos servirán más adelante: . oax contiene los datos solo para el estado de Oaxaca | edos es un GeoDataFrame con los polígonos de los estados del país. | . oax = mx.query(&#39;CVE_EDO==&quot;20&quot;&#39;) edos = mx.dissolve(by=&#39;CVE_EDO&#39;) . Y ahora visualicemos el ECI con un esquema de cortes por quintiles: . fig, ax = plt.subplots() mx.plot(column=&#39;ECI&#39;, legend=True, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;, ax=ax) fig.set_size_inches(12, 8) . Como se puede ver, algunas zonas del centro y sur del país son difíciles de distinguir por el gran número de municipios que hay. . Recuadro de acercamiento . Vamos a añadir un recuadro de acercamiento o detalle a la zona sur, señalando principalmente los municipios del estado de Oaxaca. Para eso usaremos la función inset_axes de Matplotlib. Esta función nos permite añadir un nuevo objeto axes a nuestra gráfica en cualquier parte del gráfico. La función recibe una lista con 4 valores que son: $[x, y, w, h]$: . $x$: posición en el eje x de la esquina inferior izquierda del recuadro | $y$: posición en el eje y de la esquina inferior izquierda del recuadro | $w$: ancho del recuadro | $h$: altura del recuadro | . Todos estos valores vienen expresados como proporción del tamaño de la gráfica (proporción del ancho para el eje x, proporción del alto para el eje y). Veamos un ejemplo: . fig, ax = plt.subplots() axins = ax.inset_axes([1.1, 0.5, 0.4, 0.4]) axins.set(xlim=(0.5, 0.7), ylim=(0.4, 0.6)) ax.indicate_inset_zoom(axins) . (&lt;matplotlib.patches.Rectangle at 0x2f7a3bf22e0&gt;, (&lt;matplotlib.patches.ConnectionPatch at 0x2f7a3c269d0&gt;, &lt;matplotlib.patches.ConnectionPatch at 0x2f7a3c26d30&gt;, &lt;matplotlib.patches.ConnectionPatch at 0x2f7a3c26fa0&gt;, &lt;matplotlib.patches.ConnectionPatch at 0x2f7a0951550&gt;)) . Lo que hicimos fue añadir una figura (fig) y un objeto axes principal (ax). Luego añadimos el recuadro axins y limitamos el rango del eje $x$ e $y$ en los que se enfoca el recuadro. Por último, le indicamos al recuadro cuál es la gráfica principal sobre la que esta haciendo el zoom. Lo que falta es dibujar sobre los objetos ax y axins. . x = [0, 0.5, 0.6, 0.8] y = [0, 0.4, 0.5, 0.3] ax.plot(x, y, color=&#39;C0&#39;) axins.plot(x, y, color=&#39;C0&#39;) fig . Aunque es relativamente fácil añadir un recuadro a una gráfica, es un poco más difícil añadir el recuadro a un mapa. La dificultad adicional está en que en el resultado final los parámetros $x$, $y$, $w$ y $h$ no se comportan exactamente como sería esperado, sino que por algún motivo el recuadro queda en una posición un poco diferente. No podría asegurarlo, pero me parece que esto pasa porque al graficar un mapa Matplotlib modifica el tamaño de la gráfica y su relación de aspecto para ajustarlo a la información geográfica. La solución (muy poco satisfactoria) es jugar con varios valores de $x$, $y$, $w$ y $h$ hasta que la posición sea la adecuada. . En el siguiente ejemplo vamos a acercarnos al estado de Oaxaca con un recuadro usando los parámetros $x=0.82, y=0.05, w=0.9, h=0.9$ y poniendo como límite al eje x el rango (1820000, 2350000) y al eje y el rango (1780000, 2150000). Los valores (y su unidad de medida) de los ejes X e Y vienen dadas por el sistema de coordenadas geográficas que usamos. . fig, ax = plt.subplots() # Añade recuadro axins = ax.inset_axes([0.82, 0.05, 0.9, 0.9]) # Gráfica principal mx.plot(column=&#39;ECI&#39;, legend=True, ax=ax, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;, legend_kwds={&#39;loc&#39;: &#39;lower left&#39;}) mx.boundary.plot(lw=0.05, color=&#39;k&#39;, ax=ax) # Gráfica recuadro mx.plot(column=&#39;ECI&#39;, legend=False, ax=axins, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;) mx.boundary.plot(lw=0.25, color=&#39;k&#39;, ax=axins) edos.boundary.plot(lw=1, color=&#39;red&#39;, ax=axins) # limita área a mostrar axins.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;, xlim=(1820000, 2350000), ylim=(1780000, 2150000), xticks=[], yticks=[]) # Establece líneas del recuadro a la gráfica principal ax.indicate_inset_zoom(axins) # Tamaño de la gráfica final fig.set_size_inches(12, 8) . El acercamiento resulta bastante bueno, ahora es posible identificar mejor los valores del ECI para el estado de Oaxaca y otros vecinos. Podemos añadir un segundo recuadro para ver los municipios del industrial estado de Guanajuato. El proceso es el mismo que el anterior, aunque ahora demorará más porque debe graficar más elementos. . fig, ax = plt.subplots() # Añade recuadro 1 axins = ax.inset_axes([0.86, 0.05, 0.9, 0.9]) # Añade recuadro 2 axins2 = ax.inset_axes([0.2, -0.5, 0.5, 0.5]) # Gráfica principal mx.plot(column=&#39;ECI&#39;, legend=True, ax=ax, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;, legend_kwds={&#39;loc&#39;: &#39;lower left&#39;}) mx.boundary.plot(lw=0.05, color=&#39;k&#39;, ax=ax) # Gráfica recuadro 1 mx.plot(column=&#39;ECI&#39;, legend=False, ax=axins, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;) mx.boundary.plot(lw=0.25, color=&#39;k&#39;, ax=axins) edos.boundary.plot(lw=1, color=&#39;red&#39;, ax=axins) # Gráfica recuadro 2 mx.plot(column=&#39;ECI&#39;, legend=False, ax=axins2, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;) mx.boundary.plot(lw=0.25, color=&#39;k&#39;, ax=axins2) edos.boundary.plot(lw=1, color=&#39;red&#39;, ax=axins2) # limita área a mostrar recuadro 1 y 2 axins.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;, xlim=(1820000, 2350000), ylim=(1780000, 2150000), xticks=[], yticks=[], title=&#39;Oaxaca&#39;) axins2.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;, xlim=(1425000, 1680000), ylim=(2229203, 2453767), xticks=[], yticks=[], title=&#39;Guanajuato&#39;) # Elimina marco de la gráfica principal ax.set_axis_off() # Establece líneas de los recuadros a la gráfica principal ax.indicate_inset_zoom(axins) ax.indicate_inset_zoom(axins2) # Tamaño de la gráfica final fig.set_size_inches(12, 8) . Es notable la diferencia en la complejidad entre los municipios de Guanajuato y Oaxaca. . El pequeño recuadro al interior que señala el área del mapa principal que se está señalando no se nota mucho porque se confunde con el color de los polígonos. Para evitar eso, podemos recolorear el borde de los recuadros para que sea posible asociarlos usando el color. Esta idea la vi en un mapa creado por @ElenoAM 🤯 y me pareció incluso más limpia que tener varias líneas indicando el zoom. Mapa del fracaso de la política de suelo y de vivienda de @FelipeCalderon y de @VicenteFoxQue.Desarrollos inmobiliarios periurbanos, mal ubicados, sin servicios, empleo... pic.twitter.com/dvqZRi8euR . &mdash; Adrián (@ElenoAM) March 11, 2021 . Veamos ahora cómo queda tras quitar las líneas y ponerle colores al borde de los recuadros. . fig, ax = plt.subplots() # Añade recuadro 1 axins = ax.inset_axes([0.86, 0.05, 0.9, 0.9]) # Añade recuadro 2 axins2 = ax.inset_axes([0.2, -0.5, 0.5, 0.5]) # Gráfica principal mx.plot(column=&#39;ECI&#39;, legend=True, ax=ax, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;, legend_kwds={&#39;loc&#39;: &#39;lower left&#39;}) mx.boundary.plot(lw=0.05, color=&#39;k&#39;, ax=ax) # Gráfica recuadro 1 mx.plot(column=&#39;ECI&#39;, legend=False, ax=axins, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;) mx.boundary.plot(lw=0.25, color=&#39;k&#39;, ax=axins) edos.boundary.plot(lw=1, color=&#39;red&#39;, ax=axins) # Gráfica recuadro 2 mx.plot(column=&#39;ECI&#39;, legend=False, ax=axins2, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;) mx.boundary.plot(lw=0.25, color=&#39;k&#39;, ax=axins2) edos.boundary.plot(lw=1, color=&#39;red&#39;, ax=axins2) # limita área a mostrar recuadro 1 y 2 axins.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;, xlim=(1820000, 2350000), ylim=(1780000, 2150000), xticks=[], yticks=[], title=&#39;Oaxaca&#39;) axins2.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;, xlim=(1425000, 1680000), ylim=(2229203, 2453767), xticks=[], yticks=[], title=&#39;Guanajuato&#39;) # Elimina marco de la gráfica principal ax.set_axis_off() # Establece líneas de los recuadros a la gráfica principal ax.indicate_inset_zoom(axins) ax.indicate_inset_zoom(axins2) # Parametros para cambiar colores color_insets = [&#39;red&#39;, &#39;blue&#39;] color_index = 0 # colorea los cuadros interiores for p in ax.patches: if isinstance(p, plt.matplotlib.patches.Rectangle): p.set_edgecolor(color_insets[color_index]) p.set_linewidth(2) color_index += 1 else: # Esconde las líneas hacia la gráfica principal p.set_visible(False) # Colorea el marco del recuadro 1 y aumenta grosor de línea for sp in axins.spines.values(): sp.set_color(color_insets[0]) sp.set_linewidth(2) # Colorea el marco del recuadro 2 y aumenta grosor de línea for sp in axins2.spines.values(): sp.set_color(color_insets[1]) sp.set_linewidth(2) # Tamaño final fig.set_size_inches(12, 8) . Ahora es más fácil identificar el área que se está acercando. . Recuadro de contexto . En un recuadro de contexto el mapa principal corresponde a la región que vamos a analizar con detalle y usamos el recuadro para resaltar la posición de esa región con respecto al país. Para lograr este efecto no usamos la función ax.inset_axes(), sino una muy parecida que es fig.add_axes() y recibe los mismos argumentos $x, y, w, h$. La diferencia es que ahora no tendremos las líneas desde el recuadro hasta el mapa principal. . fig, ax = plt.subplots() # Mapa principal oax.plot(column=&#39;ECI&#39;, legend=True, scheme=&#39;quantiles&#39;, k=5, cmap=&#39;viridis_r&#39;, ax=ax) # Borde de los municipios oax.boundary.plot(linewidth=0.5, color=&#39;gray&#39;, ax=ax) # Inserta recuadro ax_mex = fig.add_axes([0.91, 0.11, 0.35, 0.35], ) # Dibuja los estados del país en el recuadro edos.boundary.plot(color=&#39;gray&#39;, ax=ax_mex) # Resalta Oaxaca con el color rojo edos.query(&#39;CVE_EDO==&quot;20&quot;&#39;).plot(color=&#39;red&#39;, ax=ax_mex) # Establece tamaño final fig.set_size_inches(12, 8) . Escalas . Cuando graficamos mapas con distintos niveles de acercamiento/alejamiento es importante acompañar cada mapa con una barra de escala que nos permita saber cómo comparar longitudes. Para Matplotlib existe la excelente librería matplotlib-scalebar para agregar estas escalas y afortunadamente funciona bien con mapas de Geopandas. A continuación añadimos las escalas para cada mapa. . ax.set(xticks=[], yticks=[], title=&#39;Oaxaca&#39;) ax_mex.set(xticks=[], yticks=[], title=&#39;México&#39;) # Barras de escala scalebar_oax = ScaleBar(1, &quot;m&quot;, length_fraction=0.2, location=&#39;lower left&#39;, ) ax.add_artist(scalebar_oax) scalebar_mex = ScaleBar(1, &quot;m&quot;, length_fraction=0.25, location=&#39;lower left&#39;) ax_mex.add_artist(scalebar_mex) fig . De esta forma podemos reconocer mejor las dimensiones de Oaxaca y sus municipios con respecto a todo el país. . Recuadro de &#225;reas no contiguas . Este tipo de recuadro sirve para mostrar juntas áreas que no están cerca o es muy difícil ver juntas en su ubicación normal. El primer caso que viene a mi cabeza es el del departamento de San Andrés y Providencia en Colombia 🟨🟨🟦🟥. San Andrés y Providencia son unas diminutas islas ubicadas a 774 km del noreste de Colombia y que conforman uno de los 32 departamentos del país. En los mapas de división política-administrativa del país estas islas siempre aparecen como recuadros al lado de la parte continental, con una escala mucho mayor para poder ser visibles. . Para visualizarlas usemos este mapa de Colombia con la división por departamentos que también puede descargarse desde el Marco Geostadístico Nacional. Usaremos el sistema de coordenadas MAGNA-SIRGAS / Colombia Bogota zone. . col = gpd.read_file(&#39;mapa_colombia/&#39;).to_crs(epsg=3116) col.head() . DPTO_CCDGO DPTO_CNMBR DPTO_NANO_ DPTO_CACTO DPTO_NANO SHAPE_AREA SHAPE_LEN geometry . 0 18 | CAQUETÁ | 1981 | Ley 78 del 29 de Diciembre de 1981 | 2020 | 7.318485 | 21.384287 | POLYGON ((909199.769 818940.314, 909214.572 81... | . 1 19 | CAUCA | 1857 | 15 de junio de 1857 | 2020 | 2.534419 | 13.950263 | POLYGON ((735237.006 860162.569, 735286.223 86... | . 2 86 | PUTUMAYO | 1991 | Articulo 309 Constitucion Politica de 1991 | 2020 | 2.107965 | 12.707922 | POLYGON ((711344.106 654182.557, 711400.239 65... | . 3 76 | VALLE DEL CAUCA | 1910 | Decreto No 340 de 16 de Abril de 1910 | 2020 | 1.679487 | 12.650870 | MULTIPOLYGON (((777973.333 1049936.014, 777964... | . 4 94 | GUAINÍA | 1991 | Articulo 309 Constitucion Politica de 1991 | 2020 | 5.747937 | 21.179051 | POLYGON ((1712400.898 927096.060, 1712774.632 ... | . Al graficar el mapa del país a duras penas podremos notar las islas en la parte superior derecha. . fig, ax = plt.subplots() col.plot(ax=ax) fig.set_size_inches(10, 10) . Para hacer un típico mapa político administrativo, vamos a añadir dos objetos axes con fig.add_axes() en los que graficaremos a cada isla. Para encontrar las coordenadas de las islas lo mejor es usar el mapa de http://epsg.io/map#srs=3116&amp;x&amp;y&amp;z=10&amp;layer=streets. También agregamos las barras de escala para dimensionar mejor lo pequeñas que son. . fig, ax = plt.subplots() # Añade ax para San Andrés ax_san = fig.add_axes([0.25, 0.75, 0.05, 0.1]) # Añade ax para Providencia ax_prov = fig.add_axes([0.29, 0.77, 0.08, 0.08]) # Grafica Parte continental col.plot(ax=ax) # Grafica San Andrés col.plot(ax=ax_san) # Grafica Providencia col.plot(ax=ax_prov) # Limita los valores del eje x e y para cada gráfica ax.set(ylim=(0, 1_900_000), xlim=(400_000, 1_850_000), yticks=[], xticks=[]) ax_san.set(ylim=(1_883_000, 1_902_000), xlim=(163_000, 172_000), yticks=[], xticks=[]) ax_prov.set(ylim=(1_975_000, 1_990_000), xlim=(204_044, 211_100), yticks=[], xticks=[]) # Títulos de las gráficas ax.set_title(&#39;Colombia&#39;, fontsize=20) ax_san.set_title(&#39;San n Andrés&#39;, fontsize=7) ax_prov.set_title(&#39;Providencia&#39;, fontsize=7) # Añade barras de escala scalebar_col = ScaleBar(1, &quot;m&quot;, length_fraction=0.2, location=&#39;lower left&#39;, ) ax.add_artist(scalebar_col) scalebar_san = ScaleBar(1, &quot;m&quot;, length_fraction=1, location=&#39;upper center&#39;, font_properties={&#39;size&#39;: 7}) ax_san.add_artist(scalebar_san) scalebar_prov = ScaleBar(1, &quot;m&quot;, length_fraction=0.8, location=&#39;upper center&#39;, box_alpha=0, font_properties={&#39;size&#39;: 7}) ax_prov.add_artist(scalebar_prov) # Tamaño final de la gráfica fig.set_size_inches(10, 10) .",
            "url": "http://blog.jjsantoso.com/zoom-mapas-geopandas/",
            "relUrl": "/zoom-mapas-geopandas/",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Etiquetado de variables y valores en las encuestas de INEGI usando Python",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Introducci&#243;n . Hace poco me tocó trabajar con los datos de una encuesta de INEGI y usé Python para hacer el análisis descriptivo. Quienes trabajamos con datos del INEGI hemos visto que es usual que los archivos de datos abiertos vengan en varias carpetas que contienen tanto los datos como los metadatos e información adicional sobre la encuesta. Por ejemplo, si descargamos los datos para la Encuesta Nacional de Seguridad Pública Urbana (ENSU) veremos que vienen 3 carpetas, cada una corresponde a una sección de la encuesta: . Note: Los datos se pueden descargar directamente de la página de INEGI, pero aquí dejo una copia de los que yo usé para este tutorial. Descargar datos . conjunto_de_datos_VIV_ENSU_12_2020: Cuestionario sociodemográfico sección I y II | conjunto_de_datos_CS_ENSU_12_2020: Cuestionario sociodemográfico sección III | conjunto_de_datos_CB_ENSU_12_2020: Cuestionario principal de la encuesta sección I, II, III y IV | . Si vemos al interior de uno de estos módulos, la estructura incluye las siguientes carpetas: . Catálogos: tiene los catálogos para cada variable en el cuestionario | Conjunto de datos: tiene los datos principales de la encuesta | Diccionario de datos: el nombre e información de cada variable | Metadatos: tiene información de la encuesta. | Modelo entidad relación: es un diagrama que muestra cómo se relacionan los diferentes conjuntos de datos. | . . Si echamos un vistazo rápido a los datos en Excel (conjunto_de_datos/conjunto_de_datos_CB_ENSU_12_2020.csv) veremos que la mayor parte de las variables viene codificada. Solo con este archivo no podemos saber qué es cada columna y cuáles es el significado de sus valores. Nos hace falta el diccionario de variables y los catálogos para poder interpretarlas. . . En el archivo diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv tenemos cuál es el texto de cada pregunta. De ahí sabemos que, por ejemplo, la pregunta BP1_1 es &quot;Percepción de seguridad en la ciudad&quot;. Estos valores se conocen como etiquetas de las variables. . Por otro lado, dentro de la carpeta catalogos viene un archivo csv por cada variable del conjunto de datos. . . Este archivo nos dice cómo debemos transformar los valores numéricos de las categorías por sus valores de texto. Por ejemplo, si abrimos el archivo &quot;BP1_1.csv&quot; su contenido nos muestra que para la variable BP1_1 debemos interpretar que un 1 corresponde a la categorías &quot;seguro?&quot;, el 2 corresponde a &quot;inseguro?&quot; y el 9 a &quot;No sabe/No responde&quot;. Estos valores se conocen como etiquetas de los valores. . Note: Las etiquetas de valores tiene sentido para variables categóricas, es decir las que tienen pocos valores nominales. Para variables numéricas o puramente de texto no es necesario usar etiquetas de valores. . . Es evidente que para manejar una encuesta es fundamental conocer las etiquetas de las variables y sus valores. Sería mucho más fácil si estas etiquetas estuvieran incluidas en el mismo archivo junto con los datos, pero como están en formato .csv no es posible guadar esa información en un solo archivo y por tanto, termina repartida en muchos. Entonces, nuestro objetivo es integrar el diccionario y el catálogo a los datos para que sea más fácil hacer nuestro análisis. Queremos que en las tablas o gráficas que hagamos, las variables categóricas aparezcan como texto, en lugar de los valores numéricos que asignó INEGI. De igual forma, nos gustaría que en lugar de aparecer el nombre de la variable como en la base de datos, aparezca su descripción. Para lograr esto usaremos objetos tipo diccionario nativos de Python y dataframes de Pandas. . Tip: Otros formatos de datos, como por ejemplo los archivos .dta de Stata o .sav de SPSS sí permiten guardar esas etiquetas junto con los datos, sin embargo, esos no son formatos de datos abiertos que sean fácilmente accesible. En algunos casos, como en la ENOE, INEGI también publica archivos .dta y .sav. . Datos . Primero, vamos a importar las librerías necesarias. . import glob import sys import pandas as pd print(&#39;Python&#39;, sys.version) print(pd.__name__, pd.__version__) . Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] pandas 1.1.3 . Para ilustrar, vamos a seleccionar los datos de la carpeta conjunto_de_datos_CB_ENSU_12_2020 (Cuestionario principal de la encuesta sección I, II, III y IV). . datos = pd.read_csv(&#39;conjunto_de_datos_CB_ENSU_12_2020/conjunto_de_datos/conjunto_de_datos_CB_ENSU_12_2020.csv&#39;) datos.head() . ID_VIV ID_PER UPM VIV_SEL R_SEL CVE_ENT NOM_ENT CVE_MUN NOM_MUN LOC ... BP4_1_5 BP4_1_6 BP4_1_7 BP4_1_8 BP4_1_9 FAC_SEL DOMINIO EST UPM_DIS EST_DIS . 0 100188.049 | 0100188.049.03 r | 100188 | 1 | 3 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 4046 | U r | 3 | 10 | 1390 | . 1 100188.072 | 0100188.072.06 r | 100188 | 2 | 6 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 6069 | U r | 3 | 10 | 1390 | . 2 100188.093 | 0100188.093.03 r | 100188 | 3 | 3 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 3035 | U r | 3 | 10 | 1390 | . 3 100188.111 | 0100188.111.01 r | 100188 | 5 | 1 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 3035 | U r | 3 | 10 | 1390 | . 4 100295.009 | 0100295.009.01 r | 100295 | 1 | 1 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 1704 | U r | 4 | 20 | 1400 | . 5 rows × 176 columns . datos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22283 entries, 0 to 22282 Columns: 176 entries, ID_VIV to EST_DIS dtypes: float64(1), int64(84), object(91) memory usage: 29.9+ MB . Necesitamos el diccionario de variables y los catálogos. Para el diccionario de variables cargamos el archivo conjunto_de_datos_CB_ENSU_12_2020/diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv . preguntas = pd.read_csv(&#39;conjunto_de_datos_CB_ENSU_12_2020/diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv&#39;, encoding=&#39;latin1&#39;) preguntas.head(10) . NOMBRE_CAMPO NEMONICO TIPO LONGITUD RANGO_CLAVES . 0 Identificador de vivienda seleccionada | ID_VIV | Alfanumérico | 11 | 0100001.001,,3299999.999 r | . 1 Identificador del informante seleccionado | ID_PER | Alfanumérico | 14 | 0100001.001.01,..., 3299999.999.30 r | . 2 Unidad Primaria de Muestreo | UPM | Numérico | 7 | 01000013299999 r | . 3 Vivienda seleccionada | VIV_SEL | Numérico | 2 | 0109 r | . 4 Número de renglón de la persona seleccionada | R_SEL | Numérico | 2 | 0130 r | . 5 Clave Entidad | CVE_ENT | Numérico | 2 | 01 r | . 6 Clave Entidad | CVE_ENT | Numérico | 2 | 02 r | . 7 Clave Entidad | CVE_ENT | Numérico | 2 | 03 r | . 8 Clave Entidad | CVE_ENT | Numérico | 2 | 04 r | . 9 Clave Entidad | CVE_ENT | Numérico | 2 | 05 r | . Este nos muestra cómo aparece cada variable en el archivo de datos (&quot;NEMONICO&quot;) y cuál es su descripción (&quot;NOMBRE_CAMPO&quot;), además contiene el tipo y rango de valores que puede tener cada variable. Por esta razón, el nombre de cada variable puede estar repetido varias veces. Lo que haremos es eliminar los duplicados y quedarnos con los valores únicos, para después crear un objeto diccionario que tenga como llaves el &quot;NEMONICO&quot; y como valores el &quot;NOMBRE_CAMPO&quot;. A continuación se ve el proceso y el resultado: . dicc_preguntas = preguntas.drop_duplicates(subset=[&#39;NEMONICO&#39;]).set_index(&#39;NEMONICO&#39;)[&#39;NOMBRE_CAMPO&#39;].to_dict() print(str(dicc_preguntas)[:500]) . {&#39;ID_VIV&#39;: &#39;Identificador de vivienda seleccionada&#39;, &#39;ID_PER&#39;: &#39;Identificador del informante seleccionado&#39;, &#39;UPM&#39;: &#39;Unidad Primaria de Muestreo &#39;, &#39;VIV_SEL&#39;: &#39;Vivienda seleccionada&#39;, &#39;R_SEL&#39;: &#39;Número de renglón de la persona seleccionada&#39;, &#39;CVE_ENT&#39;: &#39;Clave Entidad&#39;, &#39;NOM_ENT&#39;: &#39;Nombre de la Entidad&#39;, &#39;CVE_MUN&#39;: &#39;Clave Municipio&#39;, &#39;NOM_MUN&#39;: &#39;Nombre del Municipio&#39;, &#39;LOC&#39;: &#39;Localidad&#39;, &#39;CD&#39;: &#39;Ciudad&#39;, &#39;NOM_CD&#39;: &#39;Nombre de la Ciudad&#39;, &#39;PER&#39;: &#39;Periodo de la entrevista&#39;, &#39;R_DEF&#39;: &#39;Resultado definiti . Para el catálogo tenemos que trabajar un poco más porque la información está en muchos archivos. Primero vamos a generar una lista de todos los archivos en la carpeta catalogo. . dir_catalogos = &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos/&#39; archivos_catalogo_respuestas = glob.glob1(dir_catalogos, &#39;*.csv&#39;) archivos_catalogo_respuestas[:10] . [&#39;BP1_1.csv&#39;, &#39;BP1_10_1.csv&#39;, &#39;BP1_10_2.csv&#39;, &#39;BP1_10_3.csv&#39;, &#39;BP1_10_4.csv&#39;, &#39;BP1_10_5.csv&#39;, &#39;BP1_2_01.csv&#39;, &#39;BP1_2_02.csv&#39;, &#39;BP1_2_03.csv&#39;, &#39;BP1_2_04.csv&#39;] . A continuación vamos a leer cada uno de los archivos individuales y generar un diccionario por comprensión que contenga como llaves el &quot;NEMONICO&quot; y los valores serán otro diccionario que tiene la relación de los valores numéricos y de texto para los valores de cada variable. . dicc_respuestas = { f[:-4]: pd.read_csv(f&#39;{dir_catalogos}/{f}&#39;, encoding=&#39;latin1&#39;, index_col=f[:-4])[&#39;descrip&#39;].to_dict() for f in archivos_catalogo_respuestas } print(str(dicc_respuestas)[:500]) . {&#39;BP1_1&#39;: {1: &#39;seguro?&#39;, 2: &#39;inseguro?&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_1&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_2&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_3&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_4&#39; . Ahora tenemos dos diccionarios, uno con las etiquetas de las variables (dicc_preguntas) y otro con las etiquetas de los valores de cada pregunta dicc_respuestas. En ambos diccionarios la llave principal es el némonico de la pregunta, por tanto si queremos saber cuáles son las etiquetas de variables y valores solo tenemos que indexar los diccionarios con el nemónico correspondiente, por ejemplo para &quot;SEX&quot; y &quot;BP1_1&quot; obtenemos: . print(&#39;Etiqueta de variable:&#39;, dicc_preguntas[&#39;SEX&#39;], &#39; nEtiqueta de valores&#39;,dicc_respuestas[&#39;SEX&#39;]) . Etiqueta de variable: Sexo Etiqueta de valores {1: &#39;Hombre&#39;, 2: &#39;Mujer&#39;} . print(&#39;Etiqueta de variable:&#39;, dicc_preguntas[&#39;BP1_1&#39;], &#39; nEtiqueta de valores&#39;,dicc_respuestas[&#39;BP1_1&#39;]) . Etiqueta de variable: Percepción de seguridad en la ciudad Etiqueta de valores {1: &#39;seguro?&#39;, 2: &#39;inseguro?&#39;, 9: &#39;No sabe / no responde&#39;} . Uso de las etiquetas . Ya que tenemos las etiquetas, veamos cómo podemos usarlas para interpretar mejor en nuestros análisis. Hagamos una tabulación cruzada para ver la distribución de respuestas de las variables &quot;SEX&quot; y &quot;BP1_1&quot; . Warning: En estos ejemplos no estamos considerando que cada observación tiene una ponderación distinta (variable FAC_SEL) por tanto los resultados no son estimaciones válidas. En una próxima entrada veremos cómo integrar las ponderaciones. . pd.crosstab(datos[&#39;SEX&#39;], datos[&#39;BP1_1&#39;]) . BP1_1 1 2 9 . SEX . 1 4268 | 5849 | 33 | . 2 3703 | 8379 | 51 | . El índice del dataframe y los nombres de las columnas contienen los valores numéricos de las categorías. Vamos a reemplazarlos por sus valores de texto renombrándolo con el método .rename() . pd.crosstab(datos[&#39;SEX&#39;], datos[&#39;BP1_1&#39;]) .rename(index=dicc_respuestas[&#39;SEX&#39;], columns=dicc_respuestas[&#39;BP1_1&#39;]) . BP1_1 seguro? inseguro? No sabe / no responde . SEX . Hombre 4268 | 5849 | 33 | . Mujer 3703 | 8379 | 51 | . Ahora es mucho más fácil de entender esta tabla. . Vamos a hacer otra tabla similar a la anterior, pero en este caso desagregando por más variables y usando el método groupby: . vars_by = [&#39;SEX&#39;, &#39;BP1_1&#39;, &#39;BP1_5_1&#39;] grouped = datos.groupby(vars_by).agg(N=(&#39;ID_PER&#39;, &#39;count&#39;)) grouped.head(15) . N . SEX BP1_1 BP1_5_1 . 1 1 1 1255 | . 2 2821 | . 3 190 | . 9 2 | . 2 1 3587 | . 2 2010 | . 3 249 | . 9 3 | . 9 1 14 | . 2 16 | . 3 3 | . 2 1 1 1463 | . 2 1995 | . 3 243 | . 9 2 | . Nuevamente reemplazamos los valores de las categorías usando el método .rename(). Hacemos un loop para reemplazar las categorías de cada variable. Como tenemos varios niveles en el índice, especificamos la opción level para que use solo el catálogo con la variable que le corresponde. . for v in vars_by: grouped.rename(index=dicc_respuestas[v], level=v, inplace=True) grouped . N . SEX BP1_1 BP1_5_1 . Hombre seguro? Sí 1255 | . No 2821 | . No aplica 190 | . No sabe / no responde 2 | . inseguro? Sí 3587 | . No 2010 | . No aplica 249 | . No sabe / no responde 3 | . No sabe / no responde Sí 14 | . No 16 | . No aplica 3 | . Mujer seguro? Sí 1463 | . No 1995 | . No aplica 243 | . No sabe / no responde 2 | . inseguro? Sí 5783 | . No 2192 | . No aplica 400 | . No sabe / no responde 4 | . No sabe / no responde Sí 19 | . No 23 | . No aplica 7 | . No sabe / no responde 2 | . Para que sea más fácil de ver, reestructuramos la tabla usando .unstack(). . grouped.unstack(&#39;BP1_1&#39;) . N . BP1_1 No sabe / no responde inseguro? seguro? . SEX BP1_5_1 . Hombre No 16.0 | 2010.0 | 2821.0 | . No aplica 3.0 | 249.0 | 190.0 | . No sabe / no responde NaN | 3.0 | 2.0 | . Sí 14.0 | 3587.0 | 1255.0 | . Mujer No 23.0 | 2192.0 | 1995.0 | . No aplica 7.0 | 400.0 | 243.0 | . No sabe / no responde 2.0 | 4.0 | 2.0 | . Sí 19.0 | 5783.0 | 1463.0 | . Ya integramps las etiquetas de los valores, ahora faltan las etiquetas de las variables. Para eso usaremos el método .rename_axis() . cuadro = grouped.unstack(&#39;BP1_1&#39;) .rename_axis(index=dicc_preguntas, columns=dicc_preguntas) cuadro . N . Percepción de seguridad en la ciudad No sabe / no responde inseguro? seguro? . Sexo Cambiar sus hábitos respecto a llevar cosas de valor por temor a sufrir algún delito . Hombre No 16.0 | 2010.0 | 2821.0 | . No aplica 3.0 | 249.0 | 190.0 | . No sabe / no responde NaN | 3.0 | 2.0 | . Sí 14.0 | 3587.0 | 1255.0 | . Mujer No 23.0 | 2192.0 | 1995.0 | . No aplica 7.0 | 400.0 | 243.0 | . No sabe / no responde 2.0 | 4.0 | 2.0 | . Sí 19.0 | 5783.0 | 1463.0 | . Nuestro cuadro ya es entendible, podemos exportarlo a Excel y verificar que tenemos las etiquetas: . cuadro.to_excel(&#39;reporte.xlsx&#39;) . . Si usas Stata...(o incluso si no) . Como dije antes, el formato .dta permite guardar las etiquetas de variables y valores junto con los datos. Los DataFrames de Pandas traen de forma nativa el método .to_stata() para exportar la información a este formato. Para que Stata reconozca correctamnete las etiquetas de valores es necesario que en pandas las variables sean de tipo categoria. A continuación, vamos a seleccionar un subconjunto de variables, reemplazaremos sus valores numéricos por las categorías de texto y convertiremos la variable en tipo categórica: . vars_export = [&#39;SEX&#39;, &#39;BP1_1&#39;, &#39;BP1_2_01&#39;, &#39;BP1_2_02&#39;, &#39;BP1_2_03&#39;, &#39;BP1_2_04&#39;, &#39;BP1_2_05&#39;, &#39;BP1_2_06&#39;, &#39;BP1_2_07&#39;, &#39;BP1_2_08&#39;, &#39;BP1_2_09&#39;, &#39;BP1_2_10&#39;, &#39;BP1_2_11&#39;, &#39;BP1_2_12&#39;] datos_stata = datos[vars_export].apply(lambda s: s.map(dicc_respuestas[s.name])).astype(&#39;category&#39;) datos_stata . SEX BP1_1 BP1_2_01 BP1_2_02 BP1_2_03 BP1_2_04 BP1_2_05 BP1_2_06 BP1_2_07 BP1_2_08 BP1_2_09 BP1_2_10 BP1_2_11 BP1_2_12 . 0 Hombre | seguro? | Seguro(a) | No aplica | Seguro(a) | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | . 1 Hombre | inseguro? | Seguro(a) | Inseguro(a) | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | Inseguro(a) | Inseguro(a) | Inseguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | . 2 Hombre | inseguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | No aplica | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . 3 Mujer | inseguro? | Inseguro(a) | No aplica | Inseguro(a) | No aplica | Inseguro(a) | No aplica | No aplica | No aplica | No aplica | Inseguro(a) | No aplica | No aplica | . 4 Hombre | seguro? | Seguro(a) | Inseguro(a) | Seguro(a) | No aplica | No aplica | No aplica | Inseguro(a) | No aplica | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 22278 Hombre | inseguro? | Seguro(a) | Seguro(a) | Seguro(a) | No aplica | Inseguro(a) | Seguro(a) | Inseguro(a) | Inseguro(a) | Seguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | . 22279 Hombre | inseguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | No aplica | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Seguro(a) | Seguro(a) | No aplica | . 22280 Hombre | seguro? | Seguro(a) | No aplica | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | No aplica | . 22281 Mujer | inseguro? | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . 22282 Mujer | seguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | Inseguro(a) | . 22283 rows × 14 columns . Este dataframe lo exportaremos a Stata, junto con el diccionario que contiene las etiquetas de variables, especificando en la opción variable_labels. . datos_stata.to_stata(&#39;datos_stata.dta&#39;, write_index=False, variable_labels=dicc_preguntas) . Al abrir el arcivo en Stata podemos ver que efectivamente se guardaron las etiquetas de los valores y de las variables: . . Hasta donde entiendo, esta sería una forma más fácil de etiquetar datos provenientes de INEGI para usuarios de Stata que haciendo el procedimiento entero en Stata. Igualmente, para los que no son usuarios de Stata, pero sí de Python o R y quieren conservar las variables categóricas etiquetadas este es un buen formato. . datos_2 = pd.read_stata(&#39;datos_stata.dta&#39;) datos_2.dtypes . SEX category BP1_1 category BP1_2_01 category BP1_2_02 category BP1_2_03 category BP1_2_04 category BP1_2_05 category BP1_2_06 category BP1_2_07 category BP1_2_08 category BP1_2_09 category BP1_2_10 category BP1_2_11 category BP1_2_12 category dtype: object .",
            "url": "http://blog.jjsantoso.com/etiquetas-encuestas-inegi/",
            "relUrl": "/etiquetas-encuestas-inegi/",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Generando archivos de Excel con formatos y gráficas usando Python",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Excel es un tipo de archivo muy común para compartir datos. No a todos les encanta, pero tiene la ventaja de que es muy popular y muchas personas no familiarizadas con la programación lo usan para su análisis. Una de las funciones de Excel que no es fácil de replicar con otras herramientas es la posibilidad de aplicar estilo a las celdas y generar tablas o reportes más actractivos para presentar los datos. En esta entrada veremos cómo, usando Python, podemos exportar datos a archivos de Excel aplicando estilos y formatos a las celdas. . Como ejemplo usaremos los datos de indicadores de desarrollo del Banco Mundial. En su página puedes ver y descargar mucha información. Acá puedes decargar los archivos que en particular uso en esta entrada. Descárgalos y guárdalos en una carpeta llamada datos. . Lo que haremos es crear fichas para los metadatos que sean más fáciles de leer, ya que leer esta información tal como viene en el archivo original es un poco difícil. La idea es pasar de esto: . . a esto: . . Además, usaremos el formato condicional para establecer detalles como el color de las celdas dependiendo de sus valores. Luego, como bonus, aprovechando que ya sabremos usar xlsxwriter, veremos cómo crear gráficas que se guardan dentro del archivo de Excel. . Para seguir este tutorial es necesario tener instaladas las bibliotecas pandas y xlsxwriter. La biblioteca xlsxwriter es excelente y está muy bien documentada por su autor. La mayor parte de lo que aquí hacemos se basa en sus ejemplos. xlsxwriter se instala usando pip . pip install XlsxWriter . import xlsxwriter import pandas as pd print(xlsxwriter.__name__, xlsxwriter.__version__) print(pd.__name__, pd.__version__) . xlsxwriter 1.2.7 pandas 0.24.2 . Por brevedad solo leeremos las 10 primeras filas del archivo que contienen los metadatos. . metadata = pd.read_csv(&#39;datos/WDISeries.csv&#39;, nrows=10).fillna(&#39;&#39;) metadata.head() . Series Code Topic Indicator Name Short definition Long definition Unit of measure Periodicity Base Period Other notes Aggregation method ... Notes from original source General comments Source Statistical concept and methodology Development relevance Related source links Other web links Related indicators License Type Unnamed: 20 . 0 AG.AGR.TRAC.NO | Environment: Agricultural production | Agricultural machinery, tractors | | Agricultural machinery refers to the number of... | | Annual | | | Sum | ... | | | Food and Agriculture Organization, electronic ... | A tractor provides the power and traction to m... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 1 AG.CON.FERT.PT.ZS | Environment: Agricultural production | Fertilizer consumption (% of fertilizer produc... | | Fertilizer consumption measures the quantity o... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Fertilizer consumption measures the quantity o... | Factors such as the green revolution, has led ... | | | | CC BY-4.0 | | . 2 AG.CON.FERT.ZS | Environment: Agricultural production | Fertilizer consumption (kilograms per hectare ... | | Fertilizer consumption measures the quantity o... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Fertilizer consumption measures the quantity o... | Factors such as the green revolution, has led ... | | | | CC BY-4.0 | | . 3 AG.LND.AGRI.K2 | Environment: Land use | Agricultural land (sq. km) | | Agricultural land refers to the share of land ... | | Annual | | | Sum | ... | | | Food and Agriculture Organization, electronic ... | Agricultural land constitutes only a part of a... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 4 AG.LND.AGRI.ZS | Environment: Land use | Agricultural land (% of land area) | | Agricultural land refers to the share of land ... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Agriculture is still a major sector in many ec... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 5 rows × 21 columns . Aplicar formato a las celdas . A continuación vamos a crear un libro (workbook) con xlsxwriter llamado fichas_metadatos.xlsx. A este libro le agregamos una hoja (worksheet). . workbook = xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) worksheet = workbook.add_worksheet() . Empezaremos a configurar el tamaño de las filas y columnas de la hoja. Esto lo hacemos con los métodos .set_column() y .set_row(). En .set_column() es importante especificar un rango, incluso si es una sola columna, de lo contrario obtendríamos un error. Las unidades son las mismas que usa Excel para fijar el ancho y alto de las celdas. Cuando terminamos de editar cerramos el libro con workbook.close() y esto escribe el contenido al archivo. . worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 20) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) # Cambiamos tamaños de filas worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 300) worksheet.set_row(5, 100) workbook.close() . Así se ve el resultado hasta ahora: Las celdas no tienen contenido pero sí tienen tamaños diferentes. . Ahora vamos a combinar varias celdas y escribir algo de contenido. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;]) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;]) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;]) . Con el método .merge_range() combinamos las celdas en el rango B1:E1 en una sola celda. Además, escribimos en esta misma celda el contenido del nombre del indicador (Indicator Name) del primer indicador que está en el DataFrame metadata. | Con el método .write() escribimos contenido específico para cada una de las celdas en las siguientes filas. Por ejemplo, en la celda B2 escribimos el texto Código de serie: y en C2 escribimos el código del indicador (Series Code) que viene en la primera fila del indicador. En las celdas D2 y E2 escribimos el tópico del indicador. | Aquí también cambiamos un poco la sintaxis para abrir el libro usando la expresion with ... as ...:. De esta forma todas las operaciones deben quedar bajo la indentación y no hay necesidad de cerrar explícitamente el libro. El resultado se ve así: | . Ahora continuamos escribiendo otras variables en las filas. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;]) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;]) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;]) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;) worksheet.write(&#39;C3&#39;, metadata.loc[0, &#39;Aggregation method&#39;]) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;) worksheet.write(&#39;E3&#39;, metadata.loc[0, &#39;Periodicity&#39;]) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[0, &#39;Long definition&#39;]) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[0, &#39;Source&#39;]) . . Ya que tenemos el contenido de las celdas, lo que nos falta es el aplicar formato para que se vea más atractivo. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Formato de titulo formato_titulo = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;center&#39;, &#39;valign&#39;: &#39;vcenter&#39;, &#39;fg_color&#39;: &#39;#333f4f&#39;, &#39;font_color&#39;: &#39;white&#39;, &#39;text_wrap&#39;: True}) # Formato de variables formato_variables = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;fg_color&#39;: &#39;#ddebf7&#39;, &#39;font_color&#39;: &#39;black&#39;, &#39;text_wrap&#39;: True}) # Formato del texto normal formato_normal = workbook.add_format({ &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;text_wrap&#39;: True}) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;], formato_titulo) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;, formato_variables) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;], formato_normal) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;, formato_variables) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;], formato_normal) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;, formato_variables) worksheet.write(&#39;C3&#39;, metadata.loc[0, &#39;Aggregation method&#39;], formato_normal) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;, formato_variables) worksheet.write(&#39;E3&#39;, metadata.loc[0, &#39;Periodicity&#39;], formato_normal) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;, formato_variables) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[0, &#39;Long definition&#39;], formato_normal) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;, formato_variables) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[0, &#39;Source&#39;], formato_normal) . Para ello debemos definir los formatos en el libro con el método workbook.add_format() al que le pasamos un diccionaro con las opciones de formato, como por ejemplo si queremos que el texto aparezca en negrita, que esté alineado al centro, el color del texto y el del fondo de la celda. Todas las opciones disponibles y más ejemplos se pueden encontrar en la documentación de xlsxwriter. . Aquí definimos tres tipos de formato: el formato para el nombre del indicador (formato_titulo), el formato para las celdas que tienen el nombre de la variable (formato_variables) y el formato para escribir la información (formato_normal). Para aplicar el formato a una celda, o a un conjunto de ellas, debemos pasar el objeto formato como tercer input al método worksheet.write(). Y así luce el resultado final: . . Estas fichas son mucho más fáciles de leer que en el formato original. . Para finalizar esta parte solo nos queda iterar sobre todo el dataframe y crear una ficha de metadato para cada indicador, cada una en una hoja diferente: . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: for fila in metadata.index: # agrega nueva ahoja worksheet = workbook.add_worksheet() # nombre de la hoja worksheet.name = metadata.loc[fila, &#39;Series Code&#39;] # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Formato de titulo formato_titulo = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;center&#39;, &#39;valign&#39;: &#39;vcenter&#39;, &#39;fg_color&#39;: &#39;#333f4f&#39;, &#39;font_color&#39;: &#39;white&#39;, &#39;text_wrap&#39;: True}) # Formato de variables formato_variables = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;fg_color&#39;: &#39;#ddebf7&#39;, &#39;font_color&#39;: &#39;black&#39;, &#39;text_wrap&#39;: True}) # Formato del texto normal formato_normal = workbook.add_format({ &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;text_wrap&#39;: True}) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[fila, &#39;Indicator Name&#39;], formato_titulo) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;, formato_variables) worksheet.write(&#39;C2&#39;, metadata.loc[fila, &#39;Series Code&#39;], formato_normal) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;, formato_variables) worksheet.write(&#39;E2&#39;, metadata.loc[fila, &#39;Topic&#39;], formato_normal) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;, formato_variables) worksheet.write(&#39;C3&#39;, metadata.loc[fila, &#39;Aggregation method&#39;], formato_normal) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;, formato_variables) worksheet.write(&#39;E3&#39;, metadata.loc[fila, &#39;Periodicity&#39;], formato_normal) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;, formato_variables) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[fila, &#39;Long definition&#39;], formato_normal) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;, formato_variables) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[fila, &#39;Source&#39;], formato_normal) . Formato condicional . xlsxwriter también tiene la opción de aplicar formato condicional, esto es que el formato de la celda varíe dependiendo del valor de la celda. Esto es útil cuando reportamos datos y queremos, por ejemplo, que el color de la celda esté relacionado con su valor, de forma que un valor más alto da un color más intenso. Esto se puede hacer directamente en Excel como menciona este tutorial. . Para ver cómo funciona usaremos los datos de uno de los indicadores de desarrollo que vimos antes. El indicador es el de porcentaje de la tierra que es de uso agrícola y se encuentra en el archivo indicador_tierra_agro.csv. El dataframe que creamos se llama datos. . datos = pd.read_csv(&#39;datos/indicador_tierra_agro.csv&#39;) .rename(columns=lambda x: str(x)) datos.head() . Country Name Country Code Indicator Name Indicator Code 1960 1961 1962 1963 1964 1965 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 Unnamed: 64 . 0 Arab World | ARB | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 27.835643 | 27.826564 | 27.845522 | 27.847925 | 27.866972 | ... | 36.440808 | 36.472300 | 36.534503 | 36.607475 | 36.624759 | 36.610850 | NaN | NaN | NaN | NaN | . 1 Caribbean small states | CSS | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 5.518775 | 5.526186 | 5.533597 | 5.538538 | 5.484190 | ... | 6.198839 | 6.186983 | 6.215388 | 6.226504 | 6.245770 | 6.268000 | NaN | NaN | NaN | NaN | . 2 Central Europe and the Baltics | CEB | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 64.667028 | 64.625380 | 64.540412 | 64.591952 | 64.402941 | ... | 47.871658 | 47.515760 | 46.958264 | 46.895589 | 46.988619 | 46.715708 | NaN | NaN | NaN | NaN | . 3 Early-demographic dividend | EAR | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 35.425733 | 35.400661 | 35.345373 | 35.313357 | 35.283656 | ... | 41.439188 | 41.511835 | 41.537124 | 41.476279 | 41.464427 | 41.466296 | NaN | NaN | NaN | NaN | . 4 East Asia &amp; Pacific | EAS | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 43.329285 | 43.552552 | 43.807975 | 44.062835 | 44.395486 | ... | 48.795620 | 48.666721 | 48.340167 | 48.777005 | 47.678013 | 47.783780 | NaN | NaN | NaN | NaN | . 5 rows × 65 columns . Vamos a seleccionar solo el nombre del país (o grupo de países) y un par de años (2010 y 2016) de datos. Estos los vamos a guardar con formato condicional usando el siguiente código: . with pd.ExcelWriter(&#39;datos/indicador_agro_formato.xlsx&#39;, engine=&#39;xlsxwriter&#39;) as excelfile: workbook = excelfile.book # Agregamos datos al libro sheetname = &#39;agro&#39; datos[[&#39;Country Name&#39;, &#39;2000&#39;, &#39;2016&#39;]].to_excel(excelfile, sheet_name=sheetname, index=False) # Definimos formatos formato_porcentaje = workbook.add_format({&#39;num_format&#39;: &#39;0.0 %&#39;, &#39;font_color&#39;: &#39;#FFFFFF&#39;}) formato_bg_blanco = workbook.add_format({&#39;bg_color&#39;: &#39;#FFFFFF&#39;}) formato_escala = {&#39;type&#39;: &#39;2_color_scale&#39;, &#39;min_color&#39;: &#39;#D9D9D9&#39;, &#39;max_color&#39;: &#39;#808080&#39;} worksheet = excelfile.sheets[sheetname] # Configuramos formato a los datos worksheet.set_column(&#39;A:A&#39;, 40, formato_bg_blanco) worksheet.set_column(&#39;B2:C265&#39;, 20, formato_porcentaje) worksheet.conditional_format(&#39;B2:C265&#39;, formato_escala) . Acá algunos detalles de lo que hace: . En esta ocasión usamos la función pd.ExcelWriter para crear el archivo excelfile, que a su vez tiene un libro (workbook). Usamos el método .to_excel() para guardar los datos en excelfile. | Definimos varios formatos. El primero, formato_porcentaje, es para que los número se vean con el símbolo de porcentaje; el segundo formato, formato_bg_blanco, es para que en la primera columna no se noten las divisiones de las celdas; y el tercero, formato_escala, es para hacer un formato de escala de 2 colores que va desde un color para el valor mínimo hasta otro en el valor máximo. Los valores intermedios reciben colores intermedios según una escala de colores lineal. | Lo que sigue es aplicar estos formatos a los valores de la hoja donde habíamos guardado los datos. El formato condicional se especifica con el método worksheet.conditional_format(). | . El resultado en Excel es el siguiente: . . Gr&#225;ficas en Excel . Por último, aprovechando que le entendemos un poco a xlsxwriter, podemos ver cómo hacer gráficas de Excel con los datos del libro. Puede parecer un poco extraño hacer gráficas de Excel usando Python si podemos hacerlas directamente en Python usando una biblioteca como Matplotlib, sin embargo, una ventaja de Excel es que cualquiera puede editar luego las gráficas, mientras que las creadas mediante Matplotlib no son directamente editables. . En este caso usamos los mismos datos anteriores del indicador de porcentaje de la tierra con uso agrícola para hacer una gráfica de barras. . with pd.ExcelWriter(&#39;datos/grafica_indicador_agro.xlsx&#39;, engine=&#39;xlsxwriter&#39;) as excelfile: sheet_name = &#39;agro&#39; # Guarda los datos en el archivo datos[[&#39;Country Name&#39;, &#39;2000&#39;, &#39;2016&#39;]].to_excel(excelfile, sheet_name=sheet_name, index=False) # Obtiene el libro y hoja de trabajo workbook = excelfile.book worksheet = excelfile.sheets[sheet_name] # Crea un objeto tipo gráfica chart = workbook.add_chart({&#39;type&#39;: &#39;bar&#39;}) # Configura las series chart.add_series({ &#39;categories&#39;: f&#39;={sheet_name}!$A$2:$A$15&#39;, &#39;values&#39;: f&#39;={sheet_name}!$B$2:$B$15&#39;, &#39;name&#39;: f&#39;={sheet_name}!$B$1&#39;, &#39;gap&#39;: 8, }) chart.add_series({ &#39;categories&#39;: f&#39;={sheet_name}!$A$2:$A$15&#39;, &#39;values&#39;: f&#39;={sheet_name}!$C$2:$C$15&#39;, &#39;name&#39;: f&#39;={sheet_name}!$C$1&#39;, &#39;gap&#39;: 8, }) # configura opciones del gráfico chart.set_size({&#39;width&#39;: 650, &#39;height&#39;: 400}) chart.set_title({&#39;name&#39;: &#39;Porcentaje de tierra de uso agrícola&#39;, &#39;name_font&#39;: {&#39;size&#39;: 12}}) chart.set_x_axis({&#39;name&#39;: &#39;Porcentaje&#39;, &#39;major_gridlines&#39;: {&#39;visible&#39;: True}}) # Introduce el grafico en la hoja worksheet.insert_chart(&#39;D2&#39;, chart) . Para agregar una gráfica creamos un objeto chart usando el método workbook.add_chart(), en este caso especificando que es de tipo barra. | Al objeto chart le pasamos 2 series, la del año 2010 y la del 2016. Solo seleccionamos las primeras 15 observaciones para no sobrecargar la gráfica. Las categorías son los nombres de los países, columna A, y las especificamos con la sintaxis de Excel para rangos de valores. Hacemos igual con los valores, que están en las columnas B y C. Ponemos los nombres de las series y también la opción gap, esta última para controlar el ancho de las barras. | Configuramos el tamaño de la gráfica, el título y la etiqueta en el eje x. | Finalmente introducimos la gráfica en la hoja de los datos, en la celda D2. | . El resultado luce así en Excel: . . Pueden encontrar más ejemplos de gráficas en la documentación de xlsxwriter. . Con esto terminamos esta entrada cuyo objetivo principal fue introducir las funcionalidades que ofrece xlsxwriter para crear archivos de Excel con formatos mucho más ricos. A mí me ha servido mucho en mi trabajo, espero que a ti también pueda resultarte útil. .",
            "url": "http://blog.jjsantoso.com/excel-formato-graficas/",
            "relUrl": "/excel-formato-graficas/",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Trabajando con archivos de Excel complejos en Pandas",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Pandas es la biblioteca por excelencia para trabajar con datos tabulares en Python. Muchos de estos datos, especialmente los que vienen de instituciones públicas, están en formato de Excel, que en ocasiones pueden ser particularmente difíciles de leer por su estructura con celdas combinadas. En esta entrada veremos cómo usar algunas de las opciones y trucos de Pandas para leer estos archivos complejos de forma efectiva. Es necesario tener instaladas las bibliotecas pandas y xlrd. . Como ejemplo usaremos los tabulados del mercado laboral que publica el INEGI para México. Estos datos se pueden descargar desde su página: https://www.inegi.org.mx/programas/enoe/15ymas/default.html#Tabulados . En particular, usaremos los tabulados del primer trimestre de 2020 para el estado de Aguascalientes. Acá puedes decargar el archivo que uso en este notebook y acá uno con todos los estados. . El archivo luce de la siguiente forma visto en Excel. . Encabezado: . . Final del archivo: . . Podemos ver que la estructura de los datos es bastante compleja. Algunos de los desafíos de este archivo son: . El encabezado de la tabla de datos no empieza desde la primera fila, sino a partir de la fila 6 | El encabezado está conformado por 3 filas (la 6, 7 y 8) con diferentes niveles de información. Por ejemplo, el primer nivel contiene los valores &quot;Enero-Marzo 2020&quot;, &quot;Coeficientes de Variación (%)&quot;, &quot;Errores Estándar&quot; e &quot;Intervalos de Confianza al 90%&quot;. El segundo nivel solo contiene valores para las columnas que están bajo &quot;Intervalos de Confianza al 90%&quot;. El tercer nivel es la desagregación por sexo, junto con el total, excepto en la columna &quot;Intervalos de Confianza al 90%&quot;, donde representa los límites inferiores y superiores. | Hay hasta 4 niveles de desagregación de los indicadores, que vienen especificados en las columnas A, B, C y D. Estos niveles expresan jerarquía entre las categorías. Por ejemplo, la celda C14 hace referencia a la población desocupada, que hace parte de la PEA (B12) y de la población de 15 años y más (A11) | Después que la tabla de datos termina, hay un montón de notas al pie y comentarios en las celdas siguientes, que pueden ser entendidas como datos. | Las variables de las que nos interesa obtener información están como filas, cuando quisiéramos que fueran columnas. | . Queda muy claro que estos datos distan mucho de tener una estructura Tidy. . En ocasiones ante una estructura tan compleja, lo más fácil es hacer manualmente los cambios necesarios para que la tabla quede en un formato mucho más entendible para nuestro programa que va a leer los datos. Esto es válido cuando solo hay que modificar uno o pocos archivos, pero si se trata de un proceso que se tiene que aplicar para muchos archivos o se va a estar haciendo de manera recurrente tenemos que pensar en una forma de automatizar el preprocesamiento. . Afortunadamente Pandas cuenta con características que nos ayudan mucho con este tipo de archivos que tienen múltiples niveles en las filas y en las columnas. Esto coincide bastante bien con las características de Multindex y Multicolumn de los dataframes. . Empecemos importando los datos y viendo cómo lucen si los cargamos tal cual como vienen. . import pandas as pd df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;) df.head() . INEGI. Encuesta Nacional de Ocupación y Empleo. Indicadores estratégicos. Primer trimestre de 2020 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 Indicadores Estratégicos de Ocupación y Empleo... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 Entidad Federativa: | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 Aguascalientes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | INDICADOR | Enero - Marzo 2020 | NaN | NaN | Coeficientes de Variación (%) | NaN | NaN | Errores Estándar | NaN | NaN | Intervalos de Confianza al 90% | NaN | NaN | NaN | NaN | NaN | . Podemos hacer muy poco con esto, no hay nombres en las columnas y hay valores nulos por todos lados. Veamos la parte final del dataframe, donde tenemos todas las notas al pie, que realmente no es información que necesitamos para el análisis. . df.tail(7) . INEGI. Encuesta Nacional de Ocupación y Empleo. Indicadores estratégicos. Primer trimestre de 2020 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 . 296 14 | Se consideran &quot;personas con interés para traba... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 297 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 298 Las estimaciones que aparecen en este cuadro e... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 299 Nivel de precisión de las estimaciones: | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 300 Alta, CV en el rango de (0,15) | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 301 Moderada, CV en el rango de [15, 30) | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 302 Baja, CV de 30% en adelante | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Para empezar a arreglar un poco las cosas, usemos las opción header que nos permite especificar cuáles son las filas que son el encabezado de los datos. Si tuvieran un formato tidy entonces solo necesitaríamos una fila como header, pero en este caso tenemos 3 niveles para el header. Pandas nos permite especificar varios niveles si a la opción header le pasamos una lista con los número de las filas (contando desde 0). . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7]) df.head(3) . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 0_level_1 Unnamed: 1_level_1 Unnamed: 2_level_1 Unnamed: 3_level_1 Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 0 I. Población total 1 | NaN | NaN | NaN | 1363581.0 | 661998.0 | 701583.0 | 1.423850 | 1.77911 | 1.43904 | 19415.0 | 11777 | 10096 | 1.33164e+06 | 1.39552e+06 | 642624 | 681372 | 684975 | 718191 | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 2. Población de 15 años y más | NaN | NaN | NaN | 1014307.0 | 484719.0 | 529588.0 | 1.350378 | 1.6399 | 1.4333 | 13696.0 | 7948 | 7590 | 991775 | 1.03684e+06 | 471643 | 497795 | 517102 | 542074 | . El resultado que obtenemos es un dataframe con 3 niveles de columnas (Multicolumn). Ahora al menos los datos empiezan donde deberían. Hagamos también que la tabla termine donde debe terminar. Para eso usamos la opción skipfooter a la que le especificamos el número de filas que debe ignorar partiendo desde la última hacia arriba. En este caso son 16 filas con contenido que no nos interesa. . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7], skipfooter=16) df.tail(3) . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 0_level_1 Unnamed: 1_level_1 Unnamed: 2_level_1 Unnamed: 3_level_1 Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 277 NaN | Tasas calculadas contra la población ocupada n... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 278 NaN | NaN | Tasa de ocupación en el sector informal 2 (TOSI2) | NaN | 20.1963 | 22.8466 | 16.3304 | 4.116332 | 4.48344 | 6.14149 | 0.831347 | 1.02432 | 1.00293 | 18.829 | 21.564 | 21.162 | 24.532 | 14.681 | 17.98 | . 279 NaN | NaN | Tasa de informalidad laboral 2 (TIL2) | NaN | 39.3077 | 37.4318 | 42.0439 | 2.655421 | 3.20142 | 3.24678 | 1.043784 | 1.19835 | 1.36507 | 37.591 | 41.025 | 35.461 | 39.403 | 39.798 | 44.289 | . Ahora el dataframe termina donde está el último indicador, que es &quot;Tasa de informalidad laboral 2 (TIL2)&quot;. . Ahora continuaremos con la opción index_col que le permite a Pandas entender que las primeras 4 columnas serán el índice del dataframe. Lo mejor es que además entiende la estructura jerárquica que está implícita en las celdas combinadas. . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7], skipfooter=16, index_col=[0, 1, 2, 3]) df.tail() . INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 10. Tasas Tasas calculadas contra la población ocupada Tasa de ocupación en el sector informal 1 (TOSI1) Mediana 19.4007 | 21.4890 | 16.1900 | 4.109763 | 4.49951 | 6.14659 | 0.797323 | 0.9669 | 0.995131 | 18.089 | 20.712 | 19.898 | 23.08 | 14.553 | 17.827 | . Tasa de informalidad laboral 1 (TIL1) Mediana 40.9383 | 40.0765 | 42.2632 | 2.532868 | 2.97923 | 3.20628 | 1.036912 | 1.19397 | 1.35508 | 39.233 | 42.644 | 38.112 | 42.041 | 40.034 | 44.492 | . Tasas calculadas contra la población ocupada no agropecuaria Tasa de informalidad laboral 1 (TIL1) Mediana NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Tasa de ocupación en el sector informal 2 (TOSI2) Mediana 20.1963 | 22.8466 | 16.3304 | 4.116332 | 4.48344 | 6.14149 | 0.831347 | 1.02432 | 1.00293 | 18.829 | 21.564 | 21.162 | 24.532 | 14.681 | 17.98 | . Tasa de informalidad laboral 2 (TIL2) Mediana 39.3077 | 37.4318 | 42.0439 | 2.655421 | 3.20142 | 3.24678 | 1.043784 | 1.19835 | 1.36507 | 37.591 | 41.025 | 35.461 | 39.403 | 39.798 | 44.289 | . En este caso nos interesa obtener las estimaciones de los indicadores y no los coeficientes de variación y los otros cálculos. Estos datos están bajo la columna &quot;Enero - Marzo 2020&quot;, así que ya que tenemos encabezados es fácil obtenerlos. . df[&#39;Enero - Marzo 2020&#39;].head() . Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . NaN NaN | NaN | NaN | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Queda sobrando un nivel que en realidad no necesitamos porque no agrega nada de información [&#39;Unnamed: 4_level_1&#39;, &#39;Unnamed: 5_level_1&#39;, &#39;Unnamed: 6_level_1&#39;]. Este lo podemos eliminar con el método .droplevel() . valores = df[&#39;Enero - Marzo 2020&#39;].droplevel(level=0, axis=1) valores.head() . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . NaN NaN | NaN | NaN | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Ya tenemos un resultado bastante útil. Todavía nos quedan algunos ajustes que hacer. Primero, hay que eliminar las filas que solo contienen valores nulos . valores = valores.dropna(subset=[&#39;Total&#39;, &#39;Hombres&#39;, &#39;Mujeres&#39;]) valores.head() . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Desocupada NaN 20040.0 | 12980.0 | 7060.0 | . Ahora pongamos nombres a los niveles para que sea fácil identificarlos. También cambiemos los NaN que hay en el índice por un valor de texto, como por ejemplo &quot;Total&quot;. Esto ayuda porque pandas no maneja muy bien valores nulos en el índice. . valores = valores.rename_axis(index=[&#39;nivel_1&#39;, &#39;nivel_2&#39;, &#39;nivel_3&#39;, &#39;nivel_4&#39;], columns=[&#39;sexo&#39;]) .rename(lambda x: &#39;Total&#39; if pd.isna(x) else x) valores.head() . sexo Total Hombres Mujeres . nivel_1 nivel_2 nivel_3 nivel_4 . I. Población total 1 Total Total Total 1363581.0 | 661998.0 | 701583.0 | . 2. Población de 15 años y más Total Total Total 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) Total Total 603802.0 | 366686.0 | 237116.0 | . Ocupada Total 583762.0 | 353706.0 | 230056.0 | . Desocupada Total 20040.0 | 12980.0 | 7060.0 | . Y bueno, ya con esto prácticamente podemos obtener el valor de cualquiera de los indicadores. Por ejemplo, si queremos la &quot;Población económicamente activa (PEA)&quot; . pea = valores.loc[(&#39;2. Población de 15 años y más&#39;, &#39;Población económicamente activa (PEA)&#39;)] pea . C: ProgramData Anaconda3 lib site-packages ipykernel_launcher.py:1: PerformanceWarning: indexing past lexsort depth may impact performance. &#34;&#34;&#34;Entry point for launching an IPython kernel. . sexo Total Hombres Mujeres . nivel_3 nivel_4 . Total Total 603802.0 | 366686.0 | 237116.0 | . Ocupada Total 583762.0 | 353706.0 | 230056.0 | . Desocupada Total 20040.0 | 12980.0 | 7060.0 | . En este resultado el nivel_4 es innecesario, así que lo podemos eliminar. En general, podemos eliminar cualquier nivel que no aporte información para quedarnos con una estructura más sencilla. Además modificamos la estructura para que sea tidy y cada columna sea una variable . pea.droplevel(1) .T .add_prefix(&#39;poblacion_&#39;) .reset_index() . nivel_3 sexo poblacion_Total poblacion_Ocupada poblacion_Desocupada . 0 Total | 603802.0 | 583762.0 | 20040.0 | . 1 Hombres | 366686.0 | 353706.0 | 12980.0 | . 2 Mujeres | 237116.0 | 230056.0 | 7060.0 | . Podemos intentar con otro indicador como la Tasa de informalidad laboral 1 (TIL1), haciendo algunas otras modificaciones: . valores.loc[(&#39;10. Tasas&#39;, &#39;Tasas calculadas contra la población ocupada&#39;, &#39;Tasa de informalidad laboral 1 (TIL1)&#39;)] .unstack(&#39;sexo&#39;) .to_frame(&#39;til_1&#39;) .droplevel(&#39;nivel_4&#39;) .reset_index() . C: ProgramData Anaconda3 lib site-packages ipykernel_launcher.py:1: PerformanceWarning: indexing past lexsort depth may impact performance. &#34;&#34;&#34;Entry point for launching an IPython kernel. . sexo til_1 . 0 Total | 40.9383 | . 1 Hombres | 40.0765 | . 2 Mujeres | 42.2632 | . Podemos convertir este procedimiento en una función para que podamos obtener estas variables para cualquier estado. Por ejemplo, para obtener la población económicamente activa creamos esta función que depende solo del nombre del estado (como aparece en el archivo de INEGI que descargué) . def obtiene_pea(edo: str): df = pd.read_excel(f&#39;datos/2020_trim_1_Entidad_{edo}.xls&#39;, header=[5, 6, 7], skipfooter=16, index_col=[0, 1, 2, 3]) pea = df[&#39;Enero - Marzo 2020&#39;].droplevel(level=0, axis=1) .dropna(subset=[&#39;Total&#39;, &#39;Hombres&#39;, &#39;Mujeres&#39;]) .rename_axis(index=[&#39;nivel_1&#39;, &#39;nivel_2&#39;, &#39;nivel_3&#39;, &#39;nivel_4&#39;], columns=[&#39;sexo&#39;]) .rename(lambda x: &#39;Total&#39; if pd.isna(x) else x) .sort_index() .loc[(&#39;2. Población de 15 años y más&#39;, &#39;Población económicamente activa (PEA)&#39;)] .droplevel(1) .T .add_prefix(&#39;poblacion_&#39;) .reset_index() .assign(estado=edo.replace(&#39;_&#39;, &#39; &#39;)) return pea . Probamos la función en otro estado y nos da el resultado esperado: . obtiene_pea(&#39;Oaxaca&#39;) . nivel_3 sexo poblacion_Desocupada poblacion_Ocupada poblacion_Total estado . 0 Total | 30743.0 | 1766690.0 | 1797433.0 | Oaxaca | . 1 Hombres | 18948.0 | 999821.0 | 1018769.0 | Oaxaca | . 2 Mujeres | 11795.0 | 766869.0 | 778664.0 | Oaxaca | . Acá por ejemplo, obtenemos la pea para los estados del Sur-Sureste mexicano: . pea_sur = pd.concat([obtiene_pea(e) for e in [&#39;Oaxaca&#39;, &#39;Chiapas&#39;, &#39;Tabasco&#39;, &#39;Campeche&#39;, &#39;Quintana_Roo&#39;, &#39;Yucatán&#39;]], ignore_index=True) pea_sur . nivel_3 sexo poblacion_Desocupada poblacion_Ocupada poblacion_Total estado . 0 Total | 30743.0 | 1766690.0 | 1797433.0 | Oaxaca | . 1 Hombres | 18948.0 | 999821.0 | 1018769.0 | Oaxaca | . 2 Mujeres | 11795.0 | 766869.0 | 778664.0 | Oaxaca | . 3 Total | 55562.0 | 2068483.0 | 2124045.0 | Chiapas | . 4 Hombres | 32413.0 | 1417037.0 | 1449450.0 | Chiapas | . 5 Mujeres | 23149.0 | 651446.0 | 674595.0 | Chiapas | . 6 Total | 57702.0 | 1031968.0 | 1089670.0 | Tabasco | . 7 Hombres | 31558.0 | 643777.0 | 675335.0 | Tabasco | . 8 Mujeres | 26144.0 | 388191.0 | 414335.0 | Tabasco | . 9 Total | 12364.0 | 435961.0 | 448325.0 | Campeche | . 10 Hombres | 7750.0 | 269822.0 | 277572.0 | Campeche | . 11 Mujeres | 4614.0 | 166139.0 | 170753.0 | Campeche | . 12 Total | 25607.0 | 851473.0 | 877080.0 | Quintana_Roo | . 13 Hombres | 14241.0 | 525859.0 | 540100.0 | Quintana_Roo | . 14 Mujeres | 11366.0 | 325614.0 | 336980.0 | Quintana_Roo | . 15 Total | 21992.0 | 1086089.0 | 1108081.0 | Yucatán | . 16 Hombres | 11421.0 | 651820.0 | 663241.0 | Yucatán | . 17 Mujeres | 10571.0 | 434269.0 | 444840.0 | Yucatán | . De esta manera logré obtener los indicadores que necesitaba de un archivo que parecía imposible de aprovechar en su estado original. . Lo que más quería destacar en esta entrada es que Pandas, con su estructura de multindex, puede facilitar mucho leer archivos de Excel cuya estructura incluye celdas combinadas y anidadas. No hay garantía de que siempra se pueda leer adecuadamente archivos de Excel muy complejos, pero es bueno saber que tampoco está todo perdido si llega a tus manos uno de estos mosntruos. .",
            "url": "http://blog.jjsantoso.com/pandas-excel/",
            "relUrl": "/pandas-excel/",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Redes y contratos públicos",
            "content": "Imports y globals . Imports . import pymongo import pandas as pd import json import matplotlib.pyplot as plt import networkx as nx from NetworkUtils import draw_network . globals . dir_datos = &#39;d:/datos/licitaciones_compranet&#39; myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;) mydb = myclient[&#39;dataton2019&#39;] . Funciones . An&#225;lisis de asociados . Contacpoints . Creaci&#243;n de la base . Creamos una base de datos que tenga identificado a cada punto de contacto. | Los puntos de contacto se obtienen a partir de los contratistas que tienen información o su dirección. | . resultado = mydb.contrataciones.find({}, {&#39;_id&#39;: 0, &#39;parties.contactPoint&#39;: 1, &#39;parties.roles&#39;: 1, &#39;parties.id&#39;: 1, &#39;parties.address&#39;: 1}) . df_contactos = pd.DataFrame([{**p.get(&#39;contactPoint&#39;, &#39;&#39;), **p.get(&#39;address&#39;, &#39;&#39;), &#39;tenderer_id&#39;: p[&#39;id&#39;] } for x in resultado for p in x[&#39;parties&#39;] if (p[&#39;roles&#39;] in [[&#39;tenderer&#39;], [&#39;tenderer&#39;, &#39;supplier&#39;]]) &amp; ((bool(p.get(&#39;address&#39;, None))) | (bool(p.get(&#39;contactPoint&#39;, None))))]) df_contactos.to_pickle(f&#39;{dir_datos}/tenderers_contacpoint.pkl&#39;) df_contactos.head() . name email telephone streetAddress locality region postalCode countryName tenderer_id faxNumber . 0 IRAM LIEVANOS VELAZQUEZ | laroca.canino@gmail.com | 52 722 093749 | PASEO DE LA ASUNCION NO. 536 | METEPEC | MX-MEX | 52148 | MÉXICO | E9C1C827AE1234CCF7AC4D9070BB597C | NaN | . 1 NaN | servillantas@prodigy.net.mx | 10191684 | JOSE MORAN 66 | MIGUEL HIDALGO | MX-CMX | 11850 | MÉXICO | SCA031118BX7 | NaN | . 2 NaN | sportingautoreparaciones@gmail.com | 55 56397681 55 54894622 | PLUTARCO ELIAS CALLES No. 660. COL. SAN FRANCI... | Iztacalco | MX-CMX | 08230 | MÉXICO | SAU0505307M9 | NaN | . 3 EDGAR GUSTAVO TREJO KEMPER | edgarg_kemper@hotmail.com; balcazar-sol@hotmai... | 5543419836 | CALZADA VALLEJO NUMERO 1020 | AZCAPOTZALCO | MX-CMX | 02300 | MÉXICO | SCK070618C21 | 52 55 55873415 ext 201, 202, 203 | . 4 Daniel Ernesto De la Fuente Barra | daniel.delafuente@segurossura.com.mx | 5519636830 | BLVD ADOLFO LOPEZ MATEOS 2448 | ALVARO OBREGON | MX-CMX | 01060 | MÉXICO | R&amp;S811221KR6 | 57237999 Ext. 7965 | . De la base de datos de contrataciones seleccionamos los datos de contacto de los que han ganado licitaciones. Estos tiene datos como nombre de la persona de contacto, email, teléfono, número de fax, dirección y id del proveedor | . Uso de la base . df_contactos = pd.read_pickle(f&#39;{dir_datos}/tenderers_contacpoint.pkl&#39;) df_contactos.head() . name email telephone streetAddress locality region postalCode countryName tenderer_id faxNumber . 0 IRAM LIEVANOS VELAZQUEZ | laroca.canino@gmail.com | 52 722 093749 | PASEO DE LA ASUNCION NO. 536 | METEPEC | MX-MEX | 52148 | MÉXICO | E9C1C827AE1234CCF7AC4D9070BB597C | NaN | . 1 NaN | servillantas@prodigy.net.mx | 10191684 | JOSE MORAN 66 | MIGUEL HIDALGO | MX-CMX | 11850 | MÉXICO | SCA031118BX7 | NaN | . 2 NaN | sportingautoreparaciones@gmail.com | 55 56397681 55 54894622 | PLUTARCO ELIAS CALLES No. 660. COL. SAN FRANCI... | Iztacalco | MX-CMX | 08230 | MÉXICO | SAU0505307M9 | NaN | . 3 EDGAR GUSTAVO TREJO KEMPER | edgarg_kemper@hotmail.com; balcazar-sol@hotmai... | 5543419836 | CALZADA VALLEJO NUMERO 1020 | AZCAPOTZALCO | MX-CMX | 02300 | MÉXICO | SCK070618C21 | 52 55 55873415 ext 201, 202, 203 | . 4 Daniel Ernesto De la Fuente Barra | daniel.delafuente@segurossura.com.mx | 5519636830 | BLVD ADOLFO LOPEZ MATEOS 2448 | ALVARO OBREGON | MX-CMX | 01060 | MÉXICO | R&amp;S811221KR6 | 57237999 Ext. 7965 | . df_contactos.shape . (563693, 10) . De todos los contratos encontramos 563693 puntos de contacto. Muchos de estos se repiten porque un contratista que ganó varias veces aparecerá como un contacto por cada contrato ganado. | Calculamos todas las combinaciones únicas de telefono y tenderer_id. | Luego verificamos si existen casos en los que varios tenderer_id comparten el: teléfono: 700 casos en los que eso ocurre. | Email: 839 casos. | Nombre: 706 casos | Número de fax: 135 casos | Dirección de la calle: 172 casos | . | Todos estos son signos de sospecha. | En muchos casos hay cuentas de funcionarios públicos. Habría que verificar cuál es su papel. | La pregunta relevante es ¿Hay casos en los que contratistas que tienen contactos en común hayan participado en un mismo proceso de licitación? | . variables_contacto = [&#39;telephone&#39;, &#39;email&#39;, &#39;name&#39;, &#39;streetAddress&#39;, &#39;faxNumber&#39;] casos = [] for var_duplicated in variables_contacto: # Encontramos todos los valores únicos de la variable de contacto y de tenderer_id dups_direccion = df_contactos.loc[lambda x: (~x.duplicated(subset=[&#39;tenderer_id&#39;, var_duplicated])) &amp; (x[var_duplicated].notnull())] .loc[lambda x: (x[var_duplicated].duplicated()) &amp; (~x[&#39;name&#39;].str[:22].eq(&#39;- (Cuenta administrada&#39;)), var_duplicated].unique() # Encontramos cuáles son los tenderers_id que comparten un mismo contacto tenderers_dup_id = [df_contactos.loc[lambda x: x[var_duplicated].eq(dup)].drop_duplicates(subset=[&#39;tenderer_id&#39;])[&#39;tenderer_id&#39;].tolist() for dup in dups_direccion] # Buscamos los contratos en los que participaron los ids asociados queries_dup = [[{&#39;parties.id&#39;: i} for i in x] for x in tenderers_dup_id] for q in queries_dup: resultado = list(mydb.contrataciones.find({&#39;$and&#39;: q}, {&#39;_id&#39;: 0, &#39;ocid&#39;: 1})) if resultado: tenderers_id = [x[&#39;parties.id&#39;] for x in q] ocids = list({x[&#39;ocid&#39;] for x in resultado}) casos.append({&#39;tenderer_ids&#39;: tenderers_id, &#39;contratos_ocid&#39;: ocids, &#39;variable&#39;: var_duplicated}) print(q) with open(&#39;datos/casos_colusion.json&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) as jsonfile: json.dump(casos, jsonfile) . El resultado que encontramos es que existen 571 casos de contratistas posiblemente relacionados en una misma licitación. | . with open(&#39;datos/casos_colusion.json&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;) as jsonfile: casos = json.load(jsonfile) . casos_ocid = list({c for cas in casos for c in cas[&#39;contratos_ocid&#39;]}) len(casos_ocid) . 571 . casos_contratos = list(mydb.contrataciones.find({&#39;ocid&#39;: {&#39;$in&#39;: casos_ocid}})) len(casos_contratos) . 619 . datos_contrato = [{&#39;titulo&#39;: c[&#39;contracts&#39;][0][&#39;title&#39;], &#39;descr&#39;: c[&#39;contracts&#39;][0].get(&#39;description&#39;, &#39;&#39;), &#39;valor&#39;: c[&#39;contracts&#39;][0][&#39;value&#39;][&#39;amount&#39;], &#39;dependencia_id&#39;: c[&#39;buyer&#39;][&#39;id&#39;], &#39;dependencia_nombre&#39;: c[&#39;buyer&#39;][&#39;name&#39;], &#39;uc_id&#39;: c[&#39;tender&#39;][&#39;procuringEntity&#39;][&#39;id&#39;], &#39;uc_name&#39;: c[&#39;tender&#39;][&#39;procuringEntity&#39;][&#39;name&#39;], &#39;ocid&#39;: c[&#39;ocid&#39;], &#39;fecha&#39;: c[&#39;date&#39;], } for c in casos_contratos if c.get(&#39;contracts&#39;, None)] df_datos_contratos = pd.DataFrame(datos_contrato).set_index(&#39;ocid&#39;) df_datos_contratos.head() . titulo descr valor dependencia_id dependencia_nombre uc_id uc_name fecha . ocid . ocds-07smqs-1317308 Servicio Integral de Suministro, Mantenimiento... | Servicio Integral de Suministro, Mantenimiento... | 450000.00 | CNBV-80 | Comisión Nacional Bancaria y de Valores | CNB950501PT6-006B00001 | CNBV-Dirección General Adjunta de Adquisicione... | 2017-03-29T05:13:19Z | . ocds-07smqs-1367848 PRESTADOR DE SERVICIOS INTEGRALES | PRESTADOR DE SERVICIOS INTEGRALES (HONORARIOS) | 44542.62 | SAGARPA-261 | Secretaría de Agricultura, Ganadería, Desarrol... | SAG010710V98-008000995 | SAGARPA-Delegacion Chihuahua #008000995 | 2017-05-22T12:24:14Z | . ocds-07smqs-1430619 CONTRATACIÓN ABIERTA DEL SERVICIO DE LECTURA E... | CONTRATACIÓN ABIERTA DEL SERVICIO DE LECTURA E... | 220000.00 | SEP-265 | Secretaría de Educación Pública | SEP210905778-011000999 | SEP-Dirección de Adquisiciones #011000999 | 2017-07-20T06:22:43Z | . ocds-07smqs-1444924 SERVICIO DE MANTENIMIENTO CORRECTIVO AL SISTEM... | SERVICIO DE MANTENIMIENTO CORRECTIVO AL SISTEM... | 287780.00 | CONAGUA-94 | Comisión Nacional del Agua | CNA890116SF2-016B00009 | CONAGUA-Gerencia de Resursos Materiales #016B0... | 2017-08-03T01:58:53Z | . ocds-07smqs-1452158 SERVICIO DE DIFUSIÓN EN MEDIOS DIGITALES DE LA... | SERVICIO DE DIFUSIÓN EN MEDIOS DIGITALES DE LA... | 68950.00 | CONUEE-98 | Comisión Nacional para el Uso Eficiente de la ... | CNU800928K31-018E00999 | CONUEE-Dirección de Recursos Materiales y Serv... | 2017-08-30T04:21:41Z | . casos_contratos[0] . {&#39;_id&#39;: ObjectId(&#39;5dcdb0c10d84ead5c49d2e99&#39;), &#39;publisher&#39;: {&#39;uid&#39;: &#39;27511&#39;, &#39;name&#39;: &#39;SECRETARÍA DE LA FUNCIÓN PÚBLICA&#39;, &#39;uri&#39;: &#39;http://www.gob.mx/sfp&#39;}, &#39;cycle&#39;: 2017, &#39;ocid&#39;: &#39;ocds-07smqs-1317308&#39;, &#39;id&#39;: &#39;SFP-1317308-2018-11-12&#39;, &#39;date&#39;: &#39;2017-03-29T05:13:19Z&#39;, &#39;tag&#39;: [&#39;tender&#39;, &#39;award&#39;], &#39;initiationType&#39;: &#39;tender&#39;, &#39;parties&#39;: [{&#39;name&#39;: &#39;Comisión Nacional Bancaria y de Valores&#39;, &#39;id&#39;: &#39;CNBV-80&#39;, &#39;roles&#39;: [&#39;buyer&#39;]}, {&#39;name&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;, &#39;legalName&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;Av. Insurgentes Sur No. 1971, Torre Sur, Piso 6, Col. Guadalupe Inn&#39;, &#39;locality&#39;: &#39;Álvaro Obregón&#39;, &#39;region&#39;: &#39;Ciudad de México&#39;, &#39;postalCode&#39;: &#39;01020&#39;, &#39;countryName&#39;: &#39;MX&#39;}, &#39;contactPoint&#39;: {&#39;name&#39;: &#39;Jannet Miriam Martínez Sánchez&#39;, &#39;email&#39;: &#39;jmartinezs@cnbv.gob.mx&#39;, &#39;telephone&#39;: &#39;1454-6537 y 1454-6538&#39;}, &#39;roles&#39;: [&#39;procuringEntity&#39;]}, {&#39;name&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;, &#39;legalName&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {}, &#39;contactPoint&#39;: {}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;id&#39;: &#39;GSA0310175N4&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;GSA0310175N4&#39;, &#39;legalName&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;AZTECAS 81 LA ROMANA&#39;, &#39;locality&#39;: &#39;Tlalnepantla de Baz&#39;, &#39;region&#39;: &#39;MX-MEX&#39;, &#39;postalCode&#39;: &#39;54050&#39;, &#39;countryName&#39;: &#39;MÉXICO&#39;}, &#39;contactPoint&#39;: {&#39;email&#39;: &#39;rafael@sanmari.com.mx&#39;, &#39;telephone&#39;: &#39;55-52409421&#39;}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;, &#39;legalName&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {}, &#39;contactPoint&#39;: {}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;, &#39;legalName&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;CEIBAS 45&#39;, &#39;locality&#39;: &#39;NAUCALPAN DE JUAREZ&#39;, &#39;region&#39;: &#39;MX-MEX&#39;, &#39;postalCode&#39;: &#39;53240&#39;, &#39;countryName&#39;: &#39;MÉXICO&#39;}, &#39;contactPoint&#39;: {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;email&#39;: &#39;mascontrolmenoscosto@yahoo.com.mx&#39;, &#39;telephone&#39;: &#39;525536259819&#39;}, &#39;roles&#39;: [&#39;tenderer&#39;, &#39;supplier&#39;]}], &#39;buyer&#39;: {&#39;name&#39;: &#39;Comisión Nacional Bancaria y de Valores&#39;, &#39;id&#39;: &#39;CNBV-80&#39;}, &#39;tender&#39;: {&#39;id&#39;: &#39;1317308&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;complete&#39;, &#39;procuringEntity&#39;: {&#39;name&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;}, &#39;items&#39;: [{&#39;id&#39;: &#39;7044016&#39;, &#39;description&#39;: &#39;Contratación de una Póliza de Seguro de Accidentes Personales para la protección de los participantes en acciones de capacitación del Programa de Apoyo al Empleo 2016.&#39;, &#39;classification&#39;: {&#39;id&#39;: &#39;33900006&#39;, &#39;description&#39;: &#39;Servicios de seguros de gastos medicos mayores&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;}}], &#39;value&#39;: {&#39;amount&#39;: 0}, &#39;procurementMethod&#39;: &#39;direct&#39;, &#39;procurementMethodRationale&#39;: &#39;Art. 42 párrafo primero&#39;, &#39;submissionMethod&#39;: [&#39;electronicSubmission&#39;], &#39;tenderPeriod&#39;: {&#39;startDate&#39;: &#39;2017-03-29T05:13:19Z&#39;}, &#39;enquiryPeriod&#39;: {&#39;startDate&#39;: &#39;2017-03-29T05:13:19Z&#39;}, &#39;hasEnquiries&#39;: False, &#39;awardPeriod&#39;: {&#39;endDate&#39;: &#39;2017-03-30T00:00:00Z&#39;}, &#39;numberOfTenderers&#39;: 4, &#39;tenderers&#39;: [{&#39;name&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;}, {&#39;name&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;id&#39;: &#39;GSA0310175N4&#39;}, {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;}, {&#39;name&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;}]}, &#39;language&#39;: &#39;es&#39;, &#39;awards&#39;: [{&#39;id&#39;: &#39;1399908&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;active&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}, &#39;suppliers&#39;: [{&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;}], &#39;items&#39;: [{&#39;id&#39;: &#39;4645830&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;classification&#39;: {&#39;scheme&#39;: &#39;CUCOP: Clasificador Único de las Contrataciones Públicas&#39;, &#39;id&#39;: &#39;35900004&#39;, &#39;description&#39;: &#39;Servicios de jardineria&#39;, &#39;uri&#39;: &#39;https://compranetinfo.funcionpublica.gob.mx/descargas/CUCOP.xlsx&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}}}], &#39;contractPeriod&#39;: {&#39;startDate&#39;: &#39;2017-04-14T09:00:00Z&#39;, &#39;endDate&#39;: &#39;2018-06-18T03:59:00Z&#39;}}], &#39;contracts&#39;: [{&#39;id&#39;: 1399908, &#39;awardID&#39;: &#39;1399908&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;terminated&#39;, &#39;period&#39;: {&#39;startDate&#39;: &#39;2017-04-14T09:00:00Z&#39;, &#39;endDate&#39;: &#39;2018-06-18T03:59:00Z&#39;}, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}, &#39;items&#39;: [{&#39;id&#39;: &#39;4645830&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;classification&#39;: {&#39;id&#39;: &#39;35900004&#39;, &#39;description&#39;: &#39;Servicios de jardineria&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}}}]}]} . participantes_contrato = {c[&#39;ocid&#39;]: [p[&#39;id&#39;] for p in c[&#39;parties&#39;] if p[&#39;roles&#39;] in [[&#39;tenderer&#39;], [&#39;tenderer&#39;, &#39;supplier&#39;]]] for c in casos_contratos} # Número de particpantes que estaban asociados en cada contrato asociados_contrato = {o: c[&#39;tenderer_ids&#39;] for c in casos for o in c[&#39;contratos_ocid&#39;]} # ganador contrato ganadores_contrato = {c[&#39;ocid&#39;]: [p[&#39;id&#39;] for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;tenderer&#39;, &#39;supplier&#39;]] for c in casos_contratos} # dataframe df_asoc = pd.DataFrame([participantes_contrato, asociados_contrato, ganadores_contrato]).T .rename(columns={0: &#39;part&#39;, 1: &#39;asoc&#39;, 2: &#39;gana&#39;}) .assign(N_part=lambda x: x[&#39;part&#39;].str.len(), N_asoc=lambda x: x[&#39;asoc&#39;].str.len(), N_gana=lambda x: x[&#39;gana&#39;].str.len(), prop_asoc_part=lambda x: x[&#39;N_asoc&#39;].div(x[&#39;N_part&#39;]), part_mayo=lambda x: x[&#39;prop_asoc_part&#39;].ge(0.5), asoc_ganadores=lambda x: x.apply(lambda y: list(set(y[&#39;gana&#39;]).intersection(set(y[&#39;asoc&#39;]))), axis=1), N_asoc_ganadores=lambda x: x[&#39;asoc_ganadores&#39;].str.len(), part_nogana=lambda x: x.apply(lambda y: list(set(y[&#39;part&#39;]).difference(set(y[&#39;gana&#39;]))), axis=1)) .join(df_datos_contratos) df_asoc.to_pickle(f&#39;{dir_datos}/df_asociados.pkl&#39;) df_asoc.head() . part asoc gana N_part N_asoc N_gana prop_asoc_part part_mayo asoc_ganadores N_asoc_ganadores part_nogana titulo descr valor dependencia_id dependencia_nombre uc_id uc_name fecha . ocds-07smqs-1043398 [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | 2 | 2 | 2 | 1.000000 | True | [BD03FBE666C3DBA5C57BCDC8BF0AA451, TME840315KT6] | 2 | [] | SERVICIO MPLS ATRAVES DE UN ENLACE DEDICADO CO... | | 169133.00 | CIJ-66 | Centros de Integración Juvenil, A.C. | CIJ731003QK3-012M7K001 | CIJ-Departamento de Adquisiciones #012M7K001 | 2016-04-18T12:02:38Z | . ocds-07smqs-1193763 [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D... | [D73016CAA1F8020E3BAC52068FB0B2D9, MLA840208FN5] | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D... | 4 | 2 | 4 | 0.500000 | True | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D9] | 2 | [] | ADQ. DE VIVERES PARA EJERCICIO 2017 | ADQ. DE VIVERES PARA EJERCICIO 2017 | 3880739.50 | IMSS-192 | Instituto Mexicano del Seguro Social | IMS421231I45-050GYR045 | IMSS-UMAE Hospital de Especilidades No.71 Dept... | 2016-12-09T05:26:28Z | . ocds-07smqs-1224403 [89E87098891F04A46318B7F775AD5E48, FAR100921AL... | [D73016CAA1F8020E3BAC52068FB0B2D9, MLA840208FN5] | [D73016CAA1F8020E3BAC52068FB0B2D9, 9D3346ADF0B... | 8 | 2 | 5 | 0.250000 | False | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D9] | 2 | [FAR100921ALA, 89E87098891F04A46318B7F775AD5E4... | AA-019GYR026-E221-2016 DESIERTAS VIVERES | | 671930.69 | IMSS-192 | Instituto Mexicano del Seguro Social | IMS421231I45-050GYR026 | IMSS-Coordinación de abastecimiento y equipami... | 2016-12-02T05:49:52Z | . ocds-07smqs-1240190 [RDO070228V11, ATD061228L34, MEX0301141G6] | [RDO070228V11, ATD061228L34] | [MEX0301141G6] | 3 | 2 | 1 | 0.666667 | True | [] | 0 | [RDO070228V11, ATD061228L34] | SERVICIO DE RESGUARDO, CUSTODIA, TRASLADO, ENV... | SERVICIO DE RESGUARDO, CUSTODIA, TRASLADO, ENV... | 116379.72 | CPTM-109 | Consejo de Promoción Turística de México, S.A.... | CPT991022DE7-021W3J001 | CPTM-Gerencia de Adquisiciones y Licitaciones ... | 2016-12-20T06:52:52Z | . ocds-07smqs-1241959 [CGE130930JV2, CDA9601297G9, 23DF515587ED8B3F4... | [CDA9601297G9, 23DF515587ED8B3F4A8B1C9E4D725CAD] | [] | 3 | 2 | 0 | 0.666667 | True | [] | 0 | [CGE130930JV2, 23DF515587ED8B3F4A8B1C9E4D725CA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . df_asoc = pd.read_pickle(f&#39;{dir_datos}/df_asociados.pkl&#39;) . ¿En cuántos de estos casos los contratistas representaban el 50% de los proponentes o más? | . print(&#39;Los contratistas representaban el 50% de los proponentes o más en&#39;, df_asoc.part_mayo.sum(), &#39;licitaciones&#39;) . Los contratistas representaban el 50% de los proponentes o más en 212 licitaciones . ¿En cuántos de estos casos los asociados fueron los únicos proponentes? | . print(&#39;¿En cuántos de estos casos los asociados fueron los únicos proponentes?&#39;, df_asoc.prop_asoc_part.eq(1).sum(), &#39;licitaciones&#39;) . ¿En cuántos de estos casos los asociados fueron los únicos proponentes? 41 licitaciones . De estos casos ¿en cuántas licitaciones los que estaban relacionados ganaron un concurso? | . print(&#39;En&#39;, df_asoc.N_asoc_ganadores.gt(0).sum(), &#39;licitaciones ganó al menos uno de los contratistas asociados&#39;) . En 370 licitaciones ganó al menos uno de los contratistas asociados . ¿Cuántos contratistas asociados recibieron un contrato? | . print(df_asoc.N_asoc_ganadores.sum(), &#39; contratistas asociados ganaron una licitación&#39;) . 552 contratistas asociados ganaron una licitación . ¿En qué dependencias, unidades compradoras y servidores públicos ocurre más esto? | . df_asoc.groupby([&#39;uc_name&#39;])[&#39;part&#39;].count().sort_values(ascending=False) . uc_name CONALITEG-Dirección de Recursos Materiales y Servicios Generales #011L6J001 33 IMSS-Departamento de Adquisición de Bienes y Contratación de Servicios #050GYR033 16 IMSS-Coordinación de Abastecimiento y Equipamiento #050GYR009 10 IMSS-Coordinación de Adquisición de Bienes y Contratación de Serv, Dirección de Administración #050GYR047 9 INER-Departamento de Adquisiciones #012NCD001 9 .. IMSS-UMAE HOSPITAL DE GINECO OBSTETRICIA No 03 DR VICTOR MANUEL ESPINOSA DE LOS REYES SANCHEZ CMN LA RAZA #050GYR050 1 IMSS-Coord. de Abastecimiento y Equipamiento Deleg. Ver. Sur #050GYR022 1 SCT-Centro SCT Chihuahua #009000980 1 SCT-CENTRO SCT EN CHIAPAS SUBDIRECCION DE ADMINISTRACION #009000992 1 Tribunales Agrarios-Dirección General de Recursos Materiales #031000001 1 Name: part, Length: 218, dtype: int64 . df_asoc.groupby([&#39;dependencia_nombre&#39;])[&#39;part&#39;].count().sort_values(ascending=False) . dependencia_nombre Instituto Mexicano del Seguro Social 215 Comisión Nacional de Libros de Texto Gratuitos 33 Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado 32 Comisión Federal de Electricidad 24 Comisión Nacional del Agua 15 ... Hospital Regional de Alta Especialidad de Oaxaca 1 Exportadora de Sal, S.A. de C.V. 1 El Colegio de la Frontera Sur 1 Corporación Mexicana de Investigación en Materiales, S.A. de C.V. 1 Administración Portuaria Integral de Progreso, S.A. de C.V. 1 Name: part, Length: 92, dtype: int64 . Crea red para visualizar . nodos = [] red = 1 for c, vals in df_asoc.iterrows(): nodos.append({&#39;id&#39;: c, &#39;tipo&#39;: &#39;contrato&#39;}) for p in vals[&#39;part_nogana&#39;]: nodos.append({&#39;id&#39;: p, &#39;tipo&#39;: &#39;tenderer&#39;}) links.append({&#39;origen_id&#39;: p, &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;participa&#39;, &#39;red&#39;: red}) for p in vals[&#39;gana&#39;]: nodos.append({&#39;id&#39;: p, &#39;tipo&#39;: &#39;supplier&#39;}) links.append({&#39;origen_id&#39;: p, &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;gana&#39;, &#39;red&#39;: red}) for p1 in vals[&#39;asoc&#39;]: for p2 in vals[&#39;asoc&#39;]: if p1!=p2: links.append({&#39;origen_id&#39;: p1, &#39;destino_id&#39;: p2, &#39;accion&#39;: &#39;asociado&#39;, &#39;red&#39;: red}) nodos.append({&#39;id&#39;: vals[&#39;uc_id&#39;], &#39;tipo&#39;: &#39;uc&#39;}) links.append({&#39;origen_id&#39;: vals[&#39;uc_id&#39;], &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;compra&#39;, &#39;red&#39;: red}) red+=1 . df_nodos = pd.DataFrame(nodos) .assign(num=lambda x:x.index) dicc_nodo_num = {v:k for k,v in df_nodos[&#39;id&#39;].to_dict().items()} df_links = pd.DataFrame(links) .assign(origen_num=lambda x: x[&#39;origen_id&#39;].map(dicc_nodo_num), destino_num=lambda x: x[&#39;destino_id&#39;].map(dicc_nodo_num)) .dropna() df_links.head() . origen_id destino_id accion red origen_num destino_num . 25696 TME840315KT6 | ocds-07smqs-1043398 | gana | 1.0 | 287 | 0 | . 25697 BD03FBE666C3DBA5C57BCDC8BF0AA451 | ocds-07smqs-1043398 | gana | 1.0 | 286 | 0 | . 25698 TME840315KT6 | BD03FBE666C3DBA5C57BCDC8BF0AA451 | asociado | 1.0 | 287 | 286 | . 25699 BD03FBE666C3DBA5C57BCDC8BF0AA451 | TME840315KT6 | asociado | 1.0 | 286 | 287 | . 25700 CIJ731003QK3-012M7K001 | ocds-07smqs-1043398 | compra | 1.0 | 284 | 0 | . df_nodos.to_csv(&#39;datos/asociados_nodos.csv&#39;, index=False) df_links.to_csv(&#39;datos/asociados_links.csv&#39;, index=False) . Visualziacion networkX . df_nodos_graph = df_nodos.set_index(&#39;id&#39;) . for red in df_links.red.unique(): G = nx.from_pandas_edgelist(df_links.query(&#39;red==@red&#39;), source=&#39;origen_id&#39;, target=&#39;destino_id&#39;, edge_attr=[&#39;accion&#39;]) dicc_color_edges = {&#39;gana&#39;: &#39;green&#39;, &#39;asociado&#39;: &#39;red&#39;, &#39;participa&#39;: &#39;#3292a8&#39;, &#39;compra&#39;: &#39;orange&#39;} dicc_color_nodos = {&#39;contrato&#39;: &#39;#3292a8&#39;, &#39;supplier&#39;: &#39;pink&#39;, &#39;tenderer&#39;: &#39;blue&#39;, &#39;uc&#39;: &#39;orange&#39;} color_edges = [dicc_color_edges[e[2][&#39;accion&#39;]] for e in G.edges(data=True)] color_nodes = [dicc_color_nodos[df_nodos_graph.loc[[i], &#39;tipo&#39;].tolist()[0]] for i in G.nodes] fig, ax = plt.subplots() draw_network(G, color_edges=color_edges, color_nodes=color_nodes, axes=ax, labels=[1, 2, 4, 5], text_size=8) fig.savefig(f&#39;graficas/redes/red_{red}.png&#39;, dpi=200) plt.cla() . print(df_asoc.loc[[x for x in df_links.query(&#39;red==@red&#39;)[&#39;destino_id&#39;].unique() if &#39;ocds&#39; in x][0]]) . Tareas: . Procesar telefonos . | procesar múltiples mails . | Es posible obtener más datos de los contratistas a partir del RUCP, como el sitio web, giro del negocio . | . Buscar otra anomalía: todos los contratos con métodos abiertos en los que solo participa un proponente. Buscar contratos en los que todos los particpantes reciben contrato. . Buscar otra anomalía: todos los contratos con métodos abiertos en los que solo participa un proponente. | Buscar contratos en los que todos los particpantes reciben contrato. | . Funcionarios que intervienen en contrataciones . mydb.func_contrat.count_documents({}) . 113795 . mydb.func_contrat.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb255d723b95da59c6a01b&#39;), &#39;id&#39;: &#39;c6dbd706-b539-476f-a400-4dd69ed4a757&#39;, &#39;fechaCaptura&#39;: &#39;&#39;, &#39;ejercicioFiscal&#39;: 2017, &#39;periodoEjercicio&#39;: {&#39;fechaInicial&#39;: &#39;2017/01/01&#39;, &#39;fechaFinal&#39;: &#39;2017/12/31&#39;}, &#39;idRamo&#39;: 6, &#39;ramo&#39;: &#39;HACIENDA Y CRÉDITO PÚBLICO&#39;, &#39;nombres&#39;: None, &#39;primerApellido&#39;: None, &#39;segundoApellido&#39;: None, &#39;genero&#39;: None, &#39;institucionDependencia&#39;: {&#39;siglas&#39;: &#39;CNBV&#39;, &#39;nombre&#39;: &#39;COMISIÓN NACIONAL BANCARIA Y DE VALORES&#39;, &#39;clave&#39;: &#39;6/B00&#39;}, &#39;puesto&#39;: {&#39;nombre&#39;: &#39;SUBDIRECTOR DE MEJORA A&#39;, &#39;nivel&#39;: None}, &#39;tipoArea&#39;: [&#39;R&#39;], &#39;nivelResponsabilidad&#39;: [&#39;A&#39;, &#39;T&#39;], &#39;tipoProcedimiento&#39;: 1, &#39;tipoActos&#39;: &#39;CONTRATACIONES&#39;, &#39;superiorInmediato&#39;: {&#39;nombres&#39;: None, &#39;primerApellido&#39;: None, &#39;segundoApellido&#39;: None, &#39;puesto&#39;: {&#39;nombre&#39;: None, &#39;nivel&#39;: None}}} . nombre_func_contrat = [f&#39;{r[&quot;nombres&quot;]} {r[&quot;primerApellido&quot;]} {r[&quot;segundoApellido&quot;]}&#39; for r in mydb.func_contrat.find({}, {&#39;_id&#39;:0, &#39;nombres&#39;: 1, &#39;primerApellido&#39;: 1, &#39;segundoApellido&#39;:1}) if all([r[&quot;nombres&quot;], r[&quot;primerApellido&quot;], r[&quot;segundoApellido&quot;]])] . Funcionarios sancionados . mydb.serv_sanc.count_documents({}) . 3575 . mydb.serv_sanc.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb27a2432e395ca7ba4a62&#39;), &#39;nombres&#39;: &#39;ZACARIAS&#39;, &#39;primerApellido&#39;: &#39;PEREZ&#39;, &#39;segundoApellido&#39;: &#39;GARCIA&#39;, &#39;institucionDependencia&#39;: {&#39;nombre&#39;: &#39;PROCURADURIA GENERAL DE LA REPUBLICA&#39;, &#39;siglas&#39;: &#39; &#39;}, &#39;autoridadSancionadora&#39;: &#39;ORGANO INTERNO DE CONTROL&#39;, &#39;expediente&#39;: &#39;520/99&#39;, &#39;resolucion&#39;: {&#39;fechaResolucion&#39;: &#39;17/11/2000&#39;}, &#39;tipoSancion&#39;: &#39;INHABILITACION&#39;, &#39;inhabilitacion&#39;: {&#39;fechaInicial&#39;: &#39;17/11/2000&#39;, &#39;fechaFinal&#39;: &#39;16/11/2020&#39;, &#39;observaciones&#39;: None}, &#39;multa&#39;: {&#39;monto&#39;: None, &#39;moneda&#39;: &#39;MXN&#39;}, &#39;causaMotivoHechos&#39;: &#39;ABUSO DE AUTORIDAD&#39;, &#39;puesto&#39;: &#39;AGENTE DE LA POLICIA JUDICIAL FEDERAL&#39;} . nombre_serv_sanc = [f&#39;{r[&quot;nombres&quot;]} {r[&quot;primerApellido&quot;]} {r[&quot;segundoApellido&quot;]}&#39; for r in mydb.serv_sanc.find({}, {&#39;_id&#39;:0, &#39;nombres&#39;: 1, &#39;primerApellido&#39;: 1, &#39;segundoApellido&#39;:1})] . casos_func = [p[&#39;contactPoint&#39;][&#39;name&#39;] for c in casos_contratos for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;procuringEntity&#39;]] . set(casos_func).intersection(set(nombre_serv_sanc)) . set() . yt = list(set(df_contactos.name.unique().tolist()).intersection(set(nombre_serv_sanc))) len(yt) . 10 . yt . [&#39;MIGUEL ANGEL TORRES HERNANDEZ&#39;, &#39;AGUSTIN TOLEDO GADEA&#39;, &#39;OSCAR CHAVEZ MARTINEZ&#39;, &#39;JOSE LUIS CHAVEZ FLORES&#39;, &#39;MARIO HERNANDEZ DIAZ&#39;, &#39;RODRIGO MALDONADO SAHAGUN&#39;, &#39;ERIKA BENITEZ GARCIA&#39;, &#39;FRANCISCO FIERRO SILVA&#39;, &#39;JOSE LUIS GARCIA RODRIGUEZ&#39;, &#39;JOSE DE LA CRUZ RAMIREZ&#39;] . Particulares sancionados . mydb.part_sanc.count_documents({}) . 1853 . mydb.part_sanc.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb27d0715998a251b6be6b&#39;), &#39;fechaCaptura&#39;: &#39;2019-08-22&#39;, &#39;expediente&#39;: &#39;000270074/2017&#39;, &#39;nombreRazonSocial&#39;: &#39;CONSTRUCCIÓN ESPECIALIZADA Y TECNOLÓGICA DE MÉXICO, S.A. DE C.V.&#39;, &#39;rfc&#39;: &#39;ACV990407&#39;, &#39;telefono&#39;: &#39;01 961 61 5 30 09&#39;, &#39;domicilio&#39;: {&#39;clave&#39;: &#39;MX&#39;}, &#39;tipoSancion&#39;: &#39;ECONOMICA E INHABILITACIÓN&#39;, &#39;institucionDependencia&#39;: {&#39;nombre&#39;: &#39;SECRETARIA DE LA FUNCIÓN PÚBLICA&#39;, &#39;siglas&#39;: &#39;SFP&#39;}, &#39;tipoFalta&#39;: &#39;&#39;, &#39;causaMotivoHechos&#39;: &#39;NO ENTREGAR LA OBRA EN LA FECHA COMPROMETIDA PARA ELLO, ESTO ES EL 24 DE SEPTIEMBRE DE 2014&#39;, &#39;objetoContrato&#39;: &#39;&#39;, &#39;autoridadSancionadora&#39;: &#39;SECRETARIA DE LA FUNCIÓN PÚBLICA&#39;, &#39;responsableSancion&#39;: {&#39;nombres&#39;: &#39;MARÍA GUADALUPE VARGAS ÁLVAREZ&#39;, &#39;primerApellido&#39;: &#39;&#39;, &#39;segundoApellido&#39;: &#39;&#39;}, &#39;resolucion&#39;: {&#39;sentido&#39;: &#39;SANCIONATORIA CON MULTA E INHABILITACIÓN&#39;}, &#39;fechaNotificacion&#39;: &#39;2019-08-14&#39;, &#39;multa&#39;: {&#39;monto&#39;: &#39;504675.00&#39;, &#39;moneda&#39;: &#39;MXN&#39;}, &#39;plazo&#39;: {&#39;fechaInicial&#39;: &#39;2019-08-23&#39;}, &#39;observaciones&#39;: None} . rfc_sanc = [r[&#39;rfc&#39;] for r in mydb.part_sanc.find({&#39;rfc&#39;: {&#39;$ne&#39;: &#39;&#39;}}, {&#39;_id&#39;:0, &#39;rfc&#39;: 1})] . Red Mitchell . result1 = mydb.contrataciones.find({&#39;contracts&#39;: {&#39;$exists&#39;: True}}, [&#39;ocid&#39;, &#39;parties.id&#39;, &#39;parties.roles&#39;, &#39;parties.contactPoint&#39;, &#39;contracts.value.amount&#39;, &#39;date&#39;]) l1 = list(result1) . ocid_tenderer = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;id&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;] in [[&#39;tenderer&#39;, &#39;supplier&#39;], [&#39;tenderer&#39;]]], columns=[&#39;ocid&#39;, &#39;tenderer_id&#39;], ).set_index(&#39;ocid&#39;) ocid_tenderer.head() . tenderer_id . ocid . ocds-07smqs-1003803 E9C1C827AE1234CCF7AC4D9070BB597C | . ocds-07smqs-1003123 SCA031118BX7 | . ocds-07smqs-1003123 SAU0505307M9 | . ocds-07smqs-1003123 SCK070618C21 | . ocds-07smqs-1009245 R&amp;S811221KR6 | . ocid_funcionario = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;contactPoint&#39;].get(&#39;name&#39;, &#39;&#39;), p[&#39;id&#39;], c[&#39;contracts&#39;][0][&#39;value&#39;][&#39;amount&#39;], c[&#39;date&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;procuringEntity&#39;]], columns=[&#39;ocid&#39;, &#39;funcionario_id&#39;, &#39;uc_id&#39;, &#39;valor_contrato&#39;, &#39;fecha&#39;]) .set_index(&#39;ocid&#39;) ocid_dependencia = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;id&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;buyer&#39;]], columns=[&#39;ocid&#39;, &#39;dep_id&#39;]) .set_index(&#39;ocid&#39;) ocid_funcionario.head() . funcionario_id uc_id valor_contrato fecha . ocid . ocds-07smqs-1003803 José Gabriel Ramos Martínez | SAT970701NN3-006E00002 | 8451072.00 | 2016-02-19T01:09:18Z | . ocds-07smqs-1003123 Ignacio Romero Sánchez | PGR850101RC6-017000017 | 168000.00 | 2016-02-19T01:49:22Z | . ocds-07smqs-1009245 Juan Fernando Meza Zavala | STP401231P53-014000999 | 420689.55 | 2016-02-26T05:33:08Z | . ocds-07smqs-1012355 Luis Eduardo Vega Becerra | CNU800928K31-018E00999 | 20000.00 | 2016-03-02T01:58:39Z | . ocds-07smqs-1025654 Marco Antonio Brito Vidales | IAA6210025R4-006A00996 | 10604000.00 | 2016-03-18T06:40:28Z | . ocid_tender_fun = ocid_tenderer.join([ocid_funcionario, ocid_dependencia]) ocid_tender_fun . tenderer_id funcionario_id uc_id valor_contrato fecha dep_id . ocid . ocds-07smqs-1001024 ELE9012281G2 | Evelyn López Valverde | LIC950821M84-020VST003 | 1.152540e+05 | 2016-03-15T01:02:50Z | LICONSA-231 | . ocds-07smqs-1001040 HIG090519H30 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 2.603075e+07 | 2016-02-16T02:44:58Z | HIM-163 | . ocds-07smqs-1001984 282910F3163E9D7DBC543E53CD9347B6 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 1.071380e+05 | 2016-02-17T04:42:35Z | HIM-163 | . ocds-07smqs-1002362 IPS040121S66 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 2.115000e+05 | 2016-02-17T07:30:57Z | HIM-163 | . ocds-07smqs-1003123 SCA031118BX7 | Ignacio Romero Sánchez | PGR850101RC6-017000017 | 1.680000e+05 | 2016-02-19T01:49:22Z | PGR-251 | . ... ... | ... | ... | ... | ... | ... | . ocds-07smqs-999514 CPC131113AT4 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 96A74A55F4E5DAEC0797B59049D8EC81 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 TGH130612IK1 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 SIN011023UC8 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 GMC09121623A | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . 726038 rows × 6 columns . ocid_tender_fun.to_csv(f&#39;{dir_datos}/ocid_tender_fun.csv&#39;) .",
            "url": "http://blog.jjsantoso.com/analisis%20de%20datos/pandas/pymongo/2020/01/20/redes-contratos-compranet.html",
            "relUrl": "/analisis%20de%20datos/pandas/pymongo/2020/01/20/redes-contratos-compranet.html",
            "date": " • Jan 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Bio",
          "content": ". Soy un economista apasionado por el análisis de datos, la estadística y la visualización. Trabajo en la Unidad de Ciencia de Datos del Laboratorio Nacional de Políticas Públicas del CIDE donde hacemos análisis de datos para temas de políticas públicas. A veces doy cursos de Python. A veces participo en datatones. Aquí pueden conocer algo de los proyectos personales en los que me gusta trabajar. . Soy de Cartagena (Colombia) y vivo en la Ciudad de México. Me gusta el montañismo y conocer la riqueza natural y cultural de México. . Me encuentran en Twitter como @jjsantoso y aquí está mi perfil en LinkedIn. Pueden consultar aquí mi CV o también revisar mi cuenta de GitHub. .",
          "url": "http://blog.jjsantoso.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.jjsantoso.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}