{
  
    
        "post0": {
            "title": "Etiquetado de variables y valores en las encuestas de INEGI usando Python",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Introducci&#243;n . Hace poco me tocó trabajar con los datos de una encuesta de INEGI y usé Python para hacer el análisis descriptivo. Quienes trabajamos con datos del INEGI hemos visto que es usual que los archivos de datos abiertos vengan en varias carpetas que contienen tanto los datos como los metadatos e información adicional sobre la encuesta. Por ejemplo, si descargamos los datos para la Encuesta Nacional de Seguridad Pública Urbana (ENSU) veremos que vienen 3 carpetas, cada una corresponde a una sección de la encuesta: . Note: Los datos se pueden descargar directamente de la página de INEGI, pero aquí dejo una copia de los que yo usé para este tutorial. Descargar datos . conjunto_de_datos_VIV_ENSU_12_2020: Cuestionario sociodemográfico sección I y II | conjunto_de_datos_CS_ENSU_12_2020: Cuestionario sociodemográfico sección III | conjunto_de_datos_CB_ENSU_12_2020: Cuestionario principal de la encuesta sección I, II, III y IV | . Si vemos al interior de uno de estos módulos, la estructura incluye las siguientes carpetas: . Catálogos: tiene los catálogos para cada variable en el cuestionario | Conjunto de datos: tiene los datos principales de la encuesta | Diccionario de datos: el nombre e información de cada variable | Metadatos: tiene información de la encuesta. | Modelo entidad relación: es un diagrama que muestra cómo se relacionan los diferentes conjuntos de datos. | . . Si echamos un vistazo rápido a los datos en Excel (conjunto_de_datos/conjunto_de_datos_CB_ENSU_12_2020.csv) veremos que la mayor parte de las variables viene codificada. Solo con este archivo no podemos saber qué es cada columna y cuáles es el significado de sus valores. Nos hace falta el diccionario de variables y los catálogos para poder interpretarlas. . . En el archivo diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv tenemos cuál es el texto de cada pregunta. De ahí sabemos que, por ejemplo, la pregunta BP1_1 es &quot;Percepción de seguridad en la ciudad&quot;. Estos valores se conocen como etiquetas de las variables. . Por otro lado, dentro de la carpeta catalogos viene un archivo csv por cada variable del conjunto de datos. . . Este archivo nos dice cómo debemos transformar los valores numéricos de las categorías por sus valores de texto. Por ejemplo, si abrimos el archivo &quot;BP1_1.csv&quot; su contenido nos muestra que para la variable BP1_1 debemos interpretar que un 1 corresponde a la categorías &quot;seguro?&quot;, el 2 corresponde a &quot;inseguro?&quot; y el 9 a &quot;No sabe/No responde&quot;. Estos valores se conocen como etiquetas de los valores. . Note: Las etiquetas de valores tiene sentido para variables categóricas, es decir las que tienen pocos valores nominales. Para variables numéricas o puramente de texto no es necesario usar etiquetas de valores. . . Es evidente que para manejar una encuesta es fundamental conocer las etiquetas de las variables y sus valores. Sería mucho más fácil si estas etiquetas estuvieran incluidas en el mismo archivo junto con los datos, pero como están en formato .csv no es posible guadar esa información en un solo archivo y por tanto, termina repartida en muchos. Entonces, nuestro objetivo es integrar el diccionario y el catálogo a los datos para que sea más fácil hacer nuestro análisis. Para lograr esto usaremos objetos tipo diccionario nativos de Python y dataframes de Pandas. . Tip: Otros formatos de datos, como por ejemplo los archivos .dta de Stata o .sav de SPSS sí permiten guardar esas etiquetas junto con los datos, sin embargo, esos no son formatos de datos abiertos que sean fácilmente accesible. En algunos casos, como en la ENOE, INEGI también publica archivos .dta y .sav. . Datos . Primero, vamos a importar las librerías necesarias. . import glob import sys import pandas as pd print(&#39;Python&#39;, sys.version) print(pd.__name__, pd.__version__) . Python 3.8.5 (default, Sep 3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] pandas 1.1.3 . Para ilustrar, vamos a seleccionar los datos de la carpeta conjunto_de_datos_CB_ENSU_12_2020 (Cuestionario principal de la encuesta sección I, II, III y IV). . datos = pd.read_csv(&#39;conjunto_de_datos_CB_ENSU_12_2020/conjunto_de_datos/conjunto_de_datos_CB_ENSU_12_2020.csv&#39;) datos.head() . ID_VIV ID_PER UPM VIV_SEL R_SEL CVE_ENT NOM_ENT CVE_MUN NOM_MUN LOC ... BP4_1_5 BP4_1_6 BP4_1_7 BP4_1_8 BP4_1_9 FAC_SEL DOMINIO EST UPM_DIS EST_DIS . 0 100188.049 | 0100188.049.03 r | 100188 | 1 | 3 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 4046 | U r | 3 | 10 | 1390 | . 1 100188.072 | 0100188.072.06 r | 100188 | 2 | 6 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 6069 | U r | 3 | 10 | 1390 | . 2 100188.093 | 0100188.093.03 r | 100188 | 3 | 3 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 3035 | U r | 3 | 10 | 1390 | . 3 100188.111 | 0100188.111.01 r | 100188 | 5 | 1 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 3035 | U r | 3 | 10 | 1390 | . 4 100295.009 | 0100295.009.01 r | 100295 | 1 | 1 | 1 | Aguascalientes r | 1 | Aguascalientes r | 1 | ... | 2 | 2 | 2 | 2 | 2 | 1704 | U r | 4 | 20 | 1400 | . 5 rows × 176 columns . datos.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 22283 entries, 0 to 22282 Columns: 176 entries, ID_VIV to EST_DIS dtypes: float64(1), int64(84), object(91) memory usage: 29.9+ MB . Necesitamos el diccionario de variables y los catálogos. Para el diccionario de variables cargamos el archivo conjunto_de_datos_CB_ENSU_12_2020/diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv . preguntas = pd.read_csv(&#39;conjunto_de_datos_CB_ENSU_12_2020/diccionario_de_datos/diccionario_de_datos_CB_ENSU_12_2020.csv&#39;, encoding=&#39;latin1&#39;) preguntas.head(10) . NOMBRE_CAMPO NEMONICO TIPO LONGITUD RANGO_CLAVES . 0 Identificador de vivienda seleccionada | ID_VIV | Alfanumérico | 11 | 0100001.001,,3299999.999 r | . 1 Identificador del informante seleccionado | ID_PER | Alfanumérico | 14 | 0100001.001.01,..., 3299999.999.30 r | . 2 Unidad Primaria de Muestreo | UPM | Numérico | 7 | 01000013299999 r | . 3 Vivienda seleccionada | VIV_SEL | Numérico | 2 | 0109 r | . 4 Número de renglón de la persona seleccionada | R_SEL | Numérico | 2 | 0130 r | . 5 Clave Entidad | CVE_ENT | Numérico | 2 | 01 r | . 6 Clave Entidad | CVE_ENT | Numérico | 2 | 02 r | . 7 Clave Entidad | CVE_ENT | Numérico | 2 | 03 r | . 8 Clave Entidad | CVE_ENT | Numérico | 2 | 04 r | . 9 Clave Entidad | CVE_ENT | Numérico | 2 | 05 r | . Este nos muestra cómo aparece cada variable en el archivo de datos (&quot;NEMONICO&quot;) y cuál es su descripción (&quot;NOMBRE_CAMPO&quot;), además contiene el tipo y rango de valores que puede tener cada variable. Por esta razón, el nombre de cada variable puede estar repetido varias veces. Lo que haremos es eliminar los duplicados y quedarnos con los valores únicos, para después crear un objeto diccionario que tenga como llaves el &quot;NEMONICO&quot; y como valores el &quot;NOMBRE_CAMPO&quot;. A continuación se ve el proceso y el resultado: . dicc_preguntas = preguntas.drop_duplicates(subset=[&#39;NEMONICO&#39;]).set_index(&#39;NEMONICO&#39;)[&#39;NOMBRE_CAMPO&#39;].to_dict() print(str(dicc_preguntas)[:500]) . {&#39;ID_VIV&#39;: &#39;Identificador de vivienda seleccionada&#39;, &#39;ID_PER&#39;: &#39;Identificador del informante seleccionado&#39;, &#39;UPM&#39;: &#39;Unidad Primaria de Muestreo &#39;, &#39;VIV_SEL&#39;: &#39;Vivienda seleccionada&#39;, &#39;R_SEL&#39;: &#39;Número de renglón de la persona seleccionada&#39;, &#39;CVE_ENT&#39;: &#39;Clave Entidad&#39;, &#39;NOM_ENT&#39;: &#39;Nombre de la Entidad&#39;, &#39;CVE_MUN&#39;: &#39;Clave Municipio&#39;, &#39;NOM_MUN&#39;: &#39;Nombre del Municipio&#39;, &#39;LOC&#39;: &#39;Localidad&#39;, &#39;CD&#39;: &#39;Ciudad&#39;, &#39;NOM_CD&#39;: &#39;Nombre de la Ciudad&#39;, &#39;PER&#39;: &#39;Periodo de la entrevista&#39;, &#39;R_DEF&#39;: &#39;Resultado definiti . Para el catálogo tenemos que trabajar un poco más porque la información está en muchos archivos. Primero vamos a generar una lista de todos los archivos en la carpeta catalogo. . archivos_catalogo_respuestas = glob.glob(&#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos/*.csv&#39;) archivos_catalogo_respuestas[:10] . [&#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_1.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_10_1.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_10_2.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_10_3.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_10_4.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_10_5.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_2_01.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_2_02.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_2_03.csv&#39;, &#39;conjunto_de_datos_CB_ENSU_12_2020/catalogos BP1_2_04.csv&#39;] . A continuación vamos a leer cada uno de los archivos individuales y generar un diccionario por comprensión que contenga como llaves el &quot;NEMONICO&quot; y los valores serán otro diccionario que tiene la relación de los valores numéricos y de texto para los valores de cada variable. . dicc_respuestas = { f.split(&#39; &#39;)[1][:-4]: pd.read_csv(f, encoding=&#39;latin1&#39;, index_col=f.split(&#39; &#39;)[1][:-4])[&#39;descrip&#39;].to_dict() for f in archivos_catalogo_respuestas } print(str(dicc_respuestas)[:500]) . {&#39;BP1_1&#39;: {1: &#39;seguro?&#39;, 2: &#39;inseguro?&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_1&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_2&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_3&#39;: {1: &#39;Mucha confianza&#39;, 2: &#39;Algo de confianza&#39;, 3: &#39;Algo de desconfianza&#39;, 4: &#39;Mucha desconfianza&#39;, 9: &#39;No sabe / no responde&#39;}, &#39;BP1_10_4&#39; . Ahora tenemos dos diccionarios, uno con las etiquetas de las variables (dicc_preguntas) y otro con las etiquetas de los valores de cada pregunta dicc_respuestas. En ambos diccionarios la llave principal es el némonico de la pregunta, por tanto si queremos saber cuáles son las etiquetas de variables y valores solo tenemos que indexar los diccionarios con el nemónico correspondiente, por ejemplo para &quot;SEX&quot; y &quot;BP1_1&quot; obtenemos: . print(&#39;Etiqueta de variable:&#39;, dicc_preguntas[&#39;SEX&#39;], &#39; nEtiqueta de valores&#39;,dicc_respuestas[&#39;SEX&#39;]) . Etiqueta de variable: Sexo Etiqueta de valores {1: &#39;Hombre&#39;, 2: &#39;Mujer&#39;} . print(&#39;Etiqueta de variable:&#39;, dicc_preguntas[&#39;BP1_1&#39;], &#39; nEtiqueta de valores&#39;,dicc_respuestas[&#39;BP1_1&#39;]) . Etiqueta de variable: Percepción de seguridad en la ciudad Etiqueta de valores {1: &#39;seguro?&#39;, 2: &#39;inseguro?&#39;, 9: &#39;No sabe / no responde&#39;} . Uso de las etiquetas . Ya que tenemos las etiquetas, veamos cómo podemos usarlas para interpretar mejor en nuestros análisis. Hagamos una tabulación cruzada para ver la distribución de respuestas de las variables &quot;SEX&quot; y &quot;BP1_1&quot; . Warning: En estos ejemplos no estamos considerando que cada observación tiene una ponderación distinta (variable FAC_SEL) por tanto los resultados no son estimaciones válidas. En una próxima entrada veremos cómo integrar las ponderaciones. . pd.crosstab(datos[&#39;SEX&#39;], datos[&#39;BP1_1&#39;]) . BP1_1 1 2 9 . SEX . 1 4268 | 5849 | 33 | . 2 3703 | 8379 | 51 | . El índice del dataframe y los nombres de las columnas contienen los valores numéricos de las categorías. Vamos a reemplazarlos por sus valores de texto renombrándolo con el método .rename() . pd.crosstab(datos[&#39;SEX&#39;], datos[&#39;BP1_1&#39;]) .rename(index=dicc_respuestas[&#39;SEX&#39;], columns=dicc_respuestas[&#39;BP1_1&#39;]) . BP1_1 seguro? inseguro? No sabe / no responde . SEX . Hombre 4268 | 5849 | 33 | . Mujer 3703 | 8379 | 51 | . Ahora es mucho más fácil de entender esta tabla. . Vamos a hacer otra tabla similar a la anterior, pero en este caso desagregando por más variables y usando el método groupby: . vars_by = [&#39;SEX&#39;, &#39;BP1_1&#39;, &#39;BP1_5_1&#39;] grouped = datos.groupby(vars_by).agg(N=(&#39;ID_PER&#39;, &#39;count&#39;)) grouped.head(15) . N . SEX BP1_1 BP1_5_1 . 1 1 1 1255 | . 2 2821 | . 3 190 | . 9 2 | . 2 1 3587 | . 2 2010 | . 3 249 | . 9 3 | . 9 1 14 | . 2 16 | . 3 3 | . 2 1 1 1463 | . 2 1995 | . 3 243 | . 9 2 | . Nuevamente reemplazamos los valores de las categorías usando el método .rename(). Hacemos un loop para reemplazar las categorías de cada variable. Como tenemos varios niveles en el índice, especificamos la opción level para que use solo el catálogo con la variable que le corresponde. . for v in vars_by: grouped.rename(index=dicc_respuestas[v], level=v, inplace=True) grouped . N . SEX BP1_1 BP1_5_1 . Hombre seguro? Sí 1255 | . No 2821 | . No aplica 190 | . No sabe / no responde 2 | . inseguro? Sí 3587 | . No 2010 | . No aplica 249 | . No sabe / no responde 3 | . No sabe / no responde Sí 14 | . No 16 | . No aplica 3 | . Mujer seguro? Sí 1463 | . No 1995 | . No aplica 243 | . No sabe / no responde 2 | . inseguro? Sí 5783 | . No 2192 | . No aplica 400 | . No sabe / no responde 4 | . No sabe / no responde Sí 19 | . No 23 | . No aplica 7 | . No sabe / no responde 2 | . Para que sea más fácil de ver, reestructuramos la tabla usando .unstack(). . grouped.unstack(&#39;BP1_1&#39;) . N . BP1_1 No sabe / no responde inseguro? seguro? . SEX BP1_5_1 . Hombre No 16.0 | 2010.0 | 2821.0 | . No aplica 3.0 | 249.0 | 190.0 | . No sabe / no responde NaN | 3.0 | 2.0 | . Sí 14.0 | 3587.0 | 1255.0 | . Mujer No 23.0 | 2192.0 | 1995.0 | . No aplica 7.0 | 400.0 | 243.0 | . No sabe / no responde 2.0 | 4.0 | 2.0 | . Sí 19.0 | 5783.0 | 1463.0 | . Ya integramps las etiquetas de los valores, ahora faltan las etiquetas de las variables. Para eso usaremos el método .rename_axis() . cuadro = grouped.unstack(&#39;BP1_1&#39;) .rename_axis(index=dicc_preguntas, columns=dicc_preguntas) cuadro . N . Percepción de seguridad en la ciudad No sabe / no responde inseguro? seguro? . Sexo Cambiar sus hábitos respecto a llevar cosas de valor por temor a sufrir algún delito . Hombre No 16.0 | 2010.0 | 2821.0 | . No aplica 3.0 | 249.0 | 190.0 | . No sabe / no responde NaN | 3.0 | 2.0 | . Sí 14.0 | 3587.0 | 1255.0 | . Mujer No 23.0 | 2192.0 | 1995.0 | . No aplica 7.0 | 400.0 | 243.0 | . No sabe / no responde 2.0 | 4.0 | 2.0 | . Sí 19.0 | 5783.0 | 1463.0 | . Nuestro cuadro ya es entendible, podemos exportarlo a Excel y verificar que tenemos las etiquetas: . cuadro.to_excel(&#39;reporte.xlsx&#39;) . . Si usas Stata...(o incluso si no) . Como dije antes, el formato .dta permite guardar las etiquetas de variables y valores junto con los datos. Los DataFrames de Pandas traen de forma nativa el método .to_stata() para exportar la información a este formato. Para que Stata reconozca correctamnete las etiquetas de valores es necesario que en pandas las variables sean de tipo categoria. A continuación, vamos a seleccionar un subconjunto de variables, reemplazaremos sus valores numéricos por las categorías de texto y convertiremos la variable en tipo categórica: . vars_export = [&#39;SEX&#39;, &#39;BP1_1&#39;, &#39;BP1_2_01&#39;, &#39;BP1_2_02&#39;, &#39;BP1_2_03&#39;, &#39;BP1_2_04&#39;, &#39;BP1_2_05&#39;, &#39;BP1_2_06&#39;, &#39;BP1_2_07&#39;, &#39;BP1_2_08&#39;, &#39;BP1_2_09&#39;, &#39;BP1_2_10&#39;, &#39;BP1_2_11&#39;, &#39;BP1_2_12&#39;] datos_stata = datos[vars_export].apply(lambda s: s.map(dicc_respuestas[s.name])).astype(&#39;category&#39;) datos_stata . SEX BP1_1 BP1_2_01 BP1_2_02 BP1_2_03 BP1_2_04 BP1_2_05 BP1_2_06 BP1_2_07 BP1_2_08 BP1_2_09 BP1_2_10 BP1_2_11 BP1_2_12 . 0 Hombre | seguro? | Seguro(a) | No aplica | Seguro(a) | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | No aplica | . 1 Hombre | inseguro? | Seguro(a) | Inseguro(a) | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | Inseguro(a) | Inseguro(a) | Inseguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | . 2 Hombre | inseguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | No aplica | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . 3 Mujer | inseguro? | Inseguro(a) | No aplica | Inseguro(a) | No aplica | Inseguro(a) | No aplica | No aplica | No aplica | No aplica | Inseguro(a) | No aplica | No aplica | . 4 Hombre | seguro? | Seguro(a) | Inseguro(a) | Seguro(a) | No aplica | No aplica | No aplica | Inseguro(a) | No aplica | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 22278 Hombre | inseguro? | Seguro(a) | Seguro(a) | Seguro(a) | No aplica | Inseguro(a) | Seguro(a) | Inseguro(a) | Inseguro(a) | Seguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | . 22279 Hombre | inseguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | No aplica | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Seguro(a) | Seguro(a) | No aplica | . 22280 Hombre | seguro? | Seguro(a) | No aplica | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | No aplica | Seguro(a) | Seguro(a) | No aplica | . 22281 Mujer | inseguro? | Seguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Inseguro(a) | Inseguro(a) | . 22282 Mujer | seguro? | Seguro(a) | Seguro(a) | Inseguro(a) | No aplica | Inseguro(a) | Seguro(a) | Inseguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | Seguro(a) | Inseguro(a) | . 22283 rows × 14 columns . Este dataframe lo exportaremos a Stata, junto con el diccionario que contiene las etiquetas de variables, especificando en la opción variable_labels. . datos_stata.to_stata(&#39;datos_stata.dta&#39;, write_index=False, variable_labels=dicc_preguntas) . Al abrir el arcivo en Stata podemos ver que efectivamente se guardaron las etiquetas de los valores y de las variables: . . Hasta donde entiendo, esta sería una forma más fácil de etiquetar datos provenientes de INEGI para usuarios de Stata que haciendo el procedimiento entero en Stata. Igualmente, para los que no son usuarios de Stata, pero sí de Python o R y quieren conservar las variables categóricas etiquetadas este es un buen formato. . datos_2 = pd.read_stata(&#39;datos_stata.dta&#39;) datos_2.dtypes . SEX category BP1_1 category BP1_2_01 category BP1_2_02 category BP1_2_03 category BP1_2_04 category BP1_2_05 category BP1_2_06 category BP1_2_07 category BP1_2_08 category BP1_2_09 category BP1_2_10 category BP1_2_11 category BP1_2_12 category dtype: object .",
            "url": "http://blog.jjsantoso.com/etiquetas-encuestas-inegi/",
            "relUrl": "/etiquetas-encuestas-inegi/",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Generando archivos de Excel con formatos y gráficas usando Python",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Excel es un tipo de archivo muy común para compartir datos. No a todos les encanta, pero tiene la ventaja de que es muy popular y muchas personas no familiarizadas con la programación lo usan para su análisis. Una de las funciones de Excel que no es fácil de replicar con otras herramientas es la posibilidad de aplicar estilo a las celdas y generar tablas o reportes más actractivos para presentar los datos. En esta entrada veremos cómo, usando Python, podemos exportar datos a archivos de Excel aplicando estilos y formatos a las celdas. . Como ejemplo usaremos los datos de indicadores de desarrollo del Banco Mundial. En su página puedes ver y descargar mucha información. Acá puedes decargar los archivos que en particular uso en esta entrada. Descárgalos y guárdalos en una carpeta llamada datos. . Lo que haremos es crear fichas para los metadatos que sean más fáciles de leer, ya que leer esta información tal como viene en el archivo original es un poco difícil. La idea es pasar de esto: . . a esto: . . Además, usaremos el formato condicional para establecer detalles como el color de las celdas dependiendo de sus valores. Luego, como bonus, aprovechando que ya sabremos usar xlsxwriter, veremos cómo crear gráficas que se guardan dentro del archivo de Excel. . Para seguir este tutorial es necesario tener instaladas las bibliotecas pandas y xlsxwriter. La biblioteca xlsxwriter es excelente y está muy bien documentada por su autor. La mayor parte de lo que aquí hacemos se basa en sus ejemplos. xlsxwriter se instala usando pip . pip install XlsxWriter . import xlsxwriter import pandas as pd print(xlsxwriter.__name__, xlsxwriter.__version__) print(pd.__name__, pd.__version__) . xlsxwriter 1.2.7 pandas 0.24.2 . Por brevedad solo leeremos las 10 primeras filas del archivo que contienen los metadatos. . metadata = pd.read_csv(&#39;datos/WDISeries.csv&#39;, nrows=10).fillna(&#39;&#39;) metadata.head() . Series Code Topic Indicator Name Short definition Long definition Unit of measure Periodicity Base Period Other notes Aggregation method ... Notes from original source General comments Source Statistical concept and methodology Development relevance Related source links Other web links Related indicators License Type Unnamed: 20 . 0 AG.AGR.TRAC.NO | Environment: Agricultural production | Agricultural machinery, tractors | | Agricultural machinery refers to the number of... | | Annual | | | Sum | ... | | | Food and Agriculture Organization, electronic ... | A tractor provides the power and traction to m... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 1 AG.CON.FERT.PT.ZS | Environment: Agricultural production | Fertilizer consumption (% of fertilizer produc... | | Fertilizer consumption measures the quantity o... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Fertilizer consumption measures the quantity o... | Factors such as the green revolution, has led ... | | | | CC BY-4.0 | | . 2 AG.CON.FERT.ZS | Environment: Agricultural production | Fertilizer consumption (kilograms per hectare ... | | Fertilizer consumption measures the quantity o... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Fertilizer consumption measures the quantity o... | Factors such as the green revolution, has led ... | | | | CC BY-4.0 | | . 3 AG.LND.AGRI.K2 | Environment: Land use | Agricultural land (sq. km) | | Agricultural land refers to the share of land ... | | Annual | | | Sum | ... | | | Food and Agriculture Organization, electronic ... | Agricultural land constitutes only a part of a... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 4 AG.LND.AGRI.ZS | Environment: Land use | Agricultural land (% of land area) | | Agricultural land refers to the share of land ... | | Annual | | | Weighted average | ... | | | Food and Agriculture Organization, electronic ... | Agriculture is still a major sector in many ec... | Agricultural land covers more than one-third o... | | | | CC BY-4.0 | | . 5 rows × 21 columns . Aplicar formato a las celdas . A continuación vamos a crear un libro (workbook) con xlsxwriter llamado fichas_metadatos.xlsx. A este libro le agregamos una hoja (worksheet). . workbook = xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) worksheet = workbook.add_worksheet() . Empezaremos a configurar el tamaño de las filas y columnas de la hoja. Esto lo hacemos con los métodos .set_column() y .set_row(). En .set_column() es importante especificar un rango, incluso si es una sola columna, de lo contrario obtendríamos un error. Las unidades son las mismas que usa Excel para fijar el ancho y alto de las celdas. Cuando terminamos de editar cerramos el libro con workbook.close() y esto escribe el contenido al archivo. . worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 20) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) # Cambiamos tamaños de filas worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 300) worksheet.set_row(5, 100) workbook.close() . Así se ve el resultado hasta ahora: Las celdas no tienen contenido pero sí tienen tamaños diferentes. . Ahora vamos a combinar varias celdas y escribir algo de contenido. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;]) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;]) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;]) . Con el método .merge_range() combinamos las celdas en el rango B1:E1 en una sola celda. Además, escribimos en esta misma celda el contenido del nombre del indicador (Indicator Name) del primer indicador que está en el DataFrame metadata. | Con el método .write() escribimos contenido específico para cada una de las celdas en las siguientes filas. Por ejemplo, en la celda B2 escribimos el texto Código de serie: y en C2 escribimos el código del indicador (Series Code) que viene en la primera fila del indicador. En las celdas D2 y E2 escribimos el tópico del indicador. | Aquí también cambiamos un poco la sintaxis para abrir el libro usando la expresion with ... as ...:. De esta forma todas las operaciones deben quedar bajo la indentación y no hay necesidad de cerrar explícitamente el libro. El resultado se ve así: | . Ahora continuamos escribiendo otras variables en las filas. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;]) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;]) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;]) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;) worksheet.write(&#39;C3&#39;, metadata.loc[0, &#39;Aggregation method&#39;]) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;) worksheet.write(&#39;E3&#39;, metadata.loc[0, &#39;Periodicity&#39;]) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[0, &#39;Long definition&#39;]) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[0, &#39;Source&#39;]) . . Ya que tenemos el contenido de las celdas, lo que nos falta es el aplicar formato para que se vea más atractivo. . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: worksheet = workbook.add_worksheet() # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Formato de titulo formato_titulo = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;center&#39;, &#39;valign&#39;: &#39;vcenter&#39;, &#39;fg_color&#39;: &#39;#333f4f&#39;, &#39;font_color&#39;: &#39;white&#39;, &#39;text_wrap&#39;: True}) # Formato de variables formato_variables = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;fg_color&#39;: &#39;#ddebf7&#39;, &#39;font_color&#39;: &#39;black&#39;, &#39;text_wrap&#39;: True}) # Formato del texto normal formato_normal = workbook.add_format({ &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;text_wrap&#39;: True}) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[0, &#39;Indicator Name&#39;], formato_titulo) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;, formato_variables) worksheet.write(&#39;C2&#39;, metadata.loc[0, &#39;Series Code&#39;], formato_normal) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;, formato_variables) worksheet.write(&#39;E2&#39;, metadata.loc[0, &#39;Topic&#39;], formato_normal) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;, formato_variables) worksheet.write(&#39;C3&#39;, metadata.loc[0, &#39;Aggregation method&#39;], formato_normal) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;, formato_variables) worksheet.write(&#39;E3&#39;, metadata.loc[0, &#39;Periodicity&#39;], formato_normal) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;, formato_variables) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[0, &#39;Long definition&#39;], formato_normal) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;, formato_variables) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[0, &#39;Source&#39;], formato_normal) . Para ello debemos definir los formatos en el libro con el método workbook.add_format() al que le pasamos un diccionaro con las opciones de formato, como por ejemplo si queremos que el texto aparezca en negrita, que esté alineado al centro, el color del texto y el del fondo de la celda. Todas las opciones disponibles y más ejemplos se pueden encontrar en la documentación de xlsxwriter. . Aquí definimos tres tipos de formato: el formato para el nombre del indicador (formato_titulo), el formato para las celdas que tienen el nombre de la variable (formato_variables) y el formato para escribir la información (formato_normal). Para aplicar el formato a una celda, o a un conjunto de ellas, debemos pasar el objeto formato como tercer input al método worksheet.write(). Y así luce el resultado final: . . Estas fichas son mucho más fáciles de leer que en el formato original. . Para finalizar esta parte solo nos queda iterar sobre todo el dataframe y crear una ficha de metadato para cada indicador, cada una en una hoja diferente: . with xlsxwriter.Workbook(f&#39;datos/fichas_metadatos.xlsx&#39;) as workbook: for fila in metadata.index: # agrega nueva ahoja worksheet = workbook.add_worksheet() # nombre de la hoja worksheet.name = metadata.loc[fila, &#39;Series Code&#39;] # Cambiamos tamaños de columnas y filas worksheet.set_column(&#39;B:B&#39;, 25) worksheet.set_column(&#39;C:C&#39;, 30) worksheet.set_column(&#39;D:D&#39;, 25) worksheet.set_column(&#39;E:E&#39;, 50) worksheet.set_row(0, 40) worksheet.set_row(1, 34) worksheet.set_row(2, 34) worksheet.set_row(3, 100) worksheet.set_row(4, 100) # Formato de titulo formato_titulo = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;center&#39;, &#39;valign&#39;: &#39;vcenter&#39;, &#39;fg_color&#39;: &#39;#333f4f&#39;, &#39;font_color&#39;: &#39;white&#39;, &#39;text_wrap&#39;: True}) # Formato de variables formato_variables = workbook.add_format({ &#39;bold&#39;: 1, &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;fg_color&#39;: &#39;#ddebf7&#39;, &#39;font_color&#39;: &#39;black&#39;, &#39;text_wrap&#39;: True}) # Formato del texto normal formato_normal = workbook.add_format({ &#39;border&#39;: 1, &#39;align&#39;: &#39;left&#39;, &#39;valign&#39;: &#39;top&#39;, &#39;text_wrap&#39;: True}) # Escribimos título worksheet.merge_range(&#39;B1:E1&#39;, metadata.loc[fila, &#39;Indicator Name&#39;], formato_titulo) # fila 2 worksheet.write(&#39;B2&#39;, &#39;Código de serie:&#39;, formato_variables) worksheet.write(&#39;C2&#39;, metadata.loc[fila, &#39;Series Code&#39;], formato_normal) worksheet.write(&#39;D2&#39;, &#39;Tópico:&#39;, formato_variables) worksheet.write(&#39;E2&#39;, metadata.loc[fila, &#39;Topic&#39;], formato_normal) # fila 3 worksheet.write(&#39;B3&#39;, &#39;Agregación:&#39;, formato_variables) worksheet.write(&#39;C3&#39;, metadata.loc[fila, &#39;Aggregation method&#39;], formato_normal) worksheet.write(&#39;D3&#39;, &#39;Periodicidad:&#39;, formato_variables) worksheet.write(&#39;E3&#39;, metadata.loc[fila, &#39;Periodicity&#39;], formato_normal) # fila 4 worksheet.write(&#39;B4&#39;, &#39;Definición:&#39;, formato_variables) worksheet.merge_range(&#39;C4:E4&#39;, metadata.loc[fila, &#39;Long definition&#39;], formato_normal) # fila 5 worksheet.write(&#39;B5&#39;, &#39;Fuente:&#39;, formato_variables) worksheet.merge_range(&#39;C5:E5&#39;, metadata.loc[fila, &#39;Source&#39;], formato_normal) . Formato condicional . xlsxwriter también tiene la opción de aplicar formato condicional, esto es que el formato de la celda varíe dependiendo del valor de la celda. Esto es útil cuando reportamos datos y queremos, por ejemplo, que el color de la celda esté relacionado con su valor, de forma que un valor más alto da un color más intenso. Esto se puede hacer directamente en Excel como menciona este tutorial. . Para ver cómo funciona usaremos los datos de uno de los indicadores de desarrollo que vimos antes. El indicador es el de porcentaje de la tierra que es de uso agrícola y se encuentra en el archivo indicador_tierra_agro.csv. El dataframe que creamos se llama datos. . datos = pd.read_csv(&#39;datos/indicador_tierra_agro.csv&#39;) .rename(columns=lambda x: str(x)) datos.head() . Country Name Country Code Indicator Name Indicator Code 1960 1961 1962 1963 1964 1965 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 Unnamed: 64 . 0 Arab World | ARB | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 27.835643 | 27.826564 | 27.845522 | 27.847925 | 27.866972 | ... | 36.440808 | 36.472300 | 36.534503 | 36.607475 | 36.624759 | 36.610850 | NaN | NaN | NaN | NaN | . 1 Caribbean small states | CSS | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 5.518775 | 5.526186 | 5.533597 | 5.538538 | 5.484190 | ... | 6.198839 | 6.186983 | 6.215388 | 6.226504 | 6.245770 | 6.268000 | NaN | NaN | NaN | NaN | . 2 Central Europe and the Baltics | CEB | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 64.667028 | 64.625380 | 64.540412 | 64.591952 | 64.402941 | ... | 47.871658 | 47.515760 | 46.958264 | 46.895589 | 46.988619 | 46.715708 | NaN | NaN | NaN | NaN | . 3 Early-demographic dividend | EAR | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 35.425733 | 35.400661 | 35.345373 | 35.313357 | 35.283656 | ... | 41.439188 | 41.511835 | 41.537124 | 41.476279 | 41.464427 | 41.466296 | NaN | NaN | NaN | NaN | . 4 East Asia &amp; Pacific | EAS | Agricultural land (% of land area) | AG.LND.AGRI.ZS | NaN | 43.329285 | 43.552552 | 43.807975 | 44.062835 | 44.395486 | ... | 48.795620 | 48.666721 | 48.340167 | 48.777005 | 47.678013 | 47.783780 | NaN | NaN | NaN | NaN | . 5 rows × 65 columns . Vamos a seleccionar solo el nombre del país (o grupo de países) y un par de años (2010 y 2016) de datos. Estos los vamos a guardar con formato condicional usando el siguiente código: . with pd.ExcelWriter(&#39;datos/indicador_agro_formato.xlsx&#39;, engine=&#39;xlsxwriter&#39;) as excelfile: workbook = excelfile.book # Agregamos datos al libro sheetname = &#39;agro&#39; datos[[&#39;Country Name&#39;, &#39;2000&#39;, &#39;2016&#39;]].to_excel(excelfile, sheet_name=sheetname, index=False) # Definimos formatos formato_porcentaje = workbook.add_format({&#39;num_format&#39;: &#39;0.0 %&#39;, &#39;font_color&#39;: &#39;#FFFFFF&#39;}) formato_bg_blanco = workbook.add_format({&#39;bg_color&#39;: &#39;#FFFFFF&#39;}) formato_escala = {&#39;type&#39;: &#39;2_color_scale&#39;, &#39;min_color&#39;: &#39;#D9D9D9&#39;, &#39;max_color&#39;: &#39;#808080&#39;} worksheet = excelfile.sheets[sheetname] # Configuramos formato a los datos worksheet.set_column(&#39;A:A&#39;, 40, formato_bg_blanco) worksheet.set_column(&#39;B2:C265&#39;, 20, formato_porcentaje) worksheet.conditional_format(&#39;B2:C265&#39;, formato_escala) . Acá algunos detalles de lo que hace: . En esta ocasión usamos la función pd.ExcelWriter para crear el archivo excelfile, que a su vez tiene un libro (workbook). Usamos el método .to_excel() para guardar los datos en excelfile. | Definimos varios formatos. El primero, formato_porcentaje, es para que los número se vean con el símbolo de porcentaje; el segundo formato, formato_bg_blanco, es para que en la primera columna no se noten las divisiones de las celdas; y el tercero, formato_escala, es para hacer un formato de escala de 2 colores que va desde un color para el valor mínimo hasta otro en el valor máximo. Los valores intermedios reciben colores intermedios según una escala de colores lineal. | Lo que sigue es aplicar estos formatos a los valores de la hoja donde habíamos guardado los datos. El formato condicional se especifica con el método worksheet.conditional_format(). | . El resultado en Excel es el siguiente: . . Gr&#225;ficas en Excel . Por último, aprovechando que le entendemos un poco a xlsxwriter, podemos ver cómo hacer gráficas de Excel con los datos del libro. Puede parecer un poco extraño hacer gráficas de Excel usando Python si podemos hacerlas directamente en Python usando una biblioteca como Matplotlib, sin embargo, una ventaja de Excel es que cualquiera puede editar luego las gráficas, mientras que las creadas mediante Matplotlib no son directamente editables. . En este caso usamos los mismos datos anteriores del indicador de porcentaje de la tierra con uso agrícola para hacer una gráfica de barras. . with pd.ExcelWriter(&#39;datos/grafica_indicador_agro.xlsx&#39;, engine=&#39;xlsxwriter&#39;) as excelfile: sheet_name = &#39;agro&#39; # Guarda los datos en el archivo datos[[&#39;Country Name&#39;, &#39;2000&#39;, &#39;2016&#39;]].to_excel(excelfile, sheet_name=sheet_name, index=False) # Obtiene el libro y hoja de trabajo workbook = excelfile.book worksheet = excelfile.sheets[sheet_name] # Crea un objeto tipo gráfica chart = workbook.add_chart({&#39;type&#39;: &#39;bar&#39;}) # Configura las series chart.add_series({ &#39;categories&#39;: f&#39;={sheet_name}!$A$2:$A$15&#39;, &#39;values&#39;: f&#39;={sheet_name}!$B$2:$B$15&#39;, &#39;name&#39;: f&#39;={sheet_name}!$B$1&#39;, &#39;gap&#39;: 8, }) chart.add_series({ &#39;categories&#39;: f&#39;={sheet_name}!$A$2:$A$15&#39;, &#39;values&#39;: f&#39;={sheet_name}!$C$2:$C$15&#39;, &#39;name&#39;: f&#39;={sheet_name}!$C$1&#39;, &#39;gap&#39;: 8, }) # configura opciones del gráfico chart.set_size({&#39;width&#39;: 650, &#39;height&#39;: 400}) chart.set_title({&#39;name&#39;: &#39;Porcentaje de tierra de uso agrícola&#39;, &#39;name_font&#39;: {&#39;size&#39;: 12}}) chart.set_x_axis({&#39;name&#39;: &#39;Porcentaje&#39;, &#39;major_gridlines&#39;: {&#39;visible&#39;: True}}) # Introduce el grafico en la hoja worksheet.insert_chart(&#39;D2&#39;, chart) . Para agregar una gráfica creamos un objeto chart usando el método workbook.add_chart(), en este caso especificando que es de tipo barra. | Al objeto chart le pasamos 2 series, la del año 2010 y la del 2016. Solo seleccionamos las primeras 15 observaciones para no sobrecargar la gráfica. Las categorías son los nombres de los países, columna A, y las especificamos con la sintaxis de Excel para rangos de valores. Hacemos igual con los valores, que están en las columnas B y C. Ponemos los nombres de las series y también la opción gap, esta última para controlar el ancho de las barras. | Configuramos el tamaño de la gráfica, el título y la etiqueta en el eje x. | Finalmente introducimos la gráfica en la hoja de los datos, en la celda D2. | . El resultado luce así en Excel: . . Pueden encontrar más ejemplos de gráficas en la documentación de xlsxwriter. . Con esto terminamos esta entrada cuyo objetivo principal fue introducir las funcionalidades que ofrece xlsxwriter para crear archivos de Excel con formatos mucho más ricos. A mí me ha servido mucho en mi trabajo, espero que a ti también pueda resultarte útil. .",
            "url": "http://blog.jjsantoso.com/excel-formato-graficas/",
            "relUrl": "/excel-formato-graficas/",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Trabajando con archivos de Excel complejos en Pandas",
            "content": "Elaborado por Juan Javier Santos Ochoa (@jjsantoso) . Pandas es la biblioteca por excelencia para trabajar con datos tabulares en Python. Muchos de estos datos, especialmente los que vienen de instituciones públicas, están en formato de Excel, que en ocasiones pueden ser particularmente difíciles de leer por su estructura con celdas combinadas. En esta entrada veremos cómo usar algunas de las opciones y trucos de Pandas para leer estos archivos complejos de forma efectiva. Es necesario tener instaladas las bibliotecas pandas y xlrd. . Como ejemplo usaremos los tabulados del mercado laboral que publica el INEGI para México. Estos datos se pueden descargar desde su página: https://www.inegi.org.mx/programas/enoe/15ymas/default.html#Tabulados . En particular, usaremos los tabulados del primer trimestre de 2020 para el estado de Aguascalientes. Acá puedes decargar el archivo que uso en este notebook y acá uno con todos los estados. . El archivo luce de la siguiente forma visto en Excel. . Encabezado: . . Final del archivo: . . Podemos ver que la estructura de los datos es bastante compleja. Algunos de los desafíos de este archivo son: . El encabezado de la tabla de datos no empieza desde la primera fila, sino a partir de la fila 6 | El encabezado está conformado por 3 filas (la 6, 7 y 8) con diferentes niveles de información. Por ejemplo, el primer nivel contiene los valores &quot;Enero-Marzo 2020&quot;, &quot;Coeficientes de Variación (%)&quot;, &quot;Errores Estándar&quot; e &quot;Intervalos de Confianza al 90%&quot;. El segundo nivel solo contiene valores para las columnas que están bajo &quot;Intervalos de Confianza al 90%&quot;. El tercer nivel es la desagregación por sexo, junto con el total, excepto en la columna &quot;Intervalos de Confianza al 90%&quot;, donde representa los límites inferiores y superiores. | Hay hasta 4 niveles de desagregación de los indicadores, que vienen especificados en las columnas A, B, C y D. Estos niveles expresan jerarquía entre las categorías. Por ejemplo, la celda C14 hace referencia a la población desocupada, que hace parte de la PEA (B12) y de la población de 15 años y más (A11) | Después que la tabla de datos termina, hay un montón de notas al pie y comentarios en las celdas siguientes, que pueden ser entendidas como datos. | Las variables de las que nos interesa obtener información están como filas, cuando quisiéramos que fueran columnas. | . Queda muy claro que estos datos distan mucho de tener una estructura Tidy. . En ocasiones ante una estructura tan compleja, lo más fácil es hacer manualmente los cambios necesarios para que la tabla quede en un formato mucho más entendible para nuestro programa que va a leer los datos. Esto es válido cuando solo hay que modificar uno o pocos archivos, pero si se trata de un proceso que se tiene que aplicar para muchos archivos o se va a estar haciendo de manera recurrente tenemos que pensar en una forma de automatizar el preprocesamiento. . Afortunadamente Pandas cuenta con características que nos ayudan mucho con este tipo de archivos que tienen múltiples niveles en las filas y en las columnas. Esto coincide bastante bien con las características de Multindex y Multicolumn de los dataframes. . Empecemos importando los datos y viendo cómo lucen si los cargamos tal cual como vienen. . import pandas as pd df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;) df.head() . INEGI. Encuesta Nacional de Ocupación y Empleo. Indicadores estratégicos. Primer trimestre de 2020 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 Indicadores Estratégicos de Ocupación y Empleo... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 Entidad Federativa: | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 Aguascalientes | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | NaN | INDICADOR | Enero - Marzo 2020 | NaN | NaN | Coeficientes de Variación (%) | NaN | NaN | Errores Estándar | NaN | NaN | Intervalos de Confianza al 90% | NaN | NaN | NaN | NaN | NaN | . Podemos hacer muy poco con esto, no hay nombres en las columnas y hay valores nulos por todos lados. Veamos la parte final del dataframe, donde tenemos todas las notas al pie, que realmente no es información que necesitamos para el análisis. . df.tail(7) . INEGI. Encuesta Nacional de Ocupación y Empleo. Indicadores estratégicos. Primer trimestre de 2020 Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 . 296 14 | Se consideran &quot;personas con interés para traba... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 297 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 298 Las estimaciones que aparecen en este cuadro e... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 299 Nivel de precisión de las estimaciones: | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 300 Alta, CV en el rango de (0,15) | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 301 Moderada, CV en el rango de [15, 30) | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 302 Baja, CV de 30% en adelante | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Para empezar a arreglar un poco las cosas, usemos las opción header que nos permite especificar cuáles son las filas que son el encabezado de los datos. Si tuvieran un formato tidy entonces solo necesitaríamos una fila como header, pero en este caso tenemos 3 niveles para el header. Pandas nos permite especificar varios niveles si a la opción header le pasamos una lista con los número de las filas (contando desde 0). . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7]) df.head(3) . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 0_level_1 Unnamed: 1_level_1 Unnamed: 2_level_1 Unnamed: 3_level_1 Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 0 I. Población total 1 | NaN | NaN | NaN | 1363581.0 | 661998.0 | 701583.0 | 1.423850 | 1.77911 | 1.43904 | 19415.0 | 11777 | 10096 | 1.33164e+06 | 1.39552e+06 | 642624 | 681372 | 684975 | 718191 | . 1 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 2. Población de 15 años y más | NaN | NaN | NaN | 1014307.0 | 484719.0 | 529588.0 | 1.350378 | 1.6399 | 1.4333 | 13696.0 | 7948 | 7590 | 991775 | 1.03684e+06 | 471643 | 497795 | 517102 | 542074 | . El resultado que obtenemos es un dataframe con 3 niveles de columnas (Multicolumn). Ahora al menos los datos empiezan donde deberían. Hagamos también que la tabla termine donde debe terminar. Para eso usamos la opción skipfooter a la que le especificamos el número de filas que debe ignorar partiendo desde la última hacia arriba. En este caso son 16 filas con contenido que no nos interesa. . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7], skipfooter=16) df.tail(3) . Unnamed: 0_level_0 Unnamed: 1_level_0 Unnamed: 2_level_0 INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 0_level_1 Unnamed: 1_level_1 Unnamed: 2_level_1 Unnamed: 3_level_1 Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Unnamed: 0_level_2 Unnamed: 1_level_2 Unnamed: 2_level_2 Unnamed: 3_level_2 Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 277 NaN | Tasas calculadas contra la población ocupada n... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 278 NaN | NaN | Tasa de ocupación en el sector informal 2 (TOSI2) | NaN | 20.1963 | 22.8466 | 16.3304 | 4.116332 | 4.48344 | 6.14149 | 0.831347 | 1.02432 | 1.00293 | 18.829 | 21.564 | 21.162 | 24.532 | 14.681 | 17.98 | . 279 NaN | NaN | Tasa de informalidad laboral 2 (TIL2) | NaN | 39.3077 | 37.4318 | 42.0439 | 2.655421 | 3.20142 | 3.24678 | 1.043784 | 1.19835 | 1.36507 | 37.591 | 41.025 | 35.461 | 39.403 | 39.798 | 44.289 | . Ahora el dataframe termina donde está el último indicador, que es &quot;Tasa de informalidad laboral 2 (TIL2)&quot;. . Ahora continuaremos con la opción index_col que le permite a Pandas entender que las primeras 4 columnas serán el índice del dataframe. Lo mejor es que además entiende la estructura jerárquica que está implícita en las celdas combinadas. . df = pd.read_excel(&#39;datos/2020_trim_1_Entidad_Aguascalientes.xls&#39;, header=[5, 6, 7], skipfooter=16, index_col=[0, 1, 2, 3]) df.tail() . INDICADOR Enero - Marzo 2020 Coeficientes de Variación (%) Errores Estándar Intervalos de Confianza al 90% . Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 Unnamed: 7_level_1 Unnamed: 8_level_1 Unnamed: 9_level_1 Unnamed: 10_level_1 Unnamed: 11_level_1 Unnamed: 12_level_1 Total Hombres Mujeres . Total Hombres Mujeres Total Hombres Mujeres Total Hombres Mujeres LIIC LSIC LIIC LSIC LIIC LSIC . 10. Tasas Tasas calculadas contra la población ocupada Tasa de ocupación en el sector informal 1 (TOSI1) Mediana 19.4007 | 21.4890 | 16.1900 | 4.109763 | 4.49951 | 6.14659 | 0.797323 | 0.9669 | 0.995131 | 18.089 | 20.712 | 19.898 | 23.08 | 14.553 | 17.827 | . Tasa de informalidad laboral 1 (TIL1) Mediana 40.9383 | 40.0765 | 42.2632 | 2.532868 | 2.97923 | 3.20628 | 1.036912 | 1.19397 | 1.35508 | 39.233 | 42.644 | 38.112 | 42.041 | 40.034 | 44.492 | . Tasas calculadas contra la población ocupada no agropecuaria Tasa de informalidad laboral 1 (TIL1) Mediana NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Tasa de ocupación en el sector informal 2 (TOSI2) Mediana 20.1963 | 22.8466 | 16.3304 | 4.116332 | 4.48344 | 6.14149 | 0.831347 | 1.02432 | 1.00293 | 18.829 | 21.564 | 21.162 | 24.532 | 14.681 | 17.98 | . Tasa de informalidad laboral 2 (TIL2) Mediana 39.3077 | 37.4318 | 42.0439 | 2.655421 | 3.20142 | 3.24678 | 1.043784 | 1.19835 | 1.36507 | 37.591 | 41.025 | 35.461 | 39.403 | 39.798 | 44.289 | . En este caso nos interesa obtener las estimaciones de los indicadores y no los coeficientes de variación y los otros cálculos. Estos datos están bajo la columna &quot;Enero - Marzo 2020&quot;, así que ya que tenemos encabezados es fácil obtenerlos. . df[&#39;Enero - Marzo 2020&#39;].head() . Unnamed: 4_level_1 Unnamed: 5_level_1 Unnamed: 6_level_1 . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . NaN NaN | NaN | NaN | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Queda sobrando un nivel que en realidad no necesitamos porque no agrega nada de información [&#39;Unnamed: 4_level_1&#39;, &#39;Unnamed: 5_level_1&#39;, &#39;Unnamed: 6_level_1&#39;]. Este lo podemos eliminar con el método .droplevel() . valores = df[&#39;Enero - Marzo 2020&#39;].droplevel(level=0, axis=1) valores.head() . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . NaN NaN | NaN | NaN | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Ya tenemos un resultado bastante útil. Todavía nos quedan algunos ajustes que hacer. Primero, hay que eliminar las filas que solo contienen valores nulos . valores = valores.dropna(subset=[&#39;Total&#39;, &#39;Hombres&#39;, &#39;Mujeres&#39;]) valores.head() . Total Hombres Mujeres . I. Población total 1 NaN NaN NaN 1363581.0 | 661998.0 | 701583.0 | . 2. Población de 15 años y más NaN NaN NaN 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) NaN NaN 603802.0 | 366686.0 | 237116.0 | . Ocupada NaN 583762.0 | 353706.0 | 230056.0 | . Desocupada NaN 20040.0 | 12980.0 | 7060.0 | . Ahora pongamos nombres a los niveles para que sea fácil identificarlos. También cambiemos los NaN que hay en el índice por un valor de texto, como por ejemplo &quot;Total&quot;. Esto ayuda porque pandas no maneja muy bien valores nulos en el índice. . valores = valores.rename_axis(index=[&#39;nivel_1&#39;, &#39;nivel_2&#39;, &#39;nivel_3&#39;, &#39;nivel_4&#39;], columns=[&#39;sexo&#39;]) .rename(lambda x: &#39;Total&#39; if pd.isna(x) else x) valores.head() . sexo Total Hombres Mujeres . nivel_1 nivel_2 nivel_3 nivel_4 . I. Población total 1 Total Total Total 1363581.0 | 661998.0 | 701583.0 | . 2. Población de 15 años y más Total Total Total 1014307.0 | 484719.0 | 529588.0 | . Población económicamente activa (PEA) Total Total 603802.0 | 366686.0 | 237116.0 | . Ocupada Total 583762.0 | 353706.0 | 230056.0 | . Desocupada Total 20040.0 | 12980.0 | 7060.0 | . Y bueno, ya con esto prácticamente podemos obtener el valor de cualquiera de los indicadores. Por ejemplo, si queremos la &quot;Población económicamente activa (PEA)&quot; . pea = valores.loc[(&#39;2. Población de 15 años y más&#39;, &#39;Población económicamente activa (PEA)&#39;)] pea . C: ProgramData Anaconda3 lib site-packages ipykernel_launcher.py:1: PerformanceWarning: indexing past lexsort depth may impact performance. &#34;&#34;&#34;Entry point for launching an IPython kernel. . sexo Total Hombres Mujeres . nivel_3 nivel_4 . Total Total 603802.0 | 366686.0 | 237116.0 | . Ocupada Total 583762.0 | 353706.0 | 230056.0 | . Desocupada Total 20040.0 | 12980.0 | 7060.0 | . En este resultado el nivel_4 es innecesario, así que lo podemos eliminar. En general, podemos eliminar cualquier nivel que no aporte información para quedarnos con una estructura más sencilla. Además modificamos la estructura para que sea tidy y cada columna sea una variable . pea.droplevel(1) .T .add_prefix(&#39;poblacion_&#39;) .reset_index() . nivel_3 sexo poblacion_Total poblacion_Ocupada poblacion_Desocupada . 0 Total | 603802.0 | 583762.0 | 20040.0 | . 1 Hombres | 366686.0 | 353706.0 | 12980.0 | . 2 Mujeres | 237116.0 | 230056.0 | 7060.0 | . Podemos intentar con otro indicador como la Tasa de informalidad laboral 1 (TIL1), haciendo algunas otras modificaciones: . valores.loc[(&#39;10. Tasas&#39;, &#39;Tasas calculadas contra la población ocupada&#39;, &#39;Tasa de informalidad laboral 1 (TIL1)&#39;)] .unstack(&#39;sexo&#39;) .to_frame(&#39;til_1&#39;) .droplevel(&#39;nivel_4&#39;) .reset_index() . C: ProgramData Anaconda3 lib site-packages ipykernel_launcher.py:1: PerformanceWarning: indexing past lexsort depth may impact performance. &#34;&#34;&#34;Entry point for launching an IPython kernel. . sexo til_1 . 0 Total | 40.9383 | . 1 Hombres | 40.0765 | . 2 Mujeres | 42.2632 | . Podemos convertir este procedimiento en una función para que podamos obtener estas variables para cualquier estado. Por ejemplo, para obtener la población económicamente activa creamos esta función que depende solo del nombre del estado (como aparece en el archivo de INEGI que descargué) . def obtiene_pea(edo: str): df = pd.read_excel(f&#39;datos/2020_trim_1_Entidad_{edo}.xls&#39;, header=[5, 6, 7], skipfooter=16, index_col=[0, 1, 2, 3]) pea = df[&#39;Enero - Marzo 2020&#39;].droplevel(level=0, axis=1) .dropna(subset=[&#39;Total&#39;, &#39;Hombres&#39;, &#39;Mujeres&#39;]) .rename_axis(index=[&#39;nivel_1&#39;, &#39;nivel_2&#39;, &#39;nivel_3&#39;, &#39;nivel_4&#39;], columns=[&#39;sexo&#39;]) .rename(lambda x: &#39;Total&#39; if pd.isna(x) else x) .sort_index() .loc[(&#39;2. Población de 15 años y más&#39;, &#39;Población económicamente activa (PEA)&#39;)] .droplevel(1) .T .add_prefix(&#39;poblacion_&#39;) .reset_index() .assign(estado=edo.replace(&#39;_&#39;, &#39; &#39;)) return pea . Probamos la función en otro estado y nos da el resultado esperado: . obtiene_pea(&#39;Oaxaca&#39;) . nivel_3 sexo poblacion_Desocupada poblacion_Ocupada poblacion_Total estado . 0 Total | 30743.0 | 1766690.0 | 1797433.0 | Oaxaca | . 1 Hombres | 18948.0 | 999821.0 | 1018769.0 | Oaxaca | . 2 Mujeres | 11795.0 | 766869.0 | 778664.0 | Oaxaca | . Acá por ejemplo, obtenemos la pea para los estados del Sur-Sureste mexicano: . pea_sur = pd.concat([obtiene_pea(e) for e in [&#39;Oaxaca&#39;, &#39;Chiapas&#39;, &#39;Tabasco&#39;, &#39;Campeche&#39;, &#39;Quintana_Roo&#39;, &#39;Yucatán&#39;]], ignore_index=True) pea_sur . nivel_3 sexo poblacion_Desocupada poblacion_Ocupada poblacion_Total estado . 0 Total | 30743.0 | 1766690.0 | 1797433.0 | Oaxaca | . 1 Hombres | 18948.0 | 999821.0 | 1018769.0 | Oaxaca | . 2 Mujeres | 11795.0 | 766869.0 | 778664.0 | Oaxaca | . 3 Total | 55562.0 | 2068483.0 | 2124045.0 | Chiapas | . 4 Hombres | 32413.0 | 1417037.0 | 1449450.0 | Chiapas | . 5 Mujeres | 23149.0 | 651446.0 | 674595.0 | Chiapas | . 6 Total | 57702.0 | 1031968.0 | 1089670.0 | Tabasco | . 7 Hombres | 31558.0 | 643777.0 | 675335.0 | Tabasco | . 8 Mujeres | 26144.0 | 388191.0 | 414335.0 | Tabasco | . 9 Total | 12364.0 | 435961.0 | 448325.0 | Campeche | . 10 Hombres | 7750.0 | 269822.0 | 277572.0 | Campeche | . 11 Mujeres | 4614.0 | 166139.0 | 170753.0 | Campeche | . 12 Total | 25607.0 | 851473.0 | 877080.0 | Quintana_Roo | . 13 Hombres | 14241.0 | 525859.0 | 540100.0 | Quintana_Roo | . 14 Mujeres | 11366.0 | 325614.0 | 336980.0 | Quintana_Roo | . 15 Total | 21992.0 | 1086089.0 | 1108081.0 | Yucatán | . 16 Hombres | 11421.0 | 651820.0 | 663241.0 | Yucatán | . 17 Mujeres | 10571.0 | 434269.0 | 444840.0 | Yucatán | . De esta manera logré obtener los indicadores que necesitaba de un archivo que parecía imposible de aprovechar en su estado original. . Lo que más quería destacar en esta entrada es que Pandas, con su estructura de multindex, puede facilitar mucho leer archivos de Excel cuya estructura incluye celdas combinadas y anidadas. No hay garantía de que siempra se pueda leer adecuadamente archivos de Excel muy complejos, pero es bueno saber que tampoco está todo perdido si llega a tus manos uno de estos mosntruos. .",
            "url": "http://blog.jjsantoso.com/pandas-excel/",
            "relUrl": "/pandas-excel/",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Redes y contratos públicos",
            "content": "Imports y globals . Imports . import pymongo import pandas as pd import json import matplotlib.pyplot as plt import networkx as nx from NetworkUtils import draw_network . globals . dir_datos = &#39;d:/datos/licitaciones_compranet&#39; myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;) mydb = myclient[&#39;dataton2019&#39;] . Funciones . An&#225;lisis de asociados . Contacpoints . Creaci&#243;n de la base . Creamos una base de datos que tenga identificado a cada punto de contacto. | Los puntos de contacto se obtienen a partir de los contratistas que tienen información o su dirección. | . resultado = mydb.contrataciones.find({}, {&#39;_id&#39;: 0, &#39;parties.contactPoint&#39;: 1, &#39;parties.roles&#39;: 1, &#39;parties.id&#39;: 1, &#39;parties.address&#39;: 1}) . df_contactos = pd.DataFrame([{**p.get(&#39;contactPoint&#39;, &#39;&#39;), **p.get(&#39;address&#39;, &#39;&#39;), &#39;tenderer_id&#39;: p[&#39;id&#39;] } for x in resultado for p in x[&#39;parties&#39;] if (p[&#39;roles&#39;] in [[&#39;tenderer&#39;], [&#39;tenderer&#39;, &#39;supplier&#39;]]) &amp; ((bool(p.get(&#39;address&#39;, None))) | (bool(p.get(&#39;contactPoint&#39;, None))))]) df_contactos.to_pickle(f&#39;{dir_datos}/tenderers_contacpoint.pkl&#39;) df_contactos.head() . name email telephone streetAddress locality region postalCode countryName tenderer_id faxNumber . 0 IRAM LIEVANOS VELAZQUEZ | laroca.canino@gmail.com | 52 722 093749 | PASEO DE LA ASUNCION NO. 536 | METEPEC | MX-MEX | 52148 | MÉXICO | E9C1C827AE1234CCF7AC4D9070BB597C | NaN | . 1 NaN | servillantas@prodigy.net.mx | 10191684 | JOSE MORAN 66 | MIGUEL HIDALGO | MX-CMX | 11850 | MÉXICO | SCA031118BX7 | NaN | . 2 NaN | sportingautoreparaciones@gmail.com | 55 56397681 55 54894622 | PLUTARCO ELIAS CALLES No. 660. COL. SAN FRANCI... | Iztacalco | MX-CMX | 08230 | MÉXICO | SAU0505307M9 | NaN | . 3 EDGAR GUSTAVO TREJO KEMPER | edgarg_kemper@hotmail.com; balcazar-sol@hotmai... | 5543419836 | CALZADA VALLEJO NUMERO 1020 | AZCAPOTZALCO | MX-CMX | 02300 | MÉXICO | SCK070618C21 | 52 55 55873415 ext 201, 202, 203 | . 4 Daniel Ernesto De la Fuente Barra | daniel.delafuente@segurossura.com.mx | 5519636830 | BLVD ADOLFO LOPEZ MATEOS 2448 | ALVARO OBREGON | MX-CMX | 01060 | MÉXICO | R&amp;S811221KR6 | 57237999 Ext. 7965 | . De la base de datos de contrataciones seleccionamos los datos de contacto de los que han ganado licitaciones. Estos tiene datos como nombre de la persona de contacto, email, teléfono, número de fax, dirección y id del proveedor | . Uso de la base . df_contactos = pd.read_pickle(f&#39;{dir_datos}/tenderers_contacpoint.pkl&#39;) df_contactos.head() . name email telephone streetAddress locality region postalCode countryName tenderer_id faxNumber . 0 IRAM LIEVANOS VELAZQUEZ | laroca.canino@gmail.com | 52 722 093749 | PASEO DE LA ASUNCION NO. 536 | METEPEC | MX-MEX | 52148 | MÉXICO | E9C1C827AE1234CCF7AC4D9070BB597C | NaN | . 1 NaN | servillantas@prodigy.net.mx | 10191684 | JOSE MORAN 66 | MIGUEL HIDALGO | MX-CMX | 11850 | MÉXICO | SCA031118BX7 | NaN | . 2 NaN | sportingautoreparaciones@gmail.com | 55 56397681 55 54894622 | PLUTARCO ELIAS CALLES No. 660. COL. SAN FRANCI... | Iztacalco | MX-CMX | 08230 | MÉXICO | SAU0505307M9 | NaN | . 3 EDGAR GUSTAVO TREJO KEMPER | edgarg_kemper@hotmail.com; balcazar-sol@hotmai... | 5543419836 | CALZADA VALLEJO NUMERO 1020 | AZCAPOTZALCO | MX-CMX | 02300 | MÉXICO | SCK070618C21 | 52 55 55873415 ext 201, 202, 203 | . 4 Daniel Ernesto De la Fuente Barra | daniel.delafuente@segurossura.com.mx | 5519636830 | BLVD ADOLFO LOPEZ MATEOS 2448 | ALVARO OBREGON | MX-CMX | 01060 | MÉXICO | R&amp;S811221KR6 | 57237999 Ext. 7965 | . df_contactos.shape . (563693, 10) . De todos los contratos encontramos 563693 puntos de contacto. Muchos de estos se repiten porque un contratista que ganó varias veces aparecerá como un contacto por cada contrato ganado. | Calculamos todas las combinaciones únicas de telefono y tenderer_id. | Luego verificamos si existen casos en los que varios tenderer_id comparten el: teléfono: 700 casos en los que eso ocurre. | Email: 839 casos. | Nombre: 706 casos | Número de fax: 135 casos | Dirección de la calle: 172 casos | . | Todos estos son signos de sospecha. | En muchos casos hay cuentas de funcionarios públicos. Habría que verificar cuál es su papel. | La pregunta relevante es ¿Hay casos en los que contratistas que tienen contactos en común hayan participado en un mismo proceso de licitación? | . variables_contacto = [&#39;telephone&#39;, &#39;email&#39;, &#39;name&#39;, &#39;streetAddress&#39;, &#39;faxNumber&#39;] casos = [] for var_duplicated in variables_contacto: # Encontramos todos los valores únicos de la variable de contacto y de tenderer_id dups_direccion = df_contactos.loc[lambda x: (~x.duplicated(subset=[&#39;tenderer_id&#39;, var_duplicated])) &amp; (x[var_duplicated].notnull())] .loc[lambda x: (x[var_duplicated].duplicated()) &amp; (~x[&#39;name&#39;].str[:22].eq(&#39;- (Cuenta administrada&#39;)), var_duplicated].unique() # Encontramos cuáles son los tenderers_id que comparten un mismo contacto tenderers_dup_id = [df_contactos.loc[lambda x: x[var_duplicated].eq(dup)].drop_duplicates(subset=[&#39;tenderer_id&#39;])[&#39;tenderer_id&#39;].tolist() for dup in dups_direccion] # Buscamos los contratos en los que participaron los ids asociados queries_dup = [[{&#39;parties.id&#39;: i} for i in x] for x in tenderers_dup_id] for q in queries_dup: resultado = list(mydb.contrataciones.find({&#39;$and&#39;: q}, {&#39;_id&#39;: 0, &#39;ocid&#39;: 1})) if resultado: tenderers_id = [x[&#39;parties.id&#39;] for x in q] ocids = list({x[&#39;ocid&#39;] for x in resultado}) casos.append({&#39;tenderer_ids&#39;: tenderers_id, &#39;contratos_ocid&#39;: ocids, &#39;variable&#39;: var_duplicated}) print(q) with open(&#39;datos/casos_colusion.json&#39;, &#39;w&#39;, encoding=&#39;utf8&#39;) as jsonfile: json.dump(casos, jsonfile) . El resultado que encontramos es que existen 571 casos de contratistas posiblemente relacionados en una misma licitación. | . with open(&#39;datos/casos_colusion.json&#39;, &#39;r&#39;, encoding=&#39;utf8&#39;) as jsonfile: casos = json.load(jsonfile) . casos_ocid = list({c for cas in casos for c in cas[&#39;contratos_ocid&#39;]}) len(casos_ocid) . 571 . casos_contratos = list(mydb.contrataciones.find({&#39;ocid&#39;: {&#39;$in&#39;: casos_ocid}})) len(casos_contratos) . 619 . datos_contrato = [{&#39;titulo&#39;: c[&#39;contracts&#39;][0][&#39;title&#39;], &#39;descr&#39;: c[&#39;contracts&#39;][0].get(&#39;description&#39;, &#39;&#39;), &#39;valor&#39;: c[&#39;contracts&#39;][0][&#39;value&#39;][&#39;amount&#39;], &#39;dependencia_id&#39;: c[&#39;buyer&#39;][&#39;id&#39;], &#39;dependencia_nombre&#39;: c[&#39;buyer&#39;][&#39;name&#39;], &#39;uc_id&#39;: c[&#39;tender&#39;][&#39;procuringEntity&#39;][&#39;id&#39;], &#39;uc_name&#39;: c[&#39;tender&#39;][&#39;procuringEntity&#39;][&#39;name&#39;], &#39;ocid&#39;: c[&#39;ocid&#39;], &#39;fecha&#39;: c[&#39;date&#39;], } for c in casos_contratos if c.get(&#39;contracts&#39;, None)] df_datos_contratos = pd.DataFrame(datos_contrato).set_index(&#39;ocid&#39;) df_datos_contratos.head() . titulo descr valor dependencia_id dependencia_nombre uc_id uc_name fecha . ocid . ocds-07smqs-1317308 Servicio Integral de Suministro, Mantenimiento... | Servicio Integral de Suministro, Mantenimiento... | 450000.00 | CNBV-80 | Comisión Nacional Bancaria y de Valores | CNB950501PT6-006B00001 | CNBV-Dirección General Adjunta de Adquisicione... | 2017-03-29T05:13:19Z | . ocds-07smqs-1367848 PRESTADOR DE SERVICIOS INTEGRALES | PRESTADOR DE SERVICIOS INTEGRALES (HONORARIOS) | 44542.62 | SAGARPA-261 | Secretaría de Agricultura, Ganadería, Desarrol... | SAG010710V98-008000995 | SAGARPA-Delegacion Chihuahua #008000995 | 2017-05-22T12:24:14Z | . ocds-07smqs-1430619 CONTRATACIÓN ABIERTA DEL SERVICIO DE LECTURA E... | CONTRATACIÓN ABIERTA DEL SERVICIO DE LECTURA E... | 220000.00 | SEP-265 | Secretaría de Educación Pública | SEP210905778-011000999 | SEP-Dirección de Adquisiciones #011000999 | 2017-07-20T06:22:43Z | . ocds-07smqs-1444924 SERVICIO DE MANTENIMIENTO CORRECTIVO AL SISTEM... | SERVICIO DE MANTENIMIENTO CORRECTIVO AL SISTEM... | 287780.00 | CONAGUA-94 | Comisión Nacional del Agua | CNA890116SF2-016B00009 | CONAGUA-Gerencia de Resursos Materiales #016B0... | 2017-08-03T01:58:53Z | . ocds-07smqs-1452158 SERVICIO DE DIFUSIÓN EN MEDIOS DIGITALES DE LA... | SERVICIO DE DIFUSIÓN EN MEDIOS DIGITALES DE LA... | 68950.00 | CONUEE-98 | Comisión Nacional para el Uso Eficiente de la ... | CNU800928K31-018E00999 | CONUEE-Dirección de Recursos Materiales y Serv... | 2017-08-30T04:21:41Z | . casos_contratos[0] . {&#39;_id&#39;: ObjectId(&#39;5dcdb0c10d84ead5c49d2e99&#39;), &#39;publisher&#39;: {&#39;uid&#39;: &#39;27511&#39;, &#39;name&#39;: &#39;SECRETARÍA DE LA FUNCIÓN PÚBLICA&#39;, &#39;uri&#39;: &#39;http://www.gob.mx/sfp&#39;}, &#39;cycle&#39;: 2017, &#39;ocid&#39;: &#39;ocds-07smqs-1317308&#39;, &#39;id&#39;: &#39;SFP-1317308-2018-11-12&#39;, &#39;date&#39;: &#39;2017-03-29T05:13:19Z&#39;, &#39;tag&#39;: [&#39;tender&#39;, &#39;award&#39;], &#39;initiationType&#39;: &#39;tender&#39;, &#39;parties&#39;: [{&#39;name&#39;: &#39;Comisión Nacional Bancaria y de Valores&#39;, &#39;id&#39;: &#39;CNBV-80&#39;, &#39;roles&#39;: [&#39;buyer&#39;]}, {&#39;name&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;, &#39;legalName&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;Av. Insurgentes Sur No. 1971, Torre Sur, Piso 6, Col. Guadalupe Inn&#39;, &#39;locality&#39;: &#39;Álvaro Obregón&#39;, &#39;region&#39;: &#39;Ciudad de México&#39;, &#39;postalCode&#39;: &#39;01020&#39;, &#39;countryName&#39;: &#39;MX&#39;}, &#39;contactPoint&#39;: {&#39;name&#39;: &#39;Jannet Miriam Martínez Sánchez&#39;, &#39;email&#39;: &#39;jmartinezs@cnbv.gob.mx&#39;, &#39;telephone&#39;: &#39;1454-6537 y 1454-6538&#39;}, &#39;roles&#39;: [&#39;procuringEntity&#39;]}, {&#39;name&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;, &#39;legalName&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {}, &#39;contactPoint&#39;: {}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;id&#39;: &#39;GSA0310175N4&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;GSA0310175N4&#39;, &#39;legalName&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;AZTECAS 81 LA ROMANA&#39;, &#39;locality&#39;: &#39;Tlalnepantla de Baz&#39;, &#39;region&#39;: &#39;MX-MEX&#39;, &#39;postalCode&#39;: &#39;54050&#39;, &#39;countryName&#39;: &#39;MÉXICO&#39;}, &#39;contactPoint&#39;: {&#39;email&#39;: &#39;rafael@sanmari.com.mx&#39;, &#39;telephone&#39;: &#39;55-52409421&#39;}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;, &#39;legalName&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {}, &#39;contactPoint&#39;: {}, &#39;roles&#39;: [&#39;tenderer&#39;]}, {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;, &#39;identifier&#39;: {&#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;, &#39;legalName&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;scheme&#39;: &#39;MX-RFC&#39;, &#39;uri&#39;: &#39;https://portalsat.plataforma.sat.gob.mx/ConsultaRFC&#39;}, &#39;address&#39;: {&#39;streetAddress&#39;: &#39;CEIBAS 45&#39;, &#39;locality&#39;: &#39;NAUCALPAN DE JUAREZ&#39;, &#39;region&#39;: &#39;MX-MEX&#39;, &#39;postalCode&#39;: &#39;53240&#39;, &#39;countryName&#39;: &#39;MÉXICO&#39;}, &#39;contactPoint&#39;: {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;email&#39;: &#39;mascontrolmenoscosto@yahoo.com.mx&#39;, &#39;telephone&#39;: &#39;525536259819&#39;}, &#39;roles&#39;: [&#39;tenderer&#39;, &#39;supplier&#39;]}], &#39;buyer&#39;: {&#39;name&#39;: &#39;Comisión Nacional Bancaria y de Valores&#39;, &#39;id&#39;: &#39;CNBV-80&#39;}, &#39;tender&#39;: {&#39;id&#39;: &#39;1317308&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;complete&#39;, &#39;procuringEntity&#39;: {&#39;name&#39;: &#39;CNBV-Dirección General Adjunta de Adquisiciones y Contratos #006B00001&#39;, &#39;id&#39;: &#39;CNB950501PT6-006B00001&#39;}, &#39;items&#39;: [{&#39;id&#39;: &#39;7044016&#39;, &#39;description&#39;: &#39;Contratación de una Póliza de Seguro de Accidentes Personales para la protección de los participantes en acciones de capacitación del Programa de Apoyo al Empleo 2016.&#39;, &#39;classification&#39;: {&#39;id&#39;: &#39;33900006&#39;, &#39;description&#39;: &#39;Servicios de seguros de gastos medicos mayores&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;}}], &#39;value&#39;: {&#39;amount&#39;: 0}, &#39;procurementMethod&#39;: &#39;direct&#39;, &#39;procurementMethodRationale&#39;: &#39;Art. 42 párrafo primero&#39;, &#39;submissionMethod&#39;: [&#39;electronicSubmission&#39;], &#39;tenderPeriod&#39;: {&#39;startDate&#39;: &#39;2017-03-29T05:13:19Z&#39;}, &#39;enquiryPeriod&#39;: {&#39;startDate&#39;: &#39;2017-03-29T05:13:19Z&#39;}, &#39;hasEnquiries&#39;: False, &#39;awardPeriod&#39;: {&#39;endDate&#39;: &#39;2017-03-30T00:00:00Z&#39;}, &#39;numberOfTenderers&#39;: 4, &#39;tenderers&#39;: [{&#39;name&#39;: &#39;MARIANA REGALADO SOBERON&#39;, &#39;id&#39;: &#39;3CAB041C0551441CB0A31EAC594B2339&#39;}, {&#39;name&#39;: &#39;GRUPO SANMARI SA DE CV&#39;, &#39;id&#39;: &#39;GSA0310175N4&#39;}, {&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;}, {&#39;name&#39;: &#39;HECTOR MANUEL SEGURA TORRE&#39;, &#39;id&#39;: &#39;E2E9D6DA235621FC08C1A0EFC4201B95&#39;}]}, &#39;language&#39;: &#39;es&#39;, &#39;awards&#39;: [{&#39;id&#39;: &#39;1399908&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;active&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}, &#39;suppliers&#39;: [{&#39;name&#39;: &#39;GABRIEL DEL POZO RUIZ&#39;, &#39;id&#39;: &#39;AA2EEEF597460501F7B8A50B4DE1F671&#39;}], &#39;items&#39;: [{&#39;id&#39;: &#39;4645830&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;classification&#39;: {&#39;scheme&#39;: &#39;CUCOP: Clasificador Único de las Contrataciones Públicas&#39;, &#39;id&#39;: &#39;35900004&#39;, &#39;description&#39;: &#39;Servicios de jardineria&#39;, &#39;uri&#39;: &#39;https://compranetinfo.funcionpublica.gob.mx/descargas/CUCOP.xlsx&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}}}], &#39;contractPeriod&#39;: {&#39;startDate&#39;: &#39;2017-04-14T09:00:00Z&#39;, &#39;endDate&#39;: &#39;2018-06-18T03:59:00Z&#39;}}], &#39;contracts&#39;: [{&#39;id&#39;: 1399908, &#39;awardID&#39;: &#39;1399908&#39;, &#39;title&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento y Conservación de Plantas Naturales y Macetas Propiedad de la CNBV&#39;, &#39;status&#39;: &#39;terminated&#39;, &#39;period&#39;: {&#39;startDate&#39;: &#39;2017-04-14T09:00:00Z&#39;, &#39;endDate&#39;: &#39;2018-06-18T03:59:00Z&#39;}, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}, &#39;items&#39;: [{&#39;id&#39;: &#39;4645830&#39;, &#39;description&#39;: &#39;Servicio Integral de Suministro, Mantenimiento Plantas y Macetas&#39;, &#39;classification&#39;: {&#39;id&#39;: &#39;35900004&#39;, &#39;description&#39;: &#39;Servicios de jardineria&#39;}, &#39;quantity&#39;: 1, &#39;unit&#39;: {&#39;name&#39;: &#39;Servicio&#39;, &#39;value&#39;: {&#39;amount&#39;: 450000, &#39;currency&#39;: &#39;MXN&#39;}}}]}]} . participantes_contrato = {c[&#39;ocid&#39;]: [p[&#39;id&#39;] for p in c[&#39;parties&#39;] if p[&#39;roles&#39;] in [[&#39;tenderer&#39;], [&#39;tenderer&#39;, &#39;supplier&#39;]]] for c in casos_contratos} # Número de particpantes que estaban asociados en cada contrato asociados_contrato = {o: c[&#39;tenderer_ids&#39;] for c in casos for o in c[&#39;contratos_ocid&#39;]} # ganador contrato ganadores_contrato = {c[&#39;ocid&#39;]: [p[&#39;id&#39;] for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;tenderer&#39;, &#39;supplier&#39;]] for c in casos_contratos} # dataframe df_asoc = pd.DataFrame([participantes_contrato, asociados_contrato, ganadores_contrato]).T .rename(columns={0: &#39;part&#39;, 1: &#39;asoc&#39;, 2: &#39;gana&#39;}) .assign(N_part=lambda x: x[&#39;part&#39;].str.len(), N_asoc=lambda x: x[&#39;asoc&#39;].str.len(), N_gana=lambda x: x[&#39;gana&#39;].str.len(), prop_asoc_part=lambda x: x[&#39;N_asoc&#39;].div(x[&#39;N_part&#39;]), part_mayo=lambda x: x[&#39;prop_asoc_part&#39;].ge(0.5), asoc_ganadores=lambda x: x.apply(lambda y: list(set(y[&#39;gana&#39;]).intersection(set(y[&#39;asoc&#39;]))), axis=1), N_asoc_ganadores=lambda x: x[&#39;asoc_ganadores&#39;].str.len(), part_nogana=lambda x: x.apply(lambda y: list(set(y[&#39;part&#39;]).difference(set(y[&#39;gana&#39;]))), axis=1)) .join(df_datos_contratos) df_asoc.to_pickle(f&#39;{dir_datos}/df_asociados.pkl&#39;) df_asoc.head() . part asoc gana N_part N_asoc N_gana prop_asoc_part part_mayo asoc_ganadores N_asoc_ganadores part_nogana titulo descr valor dependencia_id dependencia_nombre uc_id uc_name fecha . ocds-07smqs-1043398 [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | [TME840315KT6, BD03FBE666C3DBA5C57BCDC8BF0AA451] | 2 | 2 | 2 | 1.000000 | True | [BD03FBE666C3DBA5C57BCDC8BF0AA451, TME840315KT6] | 2 | [] | SERVICIO MPLS ATRAVES DE UN ENLACE DEDICADO CO... | | 169133.00 | CIJ-66 | Centros de Integración Juvenil, A.C. | CIJ731003QK3-012M7K001 | CIJ-Departamento de Adquisiciones #012M7K001 | 2016-04-18T12:02:38Z | . ocds-07smqs-1193763 [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D... | [D73016CAA1F8020E3BAC52068FB0B2D9, MLA840208FN5] | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D... | 4 | 2 | 4 | 0.500000 | True | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D9] | 2 | [] | ADQ. DE VIVERES PARA EJERCICIO 2017 | ADQ. DE VIVERES PARA EJERCICIO 2017 | 3880739.50 | IMSS-192 | Instituto Mexicano del Seguro Social | IMS421231I45-050GYR045 | IMSS-UMAE Hospital de Especilidades No.71 Dept... | 2016-12-09T05:26:28Z | . ocds-07smqs-1224403 [89E87098891F04A46318B7F775AD5E48, FAR100921AL... | [D73016CAA1F8020E3BAC52068FB0B2D9, MLA840208FN5] | [D73016CAA1F8020E3BAC52068FB0B2D9, 9D3346ADF0B... | 8 | 2 | 5 | 0.250000 | False | [MLA840208FN5, D73016CAA1F8020E3BAC52068FB0B2D9] | 2 | [FAR100921ALA, 89E87098891F04A46318B7F775AD5E4... | AA-019GYR026-E221-2016 DESIERTAS VIVERES | | 671930.69 | IMSS-192 | Instituto Mexicano del Seguro Social | IMS421231I45-050GYR026 | IMSS-Coordinación de abastecimiento y equipami... | 2016-12-02T05:49:52Z | . ocds-07smqs-1240190 [RDO070228V11, ATD061228L34, MEX0301141G6] | [RDO070228V11, ATD061228L34] | [MEX0301141G6] | 3 | 2 | 1 | 0.666667 | True | [] | 0 | [RDO070228V11, ATD061228L34] | SERVICIO DE RESGUARDO, CUSTODIA, TRASLADO, ENV... | SERVICIO DE RESGUARDO, CUSTODIA, TRASLADO, ENV... | 116379.72 | CPTM-109 | Consejo de Promoción Turística de México, S.A.... | CPT991022DE7-021W3J001 | CPTM-Gerencia de Adquisiciones y Licitaciones ... | 2016-12-20T06:52:52Z | . ocds-07smqs-1241959 [CGE130930JV2, CDA9601297G9, 23DF515587ED8B3F4... | [CDA9601297G9, 23DF515587ED8B3F4A8B1C9E4D725CAD] | [] | 3 | 2 | 0 | 0.666667 | True | [] | 0 | [CGE130930JV2, 23DF515587ED8B3F4A8B1C9E4D725CA... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . df_asoc = pd.read_pickle(f&#39;{dir_datos}/df_asociados.pkl&#39;) . ¿En cuántos de estos casos los contratistas representaban el 50% de los proponentes o más? | . print(&#39;Los contratistas representaban el 50% de los proponentes o más en&#39;, df_asoc.part_mayo.sum(), &#39;licitaciones&#39;) . Los contratistas representaban el 50% de los proponentes o más en 212 licitaciones . ¿En cuántos de estos casos los asociados fueron los únicos proponentes? | . print(&#39;¿En cuántos de estos casos los asociados fueron los únicos proponentes?&#39;, df_asoc.prop_asoc_part.eq(1).sum(), &#39;licitaciones&#39;) . ¿En cuántos de estos casos los asociados fueron los únicos proponentes? 41 licitaciones . De estos casos ¿en cuántas licitaciones los que estaban relacionados ganaron un concurso? | . print(&#39;En&#39;, df_asoc.N_asoc_ganadores.gt(0).sum(), &#39;licitaciones ganó al menos uno de los contratistas asociados&#39;) . En 370 licitaciones ganó al menos uno de los contratistas asociados . ¿Cuántos contratistas asociados recibieron un contrato? | . print(df_asoc.N_asoc_ganadores.sum(), &#39; contratistas asociados ganaron una licitación&#39;) . 552 contratistas asociados ganaron una licitación . ¿En qué dependencias, unidades compradoras y servidores públicos ocurre más esto? | . df_asoc.groupby([&#39;uc_name&#39;])[&#39;part&#39;].count().sort_values(ascending=False) . uc_name CONALITEG-Dirección de Recursos Materiales y Servicios Generales #011L6J001 33 IMSS-Departamento de Adquisición de Bienes y Contratación de Servicios #050GYR033 16 IMSS-Coordinación de Abastecimiento y Equipamiento #050GYR009 10 IMSS-Coordinación de Adquisición de Bienes y Contratación de Serv, Dirección de Administración #050GYR047 9 INER-Departamento de Adquisiciones #012NCD001 9 .. IMSS-UMAE HOSPITAL DE GINECO OBSTETRICIA No 03 DR VICTOR MANUEL ESPINOSA DE LOS REYES SANCHEZ CMN LA RAZA #050GYR050 1 IMSS-Coord. de Abastecimiento y Equipamiento Deleg. Ver. Sur #050GYR022 1 SCT-Centro SCT Chihuahua #009000980 1 SCT-CENTRO SCT EN CHIAPAS SUBDIRECCION DE ADMINISTRACION #009000992 1 Tribunales Agrarios-Dirección General de Recursos Materiales #031000001 1 Name: part, Length: 218, dtype: int64 . df_asoc.groupby([&#39;dependencia_nombre&#39;])[&#39;part&#39;].count().sort_values(ascending=False) . dependencia_nombre Instituto Mexicano del Seguro Social 215 Comisión Nacional de Libros de Texto Gratuitos 33 Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado 32 Comisión Federal de Electricidad 24 Comisión Nacional del Agua 15 ... Hospital Regional de Alta Especialidad de Oaxaca 1 Exportadora de Sal, S.A. de C.V. 1 El Colegio de la Frontera Sur 1 Corporación Mexicana de Investigación en Materiales, S.A. de C.V. 1 Administración Portuaria Integral de Progreso, S.A. de C.V. 1 Name: part, Length: 92, dtype: int64 . Crea red para visualizar . nodos = [] red = 1 for c, vals in df_asoc.iterrows(): nodos.append({&#39;id&#39;: c, &#39;tipo&#39;: &#39;contrato&#39;}) for p in vals[&#39;part_nogana&#39;]: nodos.append({&#39;id&#39;: p, &#39;tipo&#39;: &#39;tenderer&#39;}) links.append({&#39;origen_id&#39;: p, &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;participa&#39;, &#39;red&#39;: red}) for p in vals[&#39;gana&#39;]: nodos.append({&#39;id&#39;: p, &#39;tipo&#39;: &#39;supplier&#39;}) links.append({&#39;origen_id&#39;: p, &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;gana&#39;, &#39;red&#39;: red}) for p1 in vals[&#39;asoc&#39;]: for p2 in vals[&#39;asoc&#39;]: if p1!=p2: links.append({&#39;origen_id&#39;: p1, &#39;destino_id&#39;: p2, &#39;accion&#39;: &#39;asociado&#39;, &#39;red&#39;: red}) nodos.append({&#39;id&#39;: vals[&#39;uc_id&#39;], &#39;tipo&#39;: &#39;uc&#39;}) links.append({&#39;origen_id&#39;: vals[&#39;uc_id&#39;], &#39;destino_id&#39;: c, &#39;accion&#39;: &#39;compra&#39;, &#39;red&#39;: red}) red+=1 . df_nodos = pd.DataFrame(nodos) .assign(num=lambda x:x.index) dicc_nodo_num = {v:k for k,v in df_nodos[&#39;id&#39;].to_dict().items()} df_links = pd.DataFrame(links) .assign(origen_num=lambda x: x[&#39;origen_id&#39;].map(dicc_nodo_num), destino_num=lambda x: x[&#39;destino_id&#39;].map(dicc_nodo_num)) .dropna() df_links.head() . origen_id destino_id accion red origen_num destino_num . 25696 TME840315KT6 | ocds-07smqs-1043398 | gana | 1.0 | 287 | 0 | . 25697 BD03FBE666C3DBA5C57BCDC8BF0AA451 | ocds-07smqs-1043398 | gana | 1.0 | 286 | 0 | . 25698 TME840315KT6 | BD03FBE666C3DBA5C57BCDC8BF0AA451 | asociado | 1.0 | 287 | 286 | . 25699 BD03FBE666C3DBA5C57BCDC8BF0AA451 | TME840315KT6 | asociado | 1.0 | 286 | 287 | . 25700 CIJ731003QK3-012M7K001 | ocds-07smqs-1043398 | compra | 1.0 | 284 | 0 | . df_nodos.to_csv(&#39;datos/asociados_nodos.csv&#39;, index=False) df_links.to_csv(&#39;datos/asociados_links.csv&#39;, index=False) . Visualziacion networkX . df_nodos_graph = df_nodos.set_index(&#39;id&#39;) . for red in df_links.red.unique(): G = nx.from_pandas_edgelist(df_links.query(&#39;red==@red&#39;), source=&#39;origen_id&#39;, target=&#39;destino_id&#39;, edge_attr=[&#39;accion&#39;]) dicc_color_edges = {&#39;gana&#39;: &#39;green&#39;, &#39;asociado&#39;: &#39;red&#39;, &#39;participa&#39;: &#39;#3292a8&#39;, &#39;compra&#39;: &#39;orange&#39;} dicc_color_nodos = {&#39;contrato&#39;: &#39;#3292a8&#39;, &#39;supplier&#39;: &#39;pink&#39;, &#39;tenderer&#39;: &#39;blue&#39;, &#39;uc&#39;: &#39;orange&#39;} color_edges = [dicc_color_edges[e[2][&#39;accion&#39;]] for e in G.edges(data=True)] color_nodes = [dicc_color_nodos[df_nodos_graph.loc[[i], &#39;tipo&#39;].tolist()[0]] for i in G.nodes] fig, ax = plt.subplots() draw_network(G, color_edges=color_edges, color_nodes=color_nodes, axes=ax, labels=[1, 2, 4, 5], text_size=8) fig.savefig(f&#39;graficas/redes/red_{red}.png&#39;, dpi=200) plt.cla() . print(df_asoc.loc[[x for x in df_links.query(&#39;red==@red&#39;)[&#39;destino_id&#39;].unique() if &#39;ocds&#39; in x][0]]) . Tareas: . Procesar telefonos . | procesar múltiples mails . | Es posible obtener más datos de los contratistas a partir del RUCP, como el sitio web, giro del negocio . | . Buscar otra anomalía: todos los contratos con métodos abiertos en los que solo participa un proponente. Buscar contratos en los que todos los particpantes reciben contrato. . Buscar otra anomalía: todos los contratos con métodos abiertos en los que solo participa un proponente. | Buscar contratos en los que todos los particpantes reciben contrato. | . Funcionarios que intervienen en contrataciones . mydb.func_contrat.count_documents({}) . 113795 . mydb.func_contrat.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb255d723b95da59c6a01b&#39;), &#39;id&#39;: &#39;c6dbd706-b539-476f-a400-4dd69ed4a757&#39;, &#39;fechaCaptura&#39;: &#39;&#39;, &#39;ejercicioFiscal&#39;: 2017, &#39;periodoEjercicio&#39;: {&#39;fechaInicial&#39;: &#39;2017/01/01&#39;, &#39;fechaFinal&#39;: &#39;2017/12/31&#39;}, &#39;idRamo&#39;: 6, &#39;ramo&#39;: &#39;HACIENDA Y CRÉDITO PÚBLICO&#39;, &#39;nombres&#39;: None, &#39;primerApellido&#39;: None, &#39;segundoApellido&#39;: None, &#39;genero&#39;: None, &#39;institucionDependencia&#39;: {&#39;siglas&#39;: &#39;CNBV&#39;, &#39;nombre&#39;: &#39;COMISIÓN NACIONAL BANCARIA Y DE VALORES&#39;, &#39;clave&#39;: &#39;6/B00&#39;}, &#39;puesto&#39;: {&#39;nombre&#39;: &#39;SUBDIRECTOR DE MEJORA A&#39;, &#39;nivel&#39;: None}, &#39;tipoArea&#39;: [&#39;R&#39;], &#39;nivelResponsabilidad&#39;: [&#39;A&#39;, &#39;T&#39;], &#39;tipoProcedimiento&#39;: 1, &#39;tipoActos&#39;: &#39;CONTRATACIONES&#39;, &#39;superiorInmediato&#39;: {&#39;nombres&#39;: None, &#39;primerApellido&#39;: None, &#39;segundoApellido&#39;: None, &#39;puesto&#39;: {&#39;nombre&#39;: None, &#39;nivel&#39;: None}}} . nombre_func_contrat = [f&#39;{r[&quot;nombres&quot;]} {r[&quot;primerApellido&quot;]} {r[&quot;segundoApellido&quot;]}&#39; for r in mydb.func_contrat.find({}, {&#39;_id&#39;:0, &#39;nombres&#39;: 1, &#39;primerApellido&#39;: 1, &#39;segundoApellido&#39;:1}) if all([r[&quot;nombres&quot;], r[&quot;primerApellido&quot;], r[&quot;segundoApellido&quot;]])] . Funcionarios sancionados . mydb.serv_sanc.count_documents({}) . 3575 . mydb.serv_sanc.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb27a2432e395ca7ba4a62&#39;), &#39;nombres&#39;: &#39;ZACARIAS&#39;, &#39;primerApellido&#39;: &#39;PEREZ&#39;, &#39;segundoApellido&#39;: &#39;GARCIA&#39;, &#39;institucionDependencia&#39;: {&#39;nombre&#39;: &#39;PROCURADURIA GENERAL DE LA REPUBLICA&#39;, &#39;siglas&#39;: &#39; &#39;}, &#39;autoridadSancionadora&#39;: &#39;ORGANO INTERNO DE CONTROL&#39;, &#39;expediente&#39;: &#39;520/99&#39;, &#39;resolucion&#39;: {&#39;fechaResolucion&#39;: &#39;17/11/2000&#39;}, &#39;tipoSancion&#39;: &#39;INHABILITACION&#39;, &#39;inhabilitacion&#39;: {&#39;fechaInicial&#39;: &#39;17/11/2000&#39;, &#39;fechaFinal&#39;: &#39;16/11/2020&#39;, &#39;observaciones&#39;: None}, &#39;multa&#39;: {&#39;monto&#39;: None, &#39;moneda&#39;: &#39;MXN&#39;}, &#39;causaMotivoHechos&#39;: &#39;ABUSO DE AUTORIDAD&#39;, &#39;puesto&#39;: &#39;AGENTE DE LA POLICIA JUDICIAL FEDERAL&#39;} . nombre_serv_sanc = [f&#39;{r[&quot;nombres&quot;]} {r[&quot;primerApellido&quot;]} {r[&quot;segundoApellido&quot;]}&#39; for r in mydb.serv_sanc.find({}, {&#39;_id&#39;:0, &#39;nombres&#39;: 1, &#39;primerApellido&#39;: 1, &#39;segundoApellido&#39;:1})] . casos_func = [p[&#39;contactPoint&#39;][&#39;name&#39;] for c in casos_contratos for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;procuringEntity&#39;]] . set(casos_func).intersection(set(nombre_serv_sanc)) . set() . yt = list(set(df_contactos.name.unique().tolist()).intersection(set(nombre_serv_sanc))) len(yt) . 10 . yt . [&#39;MIGUEL ANGEL TORRES HERNANDEZ&#39;, &#39;AGUSTIN TOLEDO GADEA&#39;, &#39;OSCAR CHAVEZ MARTINEZ&#39;, &#39;JOSE LUIS CHAVEZ FLORES&#39;, &#39;MARIO HERNANDEZ DIAZ&#39;, &#39;RODRIGO MALDONADO SAHAGUN&#39;, &#39;ERIKA BENITEZ GARCIA&#39;, &#39;FRANCISCO FIERRO SILVA&#39;, &#39;JOSE LUIS GARCIA RODRIGUEZ&#39;, &#39;JOSE DE LA CRUZ RAMIREZ&#39;] . Particulares sancionados . mydb.part_sanc.count_documents({}) . 1853 . mydb.part_sanc.find_one() . {&#39;_id&#39;: ObjectId(&#39;5deb27d0715998a251b6be6b&#39;), &#39;fechaCaptura&#39;: &#39;2019-08-22&#39;, &#39;expediente&#39;: &#39;000270074/2017&#39;, &#39;nombreRazonSocial&#39;: &#39;CONSTRUCCIÓN ESPECIALIZADA Y TECNOLÓGICA DE MÉXICO, S.A. DE C.V.&#39;, &#39;rfc&#39;: &#39;ACV990407&#39;, &#39;telefono&#39;: &#39;01 961 61 5 30 09&#39;, &#39;domicilio&#39;: {&#39;clave&#39;: &#39;MX&#39;}, &#39;tipoSancion&#39;: &#39;ECONOMICA E INHABILITACIÓN&#39;, &#39;institucionDependencia&#39;: {&#39;nombre&#39;: &#39;SECRETARIA DE LA FUNCIÓN PÚBLICA&#39;, &#39;siglas&#39;: &#39;SFP&#39;}, &#39;tipoFalta&#39;: &#39;&#39;, &#39;causaMotivoHechos&#39;: &#39;NO ENTREGAR LA OBRA EN LA FECHA COMPROMETIDA PARA ELLO, ESTO ES EL 24 DE SEPTIEMBRE DE 2014&#39;, &#39;objetoContrato&#39;: &#39;&#39;, &#39;autoridadSancionadora&#39;: &#39;SECRETARIA DE LA FUNCIÓN PÚBLICA&#39;, &#39;responsableSancion&#39;: {&#39;nombres&#39;: &#39;MARÍA GUADALUPE VARGAS ÁLVAREZ&#39;, &#39;primerApellido&#39;: &#39;&#39;, &#39;segundoApellido&#39;: &#39;&#39;}, &#39;resolucion&#39;: {&#39;sentido&#39;: &#39;SANCIONATORIA CON MULTA E INHABILITACIÓN&#39;}, &#39;fechaNotificacion&#39;: &#39;2019-08-14&#39;, &#39;multa&#39;: {&#39;monto&#39;: &#39;504675.00&#39;, &#39;moneda&#39;: &#39;MXN&#39;}, &#39;plazo&#39;: {&#39;fechaInicial&#39;: &#39;2019-08-23&#39;}, &#39;observaciones&#39;: None} . rfc_sanc = [r[&#39;rfc&#39;] for r in mydb.part_sanc.find({&#39;rfc&#39;: {&#39;$ne&#39;: &#39;&#39;}}, {&#39;_id&#39;:0, &#39;rfc&#39;: 1})] . Red Mitchell . result1 = mydb.contrataciones.find({&#39;contracts&#39;: {&#39;$exists&#39;: True}}, [&#39;ocid&#39;, &#39;parties.id&#39;, &#39;parties.roles&#39;, &#39;parties.contactPoint&#39;, &#39;contracts.value.amount&#39;, &#39;date&#39;]) l1 = list(result1) . ocid_tenderer = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;id&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;] in [[&#39;tenderer&#39;, &#39;supplier&#39;], [&#39;tenderer&#39;]]], columns=[&#39;ocid&#39;, &#39;tenderer_id&#39;], ).set_index(&#39;ocid&#39;) ocid_tenderer.head() . tenderer_id . ocid . ocds-07smqs-1003803 E9C1C827AE1234CCF7AC4D9070BB597C | . ocds-07smqs-1003123 SCA031118BX7 | . ocds-07smqs-1003123 SAU0505307M9 | . ocds-07smqs-1003123 SCK070618C21 | . ocds-07smqs-1009245 R&amp;S811221KR6 | . ocid_funcionario = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;contactPoint&#39;].get(&#39;name&#39;, &#39;&#39;), p[&#39;id&#39;], c[&#39;contracts&#39;][0][&#39;value&#39;][&#39;amount&#39;], c[&#39;date&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;procuringEntity&#39;]], columns=[&#39;ocid&#39;, &#39;funcionario_id&#39;, &#39;uc_id&#39;, &#39;valor_contrato&#39;, &#39;fecha&#39;]) .set_index(&#39;ocid&#39;) ocid_dependencia = pd.DataFrame([(c[&#39;ocid&#39;], p[&#39;id&#39;]) for c in l1 for p in c[&#39;parties&#39;] if p[&#39;roles&#39;]==[&#39;buyer&#39;]], columns=[&#39;ocid&#39;, &#39;dep_id&#39;]) .set_index(&#39;ocid&#39;) ocid_funcionario.head() . funcionario_id uc_id valor_contrato fecha . ocid . ocds-07smqs-1003803 José Gabriel Ramos Martínez | SAT970701NN3-006E00002 | 8451072.00 | 2016-02-19T01:09:18Z | . ocds-07smqs-1003123 Ignacio Romero Sánchez | PGR850101RC6-017000017 | 168000.00 | 2016-02-19T01:49:22Z | . ocds-07smqs-1009245 Juan Fernando Meza Zavala | STP401231P53-014000999 | 420689.55 | 2016-02-26T05:33:08Z | . ocds-07smqs-1012355 Luis Eduardo Vega Becerra | CNU800928K31-018E00999 | 20000.00 | 2016-03-02T01:58:39Z | . ocds-07smqs-1025654 Marco Antonio Brito Vidales | IAA6210025R4-006A00996 | 10604000.00 | 2016-03-18T06:40:28Z | . ocid_tender_fun = ocid_tenderer.join([ocid_funcionario, ocid_dependencia]) ocid_tender_fun . tenderer_id funcionario_id uc_id valor_contrato fecha dep_id . ocid . ocds-07smqs-1001024 ELE9012281G2 | Evelyn López Valverde | LIC950821M84-020VST003 | 1.152540e+05 | 2016-03-15T01:02:50Z | LICONSA-231 | . ocds-07smqs-1001040 HIG090519H30 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 2.603075e+07 | 2016-02-16T02:44:58Z | HIM-163 | . ocds-07smqs-1001984 282910F3163E9D7DBC543E53CD9347B6 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 1.071380e+05 | 2016-02-17T04:42:35Z | HIM-163 | . ocds-07smqs-1002362 IPS040121S66 | Nicolas Gonzalez Bustos | HIM871203BS0-012NBG001 | 2.115000e+05 | 2016-02-17T07:30:57Z | HIM-163 | . ocds-07smqs-1003123 SCA031118BX7 | Ignacio Romero Sánchez | PGR850101RC6-017000017 | 1.680000e+05 | 2016-02-19T01:49:22Z | PGR-251 | . ... ... | ... | ... | ... | ... | ... | . ocds-07smqs-999514 CPC131113AT4 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 96A74A55F4E5DAEC0797B59049D8EC81 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 TGH130612IK1 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 SIN011023UC8 | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . ocds-07smqs-999514 GMC09121623A | Luis Enrique Mendoza Flores | IMS421231I45-050GYR026 | 7.317600e+04 | 2016-02-12T01:34:46Z | IMSS-192 | . 726038 rows × 6 columns . ocid_tender_fun.to_csv(f&#39;{dir_datos}/ocid_tender_fun.csv&#39;) .",
            "url": "http://blog.jjsantoso.com/analisis%20de%20datos/pandas/pymongo/2020/01/20/redes-contratos-compranet.html",
            "relUrl": "/analisis%20de%20datos/pandas/pymongo/2020/01/20/redes-contratos-compranet.html",
            "date": " • Jan 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Bio",
          "content": ". Soy un economista apasionado por el análisis de datos, la estadística y la visualización. Trabajo en la Unidad de Ciencia de Datos del Laboratorio Nacional de Políticas Públicas del CIDE donde hacemos análisis de datos para temas de políticas públicas. A veces doy cursos de Python. A veces participo en datatones. Aquí pueden conocer algo de los proyectos personales en los que me gusta trabajar. . Soy de Cartagena (Colombia) y vivo en la Ciudad de México. Me gusta el montañismo y conocer la riqueza natural y cultural de México. . Me encuentran en Twitter como @jjsantoso y aquí está mi perfil en LinkedIn. Pueden consultar aquí mi CV o también revisar mi cuenta de GitHub. .",
          "url": "http://blog.jjsantoso.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://blog.jjsantoso.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}